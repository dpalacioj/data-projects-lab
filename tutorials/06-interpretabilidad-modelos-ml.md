# Tutorial: Interpretabilidad de Modelos de Machine Learning

## Ãndice

1. [IntroducciÃ³n](#introducciÃ³n)
2. [Â¿Por quÃ© es importante la interpretabilidad?](#por-quÃ©-es-importante-la-interpretabilidad)
3. [InterpretaciÃ³n Global: Feature Importance](#interpretaciÃ³n-global-feature-importance)
4. [InterpretaciÃ³n Local: SHAP](#interpretaciÃ³n-local-shap)
5. [InterpretaciÃ³n Local: LIME](#interpretaciÃ³n-local-lime)
6. [ComparaciÃ³n de MÃ©todos](#comparaciÃ³n-de-mÃ©todos)
7. [Casos de Uso PrÃ¡cticos](#casos-de-uso-prÃ¡cticos)
8. [Referencias](#referencias)

---

## IntroducciÃ³n

### El Problema de la "Caja Negra"

```mermaid
graph LR
    A[Datos de<br/>entrada] --> B[ğŸ©<br/>Modelo ML<br/>Â¿?Â¿?Â¿?]
    B --> C[PredicciÃ³n]

    D[Usuario] --> E{Â¿Por quÃ©<br/>predijo eso?}
    E --> F[âŒ No lo sÃ©]

    style B fill:#333
    style F fill:#ffcccc
```

Muchos modelos de ML funcionan como **"cajas negras"**: dan predicciones pero no explican **por quÃ©**.

### La SoluciÃ³n: Interpretabilidad

```mermaid
graph TD
    A[Modelo ML] --> B[InterpretaciÃ³n<br/>Global]
    A --> C[InterpretaciÃ³n<br/>Local]

    B --> B1[Feature Importance<br/>Â¿QuÃ© variables son<br/>mÃ¡s importantes?]

    C --> C1[SHAP/LIME<br/>Â¿Por quÃ© esta<br/>predicciÃ³n especÃ­fica?]

    style B1 fill:#ccffcc
    style C1 fill:#cce5ff
```

**Dos niveles de interpretaciÃ³n**:

| Nivel | Pregunta | MÃ©todos | CuÃ¡ndo Usar |
|-------|----------|---------|-------------|
| **Global** | Â¿QuÃ© variables son importantes **en general**? | Feature Importance, SHAP global | Entender el modelo completo |
| **Local** | Â¿Por quÃ© **esta predicciÃ³n especÃ­fica**? | SHAP local, LIME | Explicar decisiones individuales |

---

## Â¿Por quÃ© es importante la interpretabilidad?

### Casos de Uso Reales

```mermaid
mindmap
  root((Interpretabilidad))
    RegulaciÃ³n
      GDPR: Derecho a explicaciÃ³n
      Finanzas: Cumplimiento
      Seguros: Transparencia
    Confianza
      Usuarios confÃ­an mÃ¡s
      Stakeholders entienden
      AdopciÃ³n del modelo
    Debugging
      Detectar sesgos
      Encontrar errores
      Mejorar features
    Negocio
      Insights accionables
      Decisiones estratÃ©gicas
      ROI demostrable
```

### Ejemplos Concretos

#### 1. Medicina

```
Modelo: PredicciÃ³n de riesgo de enfermedad cardÃ­aca
PredicciÃ³n: "Alto riesgo (85%)"

âŒ Sin interpretabilidad:
   "Tiene 85% de riesgo" â†’ El mÃ©dico no sabe quÃ© factores considerar

âœ… Con interpretabilidad:
   "85% de riesgo PORQUE:
   - Colesterol: +25%
   - PresiÃ³n arterial: +30%
   - Edad: +15%
   - Tabaquismo: +15%"

   â†’ El mÃ©dico puede dar recomendaciones especÃ­ficas
```

#### 2. PrÃ©stamos Bancarios

```
Modelo: AprobaciÃ³n de prÃ©stamo
PredicciÃ³n: "RECHAZADO"

âŒ Sin interpretabilidad:
   "PrÃ©stamo denegado" â†’ Cliente frustrado, posible demanda

âœ… Con interpretabilidad:
   "Denegado PORQUE:
   - Historial crediticio insuficiente (-40%)
   - Ratio deuda/ingreso alto (-35%)
   - Sin ahorros (-25%)"

   â†’ Cliente entiende y puede mejorar
```

#### 3. Marketing

```
Modelo: PredicciÃ³n de abandono de clientes (churn)
PredicciÃ³n: "Cliente probablemente abandonarÃ¡"

âŒ Sin interpretabilidad:
   "Churn probable" â†’ Â¿QuÃ© hacer?

âœ… Con interpretabilidad:
   "Churn probable PORQUE:
   - Baja frecuencia de uso (-30%)
   - No abre emails (-25%)
   - Precio alto vs competencia (-20%)"

   â†’ Equipo sabe quÃ© acciones tomar
```

---

## InterpretaciÃ³n Global: Feature Importance

### Â¿QuÃ© es Feature Importance?

**DefiniciÃ³n**: Mide cuÃ¡nto **contribuye cada variable** a las predicciones del modelo **en promedio**.

```mermaid
graph TD
    A[Modelo entrenado] --> B[Analizar todas<br/>las predicciones]
    B --> C[Calcular contribuciÃ³n<br/>de cada variable]
    C --> D[Ranking de<br/>importancia]

    D --> E[Variable 1: 35%]
    D --> F[Variable 2: 25%]
    D --> G[Variable 3: 20%]
    D --> H[Variable 4: 15%]
    D --> I[Variable 5: 5%]

    style E fill:#ff6666
    style F fill:#ff9966
    style G fill:#ffcc66
    style H fill:#ffff66
    style I fill:#ccff66
```

### Ejemplo Visual: PredicciÃ³n de Precio de Casas

```
Dataset: Predecir precio de casas

Variables:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Variable        â”‚ Importancia      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ UbicaciÃ³n       â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 40% â”‚ â† MÃS importante
â”‚ TamaÃ±o (mÂ²)     â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 30%     â”‚
â”‚ AÃ±o construc.   â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 15%        â”‚
â”‚ Num. habitac.   â”‚ â–ˆâ–ˆâ–ˆ 10%          â”‚
â”‚ Color fachada   â”‚ â–ˆ 5%             â”‚ â† MENOS importante
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

InterpretaciÃ³n:
âœ… La ubicaciÃ³n es el factor mÃ¡s determinante (40%)
âœ… El tamaÃ±o tambiÃ©n es muy importante (30%)
âš ï¸ El color de la fachada casi no importa (5%)
```

---

### MÃ©todos de CÃ¡lculo

#### 1. Importancia Basada en Impureza (Tree-based)

**Concepto**: En Ã¡rboles de decisiÃ³n, mide cuÃ¡nto **reduce la impureza** cada variable cuando se usa para dividir.

```mermaid
graph TD
    A[Nodo raÃ­z<br/>1000 muestras<br/>Impureza: 0.5] -->|Split por 'UbicaciÃ³n'| B[Nodo izq<br/>300 muestras<br/>Impureza: 0.2]
    A -->|Split por 'UbicaciÃ³n'| C[Nodo der<br/>700 muestras<br/>Impureza: 0.3]

    D["ReducciÃ³n de impureza = 0.5 - (0.3Ã—0.2 + 0.7Ã—0.3) = 0.5 - 0.27 = 0.23"]

    E[Mayor reducciÃ³n = Mayor importancia]

    style D fill:#ffffcc
    style E fill:#ccffcc
```

**VisualizaciÃ³n**:

```
Antes del split (UbicaciÃ³n):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Mezcla de precios:       â”‚
â”‚ $100k, $500k, $200k,     â”‚
â”‚ $400k, $150k, $600k...   â”‚
â”‚ Impureza ALTA = 0.5      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DespuÃ©s del split:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Zona A:       â”‚  â”‚ Zona B:      â”‚
â”‚ $100k, $150k  â”‚  â”‚ $500k, $600k â”‚
â”‚ $120k, $180k  â”‚  â”‚ $550k, $580k â”‚
â”‚ Impureza BAJA â”‚  â”‚ Impureza BAJAâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â†’ UbicaciÃ³n es IMPORTANTE porque reduce mucho la impureza
```

#### CÃ³digo con scikit-learn

```python
from sklearn.ensemble import RandomForestRegressor
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# Datos de ejemplo
data = {
    'ubicacion_score': [8, 3, 9, 2, 7, 4, 8, 3, 9, 6],
    'tamano_m2': [120, 80, 150, 70, 130, 85, 140, 75, 160, 95],
    'ano_construccion': [2020, 1990, 2018, 1985, 2015, 1995, 2019, 1988, 2021, 2000],
    'num_habitaciones': [3, 2, 4, 2, 3, 2, 4, 2, 4, 3],
    'color_fachada': [1, 2, 1, 3, 2, 1, 2, 3, 1, 2],  # 1=blanco, 2=beige, 3=gris
    'precio': [450000, 180000, 520000, 150000, 410000, 190000, 480000, 160000, 550000, 280000]
}

df = pd.DataFrame(data)

# Preparar datos
X = df.drop('precio', axis=1)
y = df['precio']

# Entrenar modelo
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X, y)

# Obtener importancias
importances = model.feature_importances_
feature_names = X.columns

# Crear DataFrame para visualizaciÃ³n
importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False)

print(importance_df)

# VisualizaciÃ³n
plt.figure(figsize=(10, 6))
plt.barh(importance_df['feature'], importance_df['importance'])
plt.xlabel('Importancia')
plt.title('Feature Importance - Random Forest')
plt.tight_layout()
plt.show()

# Ejemplo de salida:
#              feature  importance
# 0  ubicacion_score      0.45
# 1       tamano_m2      0.30
# 2  ano_construccion    0.15
# 3   num_habitaciones   0.08
# 4    color_fachada     0.02
```

---

#### 2. Importancia por PermutaciÃ³n

**Concepto**: Mide cuÃ¡nto **empeora** el modelo si **desordenamos** (permutamos) una variable.

```mermaid
graph TD
    A[Modelo con todos<br/>los datos correctos] --> B[Accuracy: 90%]

    C[Permutar variable<br/>'UbicaciÃ³n'] --> D[Datos desordenados<br/>para esa variable]
    D --> E[Accuracy: 65%]

    F["Importancia = 90% - 65% = 25%"]

    G[Mayor caÃ­da = Mayor importancia]

    style F fill:#ffffcc
    style G fill:#ccffcc
```

**VisualizaciÃ³n del Proceso**:

```
Paso 1: Datos originales
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UbicaciÃ³nâ”‚ TamaÃ±o  â”‚ Precio â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Centro   â”‚ 120 mÂ²  â”‚ $450k  â”‚
â”‚ Periferiaâ”‚ 80 mÂ²   â”‚ $180k  â”‚
â”‚ Centro   â”‚ 150 mÂ²  â”‚ $520k  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Modelo predice bien â†’ Accuracy 90%

Paso 2: Permutar 'UbicaciÃ³n'
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ UbicaciÃ³nâ”‚ TamaÃ±o  â”‚ Precio â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Centro   â”‚ 120 mÂ²  â”‚ $450k  â”‚
â”‚ Centro   â”‚ 80 mÂ²   â”‚ $180k  â”‚ â† Ahora 'Centro' con precio bajo
â”‚ Periferiaâ”‚ 150 mÂ²  â”‚ $520k  â”‚ â† Ahora 'Periferia' con precio alto
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Modelo confundido â†’ Accuracy 65%

Paso 3: Calcular importancia
Importancia de 'UbicaciÃ³n' = 90% - 65% = 25%
â†’ UbicaciÃ³n es MUY importante (perderla afecta mucho)

Paso 4: Permutar 'Color Fachada'
Modelo casi igual â†’ Accuracy 89%
Importancia de 'Color' = 90% - 89% = 1%
â†’ Color NO es importante
```

#### CÃ³digo con scikit-learn

```python
from sklearn.inspection import permutation_importance

# Calcular importancia por permutaciÃ³n
perm_importance = permutation_importance(
    model, X, y,
    n_repeats=10,       # Repetir 10 veces para promediar
    random_state=42,
    n_jobs=-1
)

# Obtener importancias
perm_importances = perm_importance.importances_mean

# Crear DataFrame
perm_importance_df = pd.DataFrame({
    'feature': feature_names,
    'importance': perm_importances,
    'std': perm_importance.importances_std
}).sort_values('importance', ascending=False)

print(perm_importance_df)

# VisualizaciÃ³n con barras de error
plt.figure(figsize=(10, 6))
plt.barh(perm_importance_df['feature'], perm_importance_df['importance'],
         xerr=perm_importance_df['std'])
plt.xlabel('Importancia por PermutaciÃ³n')
plt.title('Permutation Importance')
plt.tight_layout()
plt.show()
```

---

### ComparaciÃ³n de MÃ©todos de Feature Importance

| MÃ©todo | Ventajas | Desventajas | CuÃ¡ndo Usar |
|--------|----------|-------------|-------------|
| **Impureza** | RÃ¡pido, integrado en tree models | Solo para tree-based, sesgado a altas cardinalidades | Random Forest, Ã¡rboles de decisiÃ³n |
| **PermutaciÃ³n** | Model-agnostic, no sesgado | MÃ¡s lento, requiere datos de validaciÃ³n | Cualquier modelo, validaciÃ³n |

---

### Visualizaciones Avanzadas

#### 1. GrÃ¡fico de Barras Horizontal

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Configurar estilo
sns.set_style("whitegrid")

# Crear grÃ¡fico
fig, ax = plt.subplots(figsize=(10, 6))

# Barras horizontales con colores degradados
colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(importance_df)))
bars = ax.barh(importance_df['feature'], importance_df['importance'], color=colors)

# AÃ±adir valores en las barras
for i, (bar, val) in enumerate(zip(bars, importance_df['importance'])):
    ax.text(val + 0.01, bar.get_y() + bar.get_height()/2,
            f'{val:.1%}', va='center', fontweight='bold')

ax.set_xlabel('Importancia Relativa', fontsize=12)
ax.set_title('Feature Importance - PredicciÃ³n de Precios de Casas',
             fontsize=14, fontweight='bold')
ax.set_xlim(0, max(importance_df['importance']) * 1.15)

plt.tight_layout()
plt.show()
```

#### 2. GrÃ¡fico de Pastel

```python
# GrÃ¡fico de pastel para importancias
plt.figure(figsize=(10, 8))

# Colores
colors = plt.cm.Set3(range(len(importance_df)))

# Crear grÃ¡fico de pastel
wedges, texts, autotexts = plt.pie(
    importance_df['importance'],
    labels=importance_df['feature'],
    autopct='%1.1f%%',
    startangle=90,
    colors=colors,
    explode=[0.1 if i == 0 else 0 for i in range(len(importance_df))]  # Destacar el mÃ¡s importante
)

# Estilo
for text in texts:
    text.set_fontsize(12)
for autotext in autotexts:
    autotext.set_color('white')
    autotext.set_fontweight('bold')
    autotext.set_fontsize(10)

plt.title('DistribuciÃ³n de Feature Importance', fontsize=14, fontweight='bold')
plt.axis('equal')
plt.tight_layout()
plt.show()
```

#### 3. ComparaciÃ³n de MÃ©todos

```python
# Comparar Impureza vs PermutaciÃ³n
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# Impureza
ax1.barh(importance_df['feature'], importance_df['importance'], color='steelblue')
ax1.set_xlabel('Importancia')
ax1.set_title('Basada en Impureza (Tree-based)')

# PermutaciÃ³n
ax2.barh(perm_importance_df['feature'], perm_importance_df['importance'],
         xerr=perm_importance_df['std'], color='coral')
ax2.set_xlabel('Importancia')
ax2.set_title('Basada en PermutaciÃ³n')

plt.tight_layout()
plt.show()
```

---

## InterpretaciÃ³n Local: SHAP

### Â¿QuÃ© es SHAP?

**SHAP (SHapley Additive exPlanations)** explica **cada predicciÃ³n individual** mostrando la contribuciÃ³n de cada variable.

```mermaid
graph TD
    A[PredicciÃ³n Base<br/>Promedio de todos<br/>los datos] --> B[+ ContribuciÃ³n<br/>UbicaciÃ³n: +$50k]
    B --> C[+ ContribuciÃ³n<br/>TamaÃ±o: +$80k]
    C --> D[+ ContribuciÃ³n<br/>AÃ±o: +$20k]
    D --> E[- ContribuciÃ³n<br/>Habitaciones: -$10k]
    E --> F[= PredicciÃ³n Final<br/>$450k]

    style A fill:#ffffcc
    style F fill:#ccffcc
```

### Concepto Intuitivo

```
Pregunta: Â¿Por quÃ© esta casa cuesta $450k?

Respuesta SHAP:

Precio base (promedio de todas las casas): $310k
                                              â†“
+ UbicaciÃ³n es 'Centro'         â†’ +$50k     ($360k)
+ TamaÃ±o es 120mÂ² (grande)      â†’ +$80k     ($440k)
+ AÃ±o construcciÃ³n 2020 (nuevo) â†’ +$20k     ($460k)
- Solo 3 habitaciones (pocas)   â†’ -$10k     ($450k) âœ…
                                              â†“
                        PredicciÃ³n final: $450k

ConclusiÃ³n: El precio alto se debe principalmente al tamaÃ±o (+$80k)
            y ubicaciÃ³n (+$50k)
```

---

### Ejemplo Visual: ClasificaciÃ³n de PrÃ©stamos

```
Caso: Â¿Por quÃ© se APROBÃ“ el prÃ©stamo del Cliente A?

Probabilidad base de aprobaciÃ³n: 50%

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ingreso alto ($80k/aÃ±o)        â†’  +25%  (75%) â”‚
â”‚ Historial crediticio bueno     â†’  +15%  (90%) â”‚
â”‚ Empleo estable (5 aÃ±os)        â†’  +8%   (98%) â”‚
â”‚ Tiene ahorros ($20k)           â†’  +2%   (100%)â”‚
â”‚ Edad joven (28 aÃ±os)           â†’  -5%   (95%) â”‚
â”‚ Sin propiedad                  â†’  -10%  (85%) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    PredicciÃ³n: 85% APROBADO âœ…

InterpretaciÃ³n:
âœ… Ingreso alto es el factor MÃS importante (+25%)
âœ… Buen historial crediticio tambiÃ©n ayuda mucho (+15%)
âš ï¸ No tener propiedad es un factor negativo (-10%)
```

---

### InstalaciÃ³n y Uso de SHAP

```python
# Instalar SHAP
# pip install shap

import shap
import numpy as np
import matplotlib.pyplot as plt

# 1. Entrenar modelo
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Datos de ejemplo: aprobaciÃ³n de prÃ©stamos
data = {
    'ingreso': [80000, 45000, 120000, 35000, 95000, 50000],
    'historial_crediticio': [750, 620, 800, 580, 720, 640],
    'anos_empleo': [5, 2, 10, 1, 7, 3],
    'ahorros': [20000, 5000, 50000, 2000, 30000, 8000],
    'edad': [28, 45, 35, 22, 40, 50],
    'tiene_propiedad': [0, 1, 1, 0, 1, 0],
    'aprobado': [1, 0, 1, 0, 1, 0]  # 1=Aprobado, 0=Rechazado
}

df = pd.DataFrame(data)
X = df.drop('aprobado', axis=1)
y = df['aprobado']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# 2. Crear explainer de SHAP
explainer = shap.TreeExplainer(model)

# 3. Calcular SHAP values
shap_values = explainer.shap_values(X_test)

# Si el modelo devuelve probabilidades para ambas clases:
# shap_values serÃ¡ una lista [shap_values_clase0, shap_values_clase1]
# Usamos la clase positiva (aprobado = 1)
if isinstance(shap_values, list):
    shap_values = shap_values[1]

print("SHAP values shape:", shap_values.shape)
```

---

### Visualizaciones de SHAP

#### 1. Force Plot (ExplicaciÃ³n Individual)

```python
# Explicar UNA predicciÃ³n especÃ­fica (ej: primera muestra de test)
shap.initjs()  # Para visualizaciÃ³n en notebooks

# Force plot para la primera muestra
shap.force_plot(
    explainer.expected_value[1],  # Valor base
    shap_values[0, :],            # SHAP values de la primera muestra
    X_test.iloc[0, :],            # Valores de features de la primera muestra
    matplotlib=True
)

# Este grÃ¡fico muestra:
# - Base value (rojo): predicciÃ³n promedio
# - Flechas rojas: contribuciones positivas (aumentan probabilidad)
# - Flechas azules: contribuciones negativas (disminuyen probabilidad)
# - Output value: predicciÃ³n final
```

**InterpretaciÃ³n del Force Plot**:

```
Force Plot Ejemplo:

Base value: 0.50 (50% de probabilidad base)

[â†â”€â”€â”€â”€â”€â”€â”€â”€]  [â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’]
  Negativo     Positivo

Ingreso=80k              â”€â”€â”€â”€â†’  +0.25
Historial=750           â”€â”€â”€â”€â”€â†’  +0.15
Empleo=5aÃ±os            â”€â”€â”€â†’    +0.08
Tiene_propiedad=No  â†â”€â”€         -0.10
Edad=28             â†â”€          -0.05
Ahorros=20k              â”€â”€â†’    +0.02
                                ------
                         Output: 0.85 (85% probabilidad)

InterpretaciÃ³n:
- Ingreso alto (+0.25) es el factor MÃS importante a favor
- No tener propiedad (-0.10) es el principal factor en contra
```

---

#### 2. Waterfall Plot (Cascada)

```python
# Waterfall plot para una muestra
shap.plots.waterfall(
    shap.Explanation(
        values=shap_values[0],
        base_values=explainer.expected_value[1],
        data=X_test.iloc[0],
        feature_names=X_test.columns.tolist()
    )
)
```

**VisualizaciÃ³n de Waterfall**:

```
Waterfall Plot (Cascada):

Probabilidad
    1.0 â”¤
        â”‚                                    â”Œâ”€â”€â”€â”€â”€â”
        â”‚                               â”Œâ”€â”€â”€â”€â”¤ 0.85â”‚ â† PredicciÃ³n final
    0.8 â”¤                          â”Œâ”€â”€â”€â”€â”¤    â””â”€â”€â”€â”€â”€â”˜
        â”‚                     â”Œâ”€â”€â”€â”€â”¤ 0.73
        â”‚                â”Œâ”€â”€â”€â”€â”¤ 0.65
    0.6 â”¤           â”Œâ”€â”€â”€â”€â”¤ 0.58
        â”‚      â”Œâ”€â”€â”€â”€â”¤ 0.50 (base)
    0.4 â”¤ â”Œâ”€â”€â”€â”€â”¤ 0.45
        â”œâ”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â†’
          Edad Prop. Ahorro Empleo Hist. Ingreso
          -0.05 -0.10 +0.02 +0.08 +0.15 +0.25

Cada barra muestra cÃ³mo la contribuciÃ³n de cada feature
va construyendo la predicciÃ³n final paso a paso.
```

---

#### 3. Summary Plot (Resumen Global)

```python
# Summary plot (muestra importancia Y efecto para TODAS las muestras)
shap.summary_plot(shap_values, X_test, plot_type="dot")
```

**InterpretaciÃ³n del Summary Plot**:

```
Summary Plot (Diagrama de Puntos):

Features (ordenadas por importancia):

Ingreso           â—â—â—â—â—â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹  â† Valores altos (rojo) â†’ efecto positivo
                  â—‹â—‹â—‹â—‹â—‹â—â—â—        Valores bajos (azul) â†’ efecto negativo

Historial         â—â—â—â—â—‹â—‹â—‹â—‹â—‹â—‹
                  â—‹â—‹â—‹â—â—â—

Empleo            â—â—â—â—‹â—‹â—‹
                  â—‹â—â—

Ahorros           â—â—â—‹â—‹
                  â—‹â—

Edad              â—â—â—‹
                  â—‹â—

Propiedad         â—â—‹
                  â—‹

      â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â†’
      -0.3  -0.2  -0.1   0   0.1  0.2  0.3
           SHAP value (impacto en predicciÃ³n)

InterpretaciÃ³n:
â— Puntos rojos = valores altos de la feature
â—‹ Puntos azules = valores bajos de la feature

Ingreso: Valores altos (rojo) estÃ¡n a la derecha (efecto positivo)
         Valores bajos (azul) estÃ¡n a la izquierda (efecto negativo)
```

---

#### 4. Dependence Plot (RelaciÃ³n con una Variable)

```python
# Dependence plot: muestra la relaciÃ³n entre una feature y su SHAP value
shap.dependence_plot("ingreso", shap_values, X_test)
```

**InterpretaciÃ³n del Dependence Plot**:

```
Dependence Plot (Ingreso vs SHAP Value):

SHAP Value
   0.3 â”¤                        â—â—â—
       â”‚                    â—â—â—â—
   0.2 â”¤                â—â—â—â—
       â”‚            â—â—â—â—
   0.1 â”¤        â—â—â—â—
       â”‚    â—â—â—â—
   0.0 â”¼â—â—â—â—
       â”‚
  -0.1 â”¤
       â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â†’
        $20k  $40k  $60k  $80k $100k $120k
                    Ingreso

InterpretaciÃ³n:
- RelaciÃ³n POSITIVA: Mayor ingreso â†’ Mayor SHAP value (mayor prob. aprobaciÃ³n)
- La relaciÃ³n es casi lineal
- A partir de $80k, el efecto se estabiliza
```

---

#### 5. Bar Plot (Importancia Global)

```python
# Bar plot: importancia promedio absoluta de cada feature
shap.summary_plot(shap_values, X_test, plot_type="bar")
```

**VisualizaciÃ³n**:

```
Bar Plot (Importancia Promedio):

Ingreso           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.18
Historial         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.12
Empleo            â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.08
Propiedad         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.06
Ahorros           â–ˆâ–ˆâ–ˆ 0.04
Edad              â–ˆâ–ˆ 0.03
                  â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â†’
                   0   0.05  0.10  0.15  0.20
                   mean(|SHAP value|)

Muestra la importancia PROMEDIO de cada variable
(promedio del valor absoluto de SHAP values)
```

---

### SHAP para Diferentes Modelos

```python
# Random Forest / Tree-based
explainer = shap.TreeExplainer(model)

# Linear Models
explainer = shap.LinearExplainer(model, X_train)

# Neural Networks / Cualquier modelo
explainer = shap.KernelExplainer(model.predict, X_train)

# Deep Learning (PyTorch, TensorFlow)
explainer = shap.DeepExplainer(model, X_train)
```

---

## InterpretaciÃ³n Local: LIME

### Â¿QuÃ© es LIME?

**LIME (Local Interpretable Model-agnostic Explanations)** crea un **modelo simple** (lineal) alrededor de una predicciÃ³n para explicarla.

```mermaid
graph TD
    A[Modelo complejo<br/>Caja negra] --> B[PredicciÃ³n para<br/>muestra X]

    C[LIME] --> D[Generar muestras<br/>similares a X]
    D --> E[Predecir con<br/>modelo complejo]
    E --> F[Entrenar modelo<br/>lineal simple]
    F --> G[ExplicaciÃ³n:<br/>Coeficientes lineales]

    style A fill:#333,color:#fff
    style G fill:#ccffcc
```

### Concepto Intuitivo

```
Objetivo: Explicar una predicciÃ³n especÃ­fica

Paso 1: Modelo complejo (Random Forest) predice
        Casa con ubicaciÃ³n='Centro', tamaÃ±o=120mÂ² â†’ Precio: $450k

Paso 2: LIME genera variaciones de esta casa
        - Centro, 115mÂ² â†’ Â¿Precio?
        - Centro, 125mÂ² â†’ Â¿Precio?
        - Periferia, 120mÂ² â†’ Â¿Precio?
        - Centro, 110mÂ² â†’ Â¿Precio?
        ... (miles de variaciones)

Paso 3: LIME predice cada variaciÃ³n con el modelo complejo

Paso 4: LIME ajusta modelo lineal simple:
        Precio â‰ˆ 200k + (100k Ã— ubicaciÃ³n_centro) + (2k Ã— cada_mÂ²)

Paso 5: ExplicaciÃ³n simple:
        "Para esta casa, cada mÂ² adicional aÃ±ade $2k al precio,
         y estar en el centro aÃ±ade $100k"
```

---

### InstalaciÃ³n y Uso de LIME

```python
# Instalar LIME
# pip install lime

import lime
import lime.lime_tabular
import numpy as np

# 1. Crear explainer de LIME
explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['Rechazado', 'Aprobado'],
    mode='classification'
)

# 2. Explicar UNA predicciÃ³n
idx = 0  # Primera muestra de test
exp = explainer.explain_instance(
    data_row=X_test.iloc[idx].values,
    predict_fn=model.predict_proba,
    num_features=6  # Mostrar top 6 features
)

# 3. Visualizar explicaciÃ³n
exp.show_in_notebook(show_table=True)

# 4. Como figura matplotlib
fig = exp.as_pyplot_figure()
plt.tight_layout()
plt.show()

# 5. Obtener explicaciÃ³n como lista
print(exp.as_list())
# Ejemplo de salida:
# [('ingreso > 70000', 0.25),
#  ('historial_crediticio > 700', 0.18),
#  ('tiene_propiedad = 0', -0.12),
#  ...]
```

---

### VisualizaciÃ³n de LIME

#### GrÃ¡fico de Barras LIME

```
ExplicaciÃ³n LIME para Cliente A (PredicciÃ³n: 85% Aprobado):

ContribuciÃ³n a "Aprobado":

Ingreso > $70k           â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ +0.25
Historial > 700          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ +0.18
AÃ±os empleo > 4          â–ˆâ–ˆâ–ˆâ–ˆ +0.08
Ahorros > $15k           â–ˆâ–ˆ +0.05

ContribuciÃ³n a "Rechazado":

No tiene propiedad       â–ˆâ–ˆâ–ˆâ–ˆ -0.12
Edad < 30                â–ˆâ–ˆ -0.05

                    â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â†’
                   -0.2  -0.1   0   0.1  0.2  0.3

InterpretaciÃ³n:
- El factor MÃS importante es el ingreso alto (+0.25)
- No tener propiedad es el principal factor negativo (-0.12)
- En balance: 85% de probabilidad de aprobaciÃ³n
```

---

### Ejemplo Completo: ClasificaciÃ³n de Texto

```python
from lime.lime_text import LimeTextExplainer
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Datos de ejemplo
texts = [
    "Me encantÃ³ la pelÃ­cula, excelente actuaciÃ³n",
    "PelÃ­cula horrible, muy aburrida",
    "Obra maestra del cine",
    "No la recomiendo, muy mala"
]
labels = [1, 0, 1, 0]  # 1=Positivo, 0=Negativo

# Pipeline
pipeline = make_pipeline(
    TfidfVectorizer(),
    LogisticRegression()
)
pipeline.fit(texts, labels)

# LIME para texto
text_explainer = LimeTextExplainer(class_names=['Negativo', 'Positivo'])

# Explicar una predicciÃ³n
text_to_explain = "La pelÃ­cula estuvo excelente, muy buena"
exp = text_explainer.explain_instance(
    text_to_explain,
    pipeline.predict_proba,
    num_features=6
)

# Visualizar
print("PredicciÃ³n:", pipeline.predict([text_to_explain])[0])
print("Probabilidad Positivo:", pipeline.predict_proba([text_to_explain])[0][1])

exp.show_in_notebook(text=True)

# Lista de palabras importantes
print("\nPalabras que mÃ¡s contribuyen:")
print(exp.as_list())
```

**InterpretaciÃ³n del Output**:

```
Texto: "La pelÃ­cula estuvo excelente, muy buena"
PredicciÃ³n: Positivo (92%)

Palabras que contribuyen a POSITIVO:
  excelente   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ +0.35
  buena       â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ +0.25
  pelÃ­cula    â–ˆâ–ˆ +0.08

Palabras que contribuyen a NEGATIVO:
  (ninguna significativa)

VisualizaciÃ³n del texto:
"La pelÃ­cula estuvo EXCELENTE, muy BUENA"
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”˜
        +0.08          +0.35     +0.25

InterpretaciÃ³n:
- "excelente" es la palabra MÃS importante (+0.35)
- "buena" tambiÃ©n contribuye fuertemente (+0.25)
- El modelo detectÃ³ correctamente el sentimiento positivo
```

---

## ComparaciÃ³n de MÃ©todos

### Tabla Comparativa Completa

| Aspecto | Feature Importance | SHAP | LIME |
|---------|-------------------|------|------|
| **Nivel** | Global | Global + Local | Local |
| **Â¿QuÃ© explica?** | Importancia general | ContribuciÃ³n exacta por muestra | AproximaciÃ³n local |
| **PrecisiÃ³n** | Aproximada | MatemÃ¡ticamente exacta | AproximaciÃ³n |
| **Velocidad** | âš¡âš¡âš¡ Muy rÃ¡pido | âš¡âš¡ Medio | âš¡ Lento |
| **Modelos** | Solo tree-based | Todos (con explainers especÃ­ficos) | Todos (model-agnostic) |
| **Consistencia** | Variable | Alta (teorÃ­a de juegos) | Variable |
| **Facilidad** | ğŸŸ¢ Muy fÃ¡cil | ğŸŸ¡ Media | ğŸŸ¡ Media |
| **Visualizaciones** | Barras simples | MÃºltiples (force, waterfall, etc.) | Barras + texto |

---

### Diagrama de DecisiÃ³n

```mermaid
graph TD
    A{Â¿QuÃ© quieres<br/>entender?} -->|Importancia<br/>general| B[Feature<br/>Importance]
    A -->|Explicar<br/>predicciones| C{Â¿QuÃ© tipo<br/>de modelo?}

    C -->|Tree-based| D[SHAP<br/>TreeExplainer]
    C -->|Otro modelo| E{Â¿Velocidad<br/>o precisiÃ³n?}

    E -->|PrecisiÃ³n| F[SHAP<br/>KernelExplainer]
    E -->|Velocidad| G[LIME]

    H{Â¿Nivel de<br/>detalle?} -->|Global| I[SHAP<br/>Summary Plot]
    H -->|Individual| J[SHAP Force Plot<br/>o LIME]

    style B fill:#ffffcc
    style D fill:#ccffcc
    style F fill:#cce5ff
    style G fill:#ffcccc
```

### Ejemplo de Uso Combinado

```python
# Flujo completo de interpretabilidad

# 1. Feature Importance (visiÃ³n global rÃ¡pida)
importances = model.feature_importances_
print("Top 3 features:", X.columns[np.argsort(importances)[-3:]])

# 2. SHAP Global (entender direcciones)
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values[1], X_test)

# 3. SHAP Local (explicar predicciones especÃ­ficas)
# Casos interesantes: falsos positivos, falsos negativos
false_positives = X_test[(y_test == 0) & (model.predict(X_test) == 1)]
if len(false_positives) > 0:
    shap.force_plot(
        explainer.expected_value[1],
        shap_values[1][0],
        false_positives.iloc[0]
    )

# 4. LIME (para stakeholders no tÃ©cnicos)
lime_explainer = lime.lime_tabular.LimeTabularExplainer(
    X_train.values,
    feature_names=X_train.columns.tolist(),
    class_names=['Rechazado', 'Aprobado'],
    mode='classification'
)

exp = lime_explainer.explain_instance(
    false_positives.iloc[0].values,
    model.predict_proba,
    num_features=5
)
exp.show_in_notebook()
```

---

## Casos de Uso PrÃ¡cticos

### 1. DiagnÃ³stico MÃ©dico

```python
"""
Caso: PredicciÃ³n de riesgo de diabetes

Variables:
- Edad
- IMC (Ãndice de Masa Corporal)
- Glucosa en sangre
- PresiÃ³n arterial
- Historial familiar
"""

# Feature Importance
importances = model.feature_importances_
# Resultado: Glucosa (45%), IMC (30%), Edad (15%), ...

# SHAP para paciente especÃ­fico
shap.force_plot(
    explainer.expected_value[1],
    shap_values_paciente,
    datos_paciente
)

"""
InterpretaciÃ³n para Paciente X:
- Riesgo: 78%
- Factores principales:
  * Glucosa elevada (120 mg/dL) â†’ +25%
  * IMC alto (32) â†’ +18%
  * Edad avanzada (65 aÃ±os) â†’ +10%

RecomendaciÃ³n mÃ©dica:
â†’ Enfocarse en controlar glucosa (mayor impacto)
â†’ Plan de reducciÃ³n de peso
â†’ Monitoreo regular por edad
"""
```

---

### 2. AprobaciÃ³n de CrÃ©ditos

```python
"""
Caso: Explicar rechazo de prÃ©stamo

Cliente rechazado con 35% de probabilidad de aprobaciÃ³n
"""

# LIME explanation
exp = lime_explainer.explain_instance(
    cliente_datos,
    model.predict_proba,
    num_features=6
)

"""
ExplicaciÃ³n para el cliente:

Su solicitud fue RECHAZADA porque:

Factores negativos:
âŒ Historial crediticio bajo (580) â†’ -30%
âŒ Ingresos insuficientes ($35k/aÃ±o) â†’ -20%
âŒ Sin propiedad â†’ -15%

Factores positivos:
âœ… AÃ±os de empleo (5 aÃ±os) â†’ +10%
âœ… Ahorros ($8k) â†’ +5%

RecomendaciÃ³n:
Para mejorar sus posibilidades:
1. Mejorar historial crediticio (mayor impacto)
2. Aumentar ingresos o solicitar monto menor
3. Considerar co-solicitante
"""
```

---

### 3. Marketing: PredicciÃ³n de Churn

```python
"""
Caso: Identificar por quÃ© clientes abandonan

AnÃ¡lisis global + local
"""

# 1. Feature Importance Global
print("Factores que mÃ¡s influyen en el churn:")
# Resultado:
# - Frecuencia de uso: 35%
# - Precio vs competencia: 25%
# - SatisfacciÃ³n customer service: 20%
# - AntigÃ¼edad: 15%
# - CaracterÃ­sticas premium: 5%

# 2. SHAP para cliente en riesgo
cliente_riesgo = clientes[clientes['prob_churn'] > 0.8].iloc[0]

shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[idx],
        base_values=explainer.expected_value[1],
        data=cliente_riesgo
    )
)

"""
Cliente ID: 12345
Probabilidad de abandono: 85%

Causas principales:
1. Uso bajo (2 veces/mes) â†’ +30%
   â†’ AcciÃ³n: Email con nuevas features

2. Precio 20% mÃ¡s alto que competencia â†’ +25%
   â†’ AcciÃ³n: Oferta descuento personalizado

3. Ticket de soporte sin resolver â†’ +15%
   â†’ AcciÃ³n: Contacto prioritario de CS

4. No usa features premium â†’ +10%
   â†’ AcciÃ³n: Tutorial personalizado

Estrategia de retenciÃ³n personalizada basada en SHAP
"""
```

---

### 4. Recursos Humanos: PredicciÃ³n de Renuncia

```python
"""
Caso: Predecir quÃ© empleados renunciarÃ¡n

InterpretaciÃ³n para HR managers
"""

# Feature Importance
feature_importances = model.feature_importances_

"""
Factores de renuncia mÃ¡s importantes:

1. SatisfacciÃ³n laboral (40%)
   â†’ Focus en surveys y clima laboral

2. Salario relativo al mercado (30%)
   â†’ Benchmarking y ajustes

3. Tiempo sin promociÃ³n (20%)
   â†’ Plan de carrera

4. Balance vida-trabajo (10%)
   â†’ PolÃ­ticas de flexibilidad
"""

# SHAP para empleado en riesgo
empleado_riesgo = empleados[empleados['prob_renuncia'] > 0.7].iloc[0]

"""
Empleado: Juan PÃ©rez
Probabilidad de renuncia: 82%

AnÃ¡lisis SHAP:

Factores de riesgo:
âŒ SatisfacciÃ³n baja (3/10) â†’ +35%
âŒ Salario 15% bajo vs mercado â†’ +25%
âŒ 4 aÃ±os sin promociÃ³n â†’ +18%
âŒ Horas extra frecuentes â†’ +10%

Factores de retenciÃ³n:
âœ… Buen ambiente con equipo â†’ -5%
âœ… Beneficios competitivos â†’ -3%

Plan de acciÃ³n personalizado:
1. [URGENTE] ConversaciÃ³n salarial
2. [ALTA] Discutir plan de carrera
3. [MEDIA] Revisar carga de trabajo
4. [BAJA] Mantener ambiente de equipo
"""
```

---

## Mejores PrÃ¡cticas

### 1. Combina MÃ©todos

```python
# âŒ MAL: Usar solo un mÃ©todo
importances = model.feature_importances_
print("Top feature:", X.columns[np.argmax(importances)])
# ConclusiÃ³n: Solo una vista limitada

# âœ… BIEN: Usar mÃºltiples mÃ©todos
# Paso 1: Global overview
feature_importance_analysis()

# Paso 2: Validar con SHAP
shap_global_analysis()

# Paso 3: Explicar casos especÃ­ficos
shap_local_analysis(casos_interesantes)
lime_analysis(casos_para_stakeholders)
```

### 2. Valida las Explicaciones

```python
# Verificar consistencia
# Â¿SHAP y LIME dan explicaciones similares?

# Para la misma muestra:
lime_exp = lime_explainer.explain_instance(...)
shap_exp = shap_explainer.shap_values(...)

# Comparar top features
lime_top = [f for f, _ in lime_exp.as_list()[:3]]
shap_top = X.columns[np.argsort(np.abs(shap_exp))[-3:]]

print("LIME top 3:", lime_top)
print("SHAP top 3:", shap_top)
# DeberÃ­an ser similares si el modelo es estable
```

### 3. Documenta y Comunica

```python
# Crear reporte de interpretabilidad

report = {
    'model_type': 'Random Forest',
    'accuracy': 0.87,

    # InterpretaciÃ³n global
    'top_features': {
        'feature_1': 0.35,
        'feature_2': 0.28,
        'feature_3': 0.20
    },

    # Interpretaciones locales de casos clave
    'false_positives_analysis': [...],
    'false_negatives_analysis': [...],

    # Recomendaciones
    'actionable_insights': [
        "Feature X es el factor mÃ¡s importante",
        "Mejorar calidad de datos en Feature Y",
        "Considerar feature engineering para..."
    ]
}
```

### 4. Sanity Checks

```python
# Verificaciones de cordura

# 1. Â¿Las importancias suman ~1?
assert abs(sum(model.feature_importances_) - 1.0) < 0.01

# 2. Â¿Las features importantes tienen sentido de negocio?
top_features = X.columns[np.argsort(importances)[-3:]]
# Validar con expertos del dominio

# 3. Â¿SHAP values reconstruyen la predicciÃ³n?
prediction = model.predict_proba(X_test[0].reshape(1, -1))[0, 1]
shap_prediction = explainer.expected_value[1] + sum(shap_values[0])
assert abs(prediction - shap_prediction) < 0.01

# 4. Â¿Explicaciones estables?
# PequeÃ±os cambios en datos â†’ pequeÃ±os cambios en explicaciÃ³n
```

---

## Resumen Visual

```mermaid
mindmap
  root((Interpretabilidad<br/>ML))
    Global
      Feature Importance
        Impureza
        PermutaciÃ³n
      SHAP Global
        Summary Plot
        Bar Plot
      Uso
        Entender modelo completo
        Identificar variables clave
        Debugging
    Local
      SHAP Local
        Force Plot
        Waterfall Plot
        Dependence Plot
      LIME
        AproximaciÃ³n local
        Model-agnostic
      Uso
        Explicar predicciones
        Casos individuales
        Comunicar a stakeholders
```

### Flujo de Trabajo Recomendado

```mermaid
graph TD
    A[Entrenar modelo] --> B[1. Feature Importance<br/>Vista rÃ¡pida]
    B --> C[2. SHAP Global<br/>AnÃ¡lisis profundo]
    C --> D{Â¿Patrones<br/>sospechosos?}
    D -->|SÃ­| E[Revisar datos<br/>y features]
    D -->|No| F[3. SHAP Local<br/>Casos clave]
    F --> G[4. LIME<br/>Para stakeholders]
    G --> H[5. Documentar<br/>insights]
    H --> I[âœ… Modelo<br/>interpretado]

    E --> B

    style I fill:#ccffcc
```

---

## Referencias

### LibrerÃ­as

1. **SHAP GitHub**: https://github.com/slundberg/shap
2. **SHAP Documentation**: https://shap.readthedocs.io/
3. **LIME GitHub**: https://github.com/marcotcr/lime
4. **LIME Paper**: "Why Should I Trust You?" Ribeiro et al., 2016
5. **scikit-learn Inspection**: https://scikit-learn.org/stable/modules/inspection.html

### ArtÃ­culos y Papers

6. **"A Unified Approach to Interpreting Model Predictions"** - Lundberg & Lee, 2017 (SHAP)
7. **"Interpretable Machine Learning"** - Christoph Molnar (libro online gratuito)
8. **"Stop Explaining Black Box Machine Learning Models"** - Rudin, 2019

### Tutoriales

9. **SHAP Tutorial**: https://shap.readthedocs.io/en/latest/example_notebooks/
10. **LIME Tutorial**: https://marcotcr.github.io/lime/tutorials/
11. **Feature Importance in scikit-learn**: https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html

### Herramientas Adicionales

12. **InterpretML**: https://interpret.ml/
13. **ELI5**: https://eli5.readthedocs.io/
14. **Yellowbrick**: https://www.scikit-yb.org/

---

**Licencia**: MIT License
**Autor**: David Palacio JimÃ©nez
**Fecha**: 2025
**VersiÃ³n**: 1.0
