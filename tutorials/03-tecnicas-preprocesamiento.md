# Tutorial: TÃ©cnicas de Preprocesamiento en Machine Learning

## Ãndice

1. [IntroducciÃ³n](#introducciÃ³n)
2. [ImputaciÃ³n de Valores Faltantes](#imputaciÃ³n-de-valores-faltantes)
3. [CodificaciÃ³n de Variables CategÃ³ricas](#codificaciÃ³n-de-variables-categÃ³ricas)
4. [Escalado y NormalizaciÃ³n](#escalado-y-normalizaciÃ³n)
5. [ReducciÃ³n de Dimensionalidad: PCA](#reducciÃ³n-de-dimensionalidad-pca)
6. [Feature Engineering](#feature-engineering)
7. [Preprocesamiento de Texto](#preprocesamiento-de-texto)
8. [Preprocesamiento de ImÃ¡genes](#preprocesamiento-de-imÃ¡genes)
9. [Pipeline Completo](#pipeline-completo)
10. [Referencias](#referencias)

---

## IntroducciÃ³n

El **preprocesamiento de datos** es un paso fundamental en cualquier proyecto de machine learning. Los datos del mundo real rara vez vienen en el formato ideal para entrenar modelos: pueden tener valores faltantes, diferentes escalas, categorÃ­as en forma de texto, y mÃ¡s.

### Â¿Por quÃ© es importante?

```mermaid
graph LR
    A[Datos Crudos] -->|âŒ Sin Preprocesar| B[Modelo ML]
    B -->|Problemas| C[Bajo Rendimiento<br/>Errores<br/>Sesgos]

    A -->|âœ… Con Preprocesamiento| D[Datos Limpios]
    D --> E[Modelo ML]
    E --> F[Alto Rendimiento<br/>Predicciones Precisas<br/>GeneralizaciÃ³n]

    style A fill:#ffcccc
    style D fill:#ccffcc
    style C fill:#ffaaaa
    style F fill:#aaffaa
```

### Flujo TÃ­pico de Preprocesamiento

```mermaid
flowchart TD
    A[ğŸ“Š Datos Crudos] --> B{Â¿Valores<br/>Faltantes?}
    B -->|SÃ­| C[ImputaciÃ³n]
    B -->|No| D{Â¿Variables<br/>CategÃ³ricas?}
    C --> D
    D -->|SÃ­| E[CodificaciÃ³n]
    D -->|No| F{Â¿Diferentes<br/>Escalas?}
    E --> F
    F -->|SÃ­| G[Escalado/<br/>NormalizaciÃ³n]
    F -->|No| H{Â¿Muchas<br/>Dimensiones?}
    G --> H
    H -->|SÃ­| I[ReducciÃ³n<br/>Dimensionalidad]
    H -->|No| J[âœ… Datos Listos]
    I --> J
    J --> K[ğŸ¤– Entrenamiento<br/>del Modelo]
```

---

## ImputaciÃ³n de Valores Faltantes

### Â¿QuÃ© son los valores faltantes?

Los valores faltantes (NaN, None, null) son datos ausentes en nuestro dataset. Pueden aparecer por:
- Errores de recolecciÃ³n
- Datos no aplicables
- Respuestas omitidas en encuestas
- Fallas en sensores

### Estrategias de ImputaciÃ³n

```mermaid
graph TD
    A[Valores Faltantes] --> B[ImputaciÃ³n<br/>Univariada]
    A --> C[ImputaciÃ³n<br/>Multivariada]
    A --> D[Indicadores de<br/>Valores Faltantes]

    B --> B1[SimpleImputer]
    C --> C1[KNNImputer]
    C --> C2[IterativeImputer]
    D --> D1[MissingIndicator]

    style A fill:#ffcccc
    style B fill:#cce5ff
    style C fill:#ccffcc
    style D fill:#ffffcc
```

---

### 1. SimpleImputer (ImputaciÃ³n Univariada)

**Concepto**: Reemplaza valores faltantes usando estadÃ­sticas de **una sola columna** (media, mediana, moda, o valor constante).

#### Estrategias Disponibles

| Estrategia | Tipo de Datos | DescripciÃ³n | CuÃ¡ndo Usarla |
|------------|---------------|-------------|---------------|
| **`mean`** | NumÃ©ricos | Promedio de la columna | Datos con distribuciÃ³n normal, sin outliers |
| **`median`** | NumÃ©ricos | Valor central de la columna | Datos con outliers o distribuciÃ³n sesgada |
| **`most_frequent`** | NumÃ©ricos o CategÃ³ricos | Valor mÃ¡s comÃºn (moda) | Variables categÃ³ricas o discretas |
| **`constant`** | Cualquiera | Valor fijo especificado | Cuando 0 o un valor especÃ­fico tiene sentido |

#### Ejemplo Visual

```
Dataset Original:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Age â”‚ Sex â”‚ Embarked â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 22  â”‚ M   â”‚ S        â”‚
â”‚ NaN â”‚ F   â”‚ C        â”‚
â”‚ 35  â”‚ F   â”‚ NaN      â”‚
â”‚ 45  â”‚ M   â”‚ S        â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DespuÃ©s de SimpleImputer:
- Age: NaN â†’ 34 (median)
- Embarked: NaN â†’ 'S' (most_frequent)

â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Age â”‚ Sex â”‚ Embarked â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 22  â”‚ M   â”‚ S        â”‚
â”‚ 34  â”‚ F   â”‚ C        â”‚
â”‚ 35  â”‚ F   â”‚ S        â”‚
â”‚ 45  â”‚ M   â”‚ S        â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### CÃ³digo con scikit-learn

```python
from sklearn.impute import SimpleImputer
import numpy as np
import pandas as pd

# Datos con valores faltantes
X = np.array([[1, 2], [np.nan, 3], [7, 6]])

# ImputaciÃ³n con la MEDIA
imputer_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
X_imputed = imputer_mean.fit_transform(X)
# Resultado: [[1, 2], [4, 3], [7, 6]]  â† El NaN se reemplaza por 4 (media de 1 y 7)

# ImputaciÃ³n con la MEDIANA
imputer_median = SimpleImputer(strategy='median')
X_imputed = imputer_median.fit_transform(X)

# ImputaciÃ³n con VALOR CONSTANTE
imputer_const = SimpleImputer(strategy='constant', fill_value=0)
X_imputed = imputer_const.fit_transform(X)

# Para datos categÃ³ricos
df = pd.DataFrame([["a", "x"],
                   [np.nan, "y"],
                   ["a", np.nan],
                   ["b", "y"]], dtype="category")

imputer_cat = SimpleImputer(strategy="most_frequent")
df_imputed = imputer_cat.fit_transform(df)
# NaN se reemplaza con 'a' (mÃ¡s frecuente en col 1) y 'y' (mÃ¡s frecuente en col 2)
```

#### Ventajas y Desventajas

| âœ… Ventajas | âŒ Desventajas |
|------------|----------------|
| RÃ¡pido y simple | No considera relaciones entre variables |
| Funciona bien con pocos valores faltantes | Puede reducir la varianza |
| No requiere muchos datos | Puede introducir sesgo |
| Compatible con sparse matrices | No captura patrones complejos |

---

### 2. KNNImputer (K-Nearest Neighbors)

**Concepto**: Imputa valores faltantes usando el **promedio de los k vecinos mÃ¡s cercanos**. Considera **mÃºltiples columnas** para encontrar similitudes.

#### Â¿CÃ³mo Funciona?

```mermaid
graph TD
    A[Muestra con<br/>valor faltante] --> B[Calcular distancias<br/>a todas las muestras]
    B --> C[Seleccionar k<br/>vecinos mÃ¡s cercanos]
    C --> D[Promediar valores<br/>de esos vecinos]
    D --> E[Imputar el<br/>valor faltante]

    style A fill:#ffcccc
    style E fill:#ccffcc
```

#### Ejemplo Visual

```
Dataset:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”
â”‚ Age â”‚ Fare â”‚ Sex  â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ 22  â”‚ 7.25 â”‚ M    â”‚  â† Vecino 1
â”‚ 25  â”‚ 8.05 â”‚ F    â”‚  â† Vecino 2
â”‚ NaN â”‚ 7.50 â”‚ M    â”‚  â† Muestra a imputar
â”‚ 50  â”‚ 30.0 â”‚ M    â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

KNNImputer (k=2):
1. Encuentra las 2 muestras mÃ¡s similares (por Fare y Sex)
2. Vecinos: Muestra 1 (Age=22) y Muestra 2 (Age=25)
3. Imputa: Age = (22 + 25) / 2 = 23.5
```

#### CÃ³digo con scikit-learn

```python
from sklearn.impute import KNNImputer
import numpy as np

X = np.array([[1, 2, np.nan],
              [3, 4, 3],
              [np.nan, 6, 5],
              [8, 8, 7]])

# ImputaciÃ³n con k=2 vecinos mÃ¡s cercanos
imputer = KNNImputer(n_neighbors=2, weights="uniform")
X_imputed = imputer.fit_transform(X)

# TambiÃ©n puedes usar weights="distance" para dar mÃ¡s peso a vecinos cercanos
imputer_weighted = KNNImputer(n_neighbors=2, weights="distance")
X_imputed_weighted = imputer_weighted.fit_transform(X)
```

#### ParÃ¡metros Importantes

| ParÃ¡metro | DescripciÃ³n | Valores Comunes |
|-----------|-------------|-----------------|
| `n_neighbors` | NÃºmero de vecinos a considerar | 3, 5, 7 (impar preferible) |
| `weights` | Peso de los vecinos | `"uniform"` o `"distance"` |
| `metric` | MÃ©trica de distancia | `"euclidean"`, `"manhattan"` |

#### Ventajas y Desventajas

| âœ… Ventajas | âŒ Desventajas |
|------------|----------------|
| Considera relaciones entre variables | MÃ¡s lento que SimpleImputer |
| MÃ¡s preciso que mÃ©todos univariados | Sensible a la escala de los datos |
| Funciona bien con datos continuos | Requiere que k < nÃºmero de muestras |
| No reduce tanto la varianza | No funciona bien con alta dimensionalidad |

---

### 3. IterativeImputer (ImputaciÃ³n Iterativa Multivariada)

**Concepto**: Modela cada variable con valores faltantes como una **funciÃ³n de las demÃ¡s variables**. Usa un modelo de regresiÃ³n para predecir los valores faltantes de forma iterativa.

**InspiraciÃ³n**: Algoritmo MICE (Multivariate Imputation by Chained Equations).

#### Â¿CÃ³mo Funciona?

```mermaid
graph TD
    A[Inicializar con<br/>SimpleImputer] --> B[Seleccionar columna<br/>con valores faltantes]
    B --> C[Entrenar modelo de<br/>regresiÃ³n con otras columnas]
    C --> D[Predecir valores<br/>faltantes]
    D --> E{Â¿MÃ¡s columnas<br/>con NaN?}
    E -->|SÃ­| B
    E -->|No| F{Â¿ConvergiÃ³?}
    F -->|No| B
    F -->|SÃ­| G[âœ… ImputaciÃ³n<br/>completa]

    style A fill:#ffffcc
    style G fill:#ccffcc
```

#### Ejemplo Conceptual

```
IteraciÃ³n 1:
- Columna 'Age' tiene NaN
- Usa 'Fare' y 'Pclass' para predecir Age
- Age_predicted = f(Fare, Pclass)

IteraciÃ³n 2:
- Columna 'Fare' tiene NaN
- Usa 'Age' (ya imputada) y 'Pclass' para predecir Fare
- Fare_predicted = g(Age, Pclass)

IteraciÃ³n 3:
- Refina 'Age' con el nuevo 'Fare' imputado
- Age_refined = f(Fare_new, Pclass)

... Repite hasta convergencia
```

#### CÃ³digo con scikit-learn

```python
from sklearn.experimental import enable_iterative_imputer  # Â¡Necesario!
from sklearn.impute import IterativeImputer
import numpy as np

X = np.array([[1, 2],
              [3, 6],
              [4, 8],
              [np.nan, 3],
              [7, np.nan]])

# ImputaciÃ³n iterativa con 10 iteraciones mÃ¡ximas
imputer = IterativeImputer(max_iter=10, random_state=0)
X_imputed = imputer.fit_transform(X)

# Puedes especificar el estimador (modelo de regresiÃ³n)
from sklearn.ensemble import RandomForestRegressor

imputer_rf = IterativeImputer(
    estimator=RandomForestRegressor(n_estimators=10, random_state=0),
    max_iter=10,
    random_state=0
)
X_imputed_rf = imputer_rf.fit_transform(X)
```

#### ParÃ¡metros Importantes

| ParÃ¡metro | DescripciÃ³n | Valores Comunes |
|-----------|-------------|-----------------|
| `estimator` | Modelo de regresiÃ³n a usar | `BayesianRidge` (default), `RandomForest`, `LinearRegression` |
| `max_iter` | NÃºmero mÃ¡ximo de iteraciones | 10, 20 |
| `tol` | Tolerancia para convergencia | 1e-3 (default) |
| `initial_strategy` | ImputaciÃ³n inicial | `'mean'`, `'median'`, `'most_frequent'` |
| `imputation_order` | Orden de imputaciÃ³n | `'ascending'`, `'descending'`, `'random'` |

#### Ventajas y Desventajas

| âœ… Ventajas | âŒ Desventajas |
|------------|----------------|
| MÃ¡s sofisticado y preciso | **Muy lento** |
| Captura relaciones complejas | Requiere convergencia |
| Flexible (puedes elegir el modelo) | API experimental (puede cambiar) |
| Reduce menos la varianza | Complejo de ajustar |

---

### 4. MissingIndicator (Indicador de Valores Faltantes)

**Concepto**: No imputa valores, sino que **crea columnas binarias** indicando dÃ³nde habÃ­a valores faltantes. Ãštil para preservar informaciÃ³n sobre la ausencia de datos.

#### Â¿Por quÃ© es Ãºtil?

A veces, el **hecho de que un valor estÃ© faltante** es informaciÃ³n relevante:
- Edad faltante â†’ Persona no quiso revelarla (privacidad)
- Salario faltante â†’ Persona desempleada
- MediciÃ³n faltante â†’ Sensor fallÃ³ en momento crÃ­tico

#### Ejemplo Visual

```
Dataset Original:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬
â”‚ Age â”‚ Fare â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¤
â”‚ 22  â”‚ 7.25 â”‚
â”‚ NaN â”‚ 8.05 â”‚
â”‚ 35  â”‚ NaN  â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜

DespuÃ©s de MissingIndicator:
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Age â”‚ Fare â”‚ Age_missingâ”‚ Fare_missingâ”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 22  â”‚ 7.25 â”‚ False      â”‚ False      â”‚
â”‚ 28* â”‚ 8.05 â”‚ True       â”‚ False      â”‚
â”‚ 35  â”‚ 7.6* â”‚ False      â”‚ True       â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
* Valores imputados
```

#### CÃ³digo con scikit-learn

```python
from sklearn.impute import MissingIndicator
import numpy as np

X = np.array([[-1, -1, 1, 3],
              [4, -1, 0, -1],
              [8, -1, 1, 0]])

# Crear indicadores solo para columnas con valores faltantes
indicator = MissingIndicator(missing_values=-1)
mask = indicator.fit_transform(X)
# mask shape: (3, 2) - Solo 2 columnas porque solo 2 tienen -1 en todas las filas

# Crear indicadores para TODAS las columnas
indicator_all = MissingIndicator(missing_values=-1, features="all")
mask_all = indicator_all.fit_transform(X)
# mask_all shape: (3, 4) - Todas las columnas
```

#### CombinaciÃ³n con SimpleImputer usando Pipeline

```python
from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.impute import SimpleImputer, MissingIndicator

# Pipeline que imputa Y agrega indicadores
transformer = FeatureUnion([
    ('features', SimpleImputer(strategy='mean')),
    ('indicators', MissingIndicator())
])

X_transformed = transformer.fit_transform(X)
# X_transformed contendrÃ¡: [valores_imputados, indicadores_binarios]
```

---

### ComparaciÃ³n de MÃ©todos de ImputaciÃ³n

```mermaid
graph TB
    A{Â¿CuÃ¡ntos valores<br/>faltantes?} -->|Muy pocos<br/><5%| B[SimpleImputer<br/>mean/median]
    A -->|Moderados<br/>5-20%| C{Â¿Las variables<br/>estÃ¡n relacionadas?}
    A -->|Muchos<br/>>20%| D[Considerar eliminar<br/>columna o usar<br/>IterativeImputer]

    C -->|SÃ­| E[KNNImputer o<br/>IterativeImputer]
    C -->|No| F[SimpleImputer<br/>+ MissingIndicator]

    style B fill:#ccffcc
    style E fill:#ffffcc
    style F fill:#cce5ff
    style D fill:#ffcccc
```

### Tabla Comparativa Completa

| MÃ©todo | Velocidad | PrecisiÃ³n | Complejidad | Mejor Para |
|--------|-----------|-----------|-------------|------------|
| **SimpleImputer** | âš¡âš¡âš¡ Muy rÃ¡pido | â­â­ BÃ¡sica | ğŸŸ¢ Simple | Pocos NaN, exploraciÃ³n rÃ¡pida |
| **KNNImputer** | âš¡âš¡ Moderado | â­â­â­ Buena | ğŸŸ¡ Media | Variables correlacionadas, datasets medianos |
| **IterativeImputer** | âš¡ Lento | â­â­â­â­ Excelente | ğŸ”´ Compleja | MÃ¡xima precisiÃ³n, relaciones complejas |
| **MissingIndicator** | âš¡âš¡âš¡ Muy rÃ¡pido | N/A | ğŸŸ¢ Simple | Preservar info de ausencia + otro mÃ©todo |

---

## CodificaciÃ³n de Variables CategÃ³ricas

### Â¿Por quÃ© codificar?

Los algoritmos de machine learning trabajan con **nÃºmeros**, no con texto. Necesitamos convertir variables categÃ³ricas como "Rojo", "Azul", "Verde" en representaciones numÃ©ricas.

### Tipos de Variables CategÃ³ricas

```mermaid
graph TD
    A[Variables<br/>CategÃ³ricas] --> B[Nominales<br/>Sin orden]
    A --> C[Ordinales<br/>Con orden]

    B --> B1[Ejemplos:<br/>Color, Ciudad,<br/>GÃ©nero]
    C --> C1[Ejemplos:<br/>EducaciÃ³n, SatisfacciÃ³n,<br/>Talla de ropa]

    B --> B2[CodificaciÃ³n:<br/>OneHotEncoder]
    C --> C2[CodificaciÃ³n:<br/>OrdinalEncoder]

    style B fill:#cce5ff
    style C fill:#ccffcc
```

---

### 1. LabelEncoder (CodificaciÃ³n de Etiquetas)

**Concepto**: Convierte cada categorÃ­a Ãºnica en un **nÃºmero entero** (0, 1, 2, ...).

âš ï¸ **IMPORTANTE**: Solo para la **variable objetivo (y)**, NO para caracterÃ­sticas (X).

#### Â¿Por quÃ© no usar LabelEncoder en caracterÃ­sticas?

```
Problema: LabelEncoder introduce orden artificial

CategorÃ­as: ["Rojo", "Verde", "Azul"]
LabelEncoder: [0, 1, 2]

âŒ El modelo pensarÃ¡: Verde (1) estÃ¡ "entre" Rojo (0) y Azul (2)
âŒ El modelo pensarÃ¡: Azul (2) > Verde (1) > Rojo (0)
âŒ Introduce relaciones matemÃ¡ticas falsas
```

#### CÃ³digo con scikit-learn

```python
from sklearn.preprocessing import LabelEncoder

# CORRECTO: Para variable objetivo
y = ['gato', 'perro', 'gato', 'pÃ¡jaro']
le = LabelEncoder()
y_encoded = le.fit_transform(y)
# Resultado: [0, 1, 0, 2]

# Ver el mapeo
print(le.classes_)  # ['gato', 'pÃ¡jaro', 'perro']

# Decodificar
y_original = le.inverse_transform([0, 1, 2])
# Resultado: ['gato', 'pÃ¡jaro', 'perro']
```

---

### 2. OrdinalEncoder (CodificaciÃ³n Ordinal)

**Concepto**: Similar a LabelEncoder pero para **variables ordinales** en caracterÃ­sticas (X). Respeta el **orden natural** de las categorÃ­as.

#### Â¿CuÃ¡ndo Usar?

```mermaid
graph LR
    A[Variable CategÃ³rica] --> B{Â¿Tiene orden<br/>natural?}
    B -->|SÃ­| C[OrdinalEncoder]
    B -->|No| D[OneHotEncoder]

    C --> C1[Ejemplos:<br/>- EducaciÃ³n<br/>- SatisfacciÃ³n<br/>- Talla]
    D --> D1[Ejemplos:<br/>- Color<br/>- Ciudad<br/>- Marca]

    style C fill:#ccffcc
    style D fill:#cce5ff
```

#### Ejemplo Visual

```
Variable: EducaciÃ³n

CategorÃ­as con orden:
["Primaria", "Secundaria", "Universidad", "Posgrado"]

OrdinalEncoder con categories especificadas:
[0, 1, 2, 3]

âœ… El orden es correcto: 3 > 2 > 1 > 0
âœ… El modelo entiende que Posgrado > Universidad
```

#### CÃ³digo con scikit-learn

```python
from sklearn.preprocessing import OrdinalEncoder
import numpy as np

# Datos con variables ordinales
X = [['male', 'from US', 'uses Safari'],
     ['female', 'from Europe', 'uses Firefox']]

# CodificaciÃ³n automÃ¡tica (orden alfabÃ©tico)
enc = OrdinalEncoder()
X_encoded = enc.fit_transform(X)

# CodificaciÃ³n con orden ESPECÃFICO (recomendado para ordinales)
education = ['Primaria', 'Secundaria', 'Universidad', 'Posgrado']
satisfaction = ['Muy Insatisfecho', 'Insatisfecho', 'Neutral', 'Satisfecho', 'Muy Satisfecho']

enc_ordinal = OrdinalEncoder(categories=[education, satisfaction])
X_ordinal = [['Secundaria', 'Satisfecho'],
             ['Universidad', 'Muy Satisfecho']]
X_encoded = enc_ordinal.fit_transform(X_ordinal)
# Resultado: [[1., 3.], [2., 4.]]

# Manejo de valores faltantes
X_with_nan = [['male'], ['female'], [np.nan], ['female']]
enc_nan = OrdinalEncoder(encoded_missing_value=-1)
X_encoded_nan = enc_nan.fit_transform(X_with_nan)
# NaN se codifica como -1

# Manejo de categorÃ­as desconocidas
enc_unknown = OrdinalEncoder(
    handle_unknown='use_encoded_value',
    unknown_value=999
)
```

#### ParÃ¡metros Importantes

| ParÃ¡metro | DescripciÃ³n | Ejemplo |
|-----------|-------------|---------|
| `categories` | Lista de categorÃ­as en orden | `[['bajo', 'medio', 'alto']]` |
| `handle_unknown` | CÃ³mo manejar categorÃ­as nuevas | `'error'`, `'use_encoded_value'` |
| `unknown_value` | Valor para categorÃ­as desconocidas | `999`, `-1` |
| `encoded_missing_value` | Valor para NaN | `-1`, `999` |

---

### 3. OneHotEncoder (CodificaciÃ³n One-Hot)

**Concepto**: Crea una **columna binaria (0/1) por cada categorÃ­a**. No introduce orden artificial.

#### Â¿CÃ³mo Funciona?

```
Variable: Color
CategorÃ­as: ["Rojo", "Verde", "Azul"]

Dataset Original:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚ Color â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Rojo  â”‚
â”‚ Verde â”‚
â”‚ Azul  â”‚
â”‚ Rojo  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”˜

DespuÃ©s de OneHotEncoder:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rojo  â”‚ Verde â”‚ Azul  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   1   â”‚   0   â”‚   0   â”‚
â”‚   0   â”‚   1   â”‚   0   â”‚
â”‚   0   â”‚   0   â”‚   1   â”‚
â”‚   1   â”‚   0   â”‚   0   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… No hay orden implÃ­cito
âœ… Cada categorÃ­a es independiente
```

#### CÃ³digo con scikit-learn

```python
from sklearn.preprocessing import OneHotEncoder

# Datos categÃ³ricos
X = [['male', 'from US', 'uses Safari'],
     ['female', 'from Europe', 'uses Firefox']]

# OneHotEncoder bÃ¡sico
enc = OneHotEncoder()
X_encoded = enc.fit_transform(X)
# Resultado: sparse matrix (eficiente para muchas categorÃ­as)

# OneHotEncoder con array denso
enc_dense = OneHotEncoder(sparse_output=False)
X_encoded_dense = enc_dense.fit_transform(X)

# Ver las categorÃ­as
print(enc.categories_)
# [array(['female', 'male'], dtype=object),
#  array(['from Europe', 'from US'], dtype=object),
#  array(['uses Firefox', 'uses Safari'], dtype=object)]

# Obtener nombres de columnas
print(enc.get_feature_names_out())
# ['x0_female', 'x0_male', 'x1_from Europe', 'x1_from US', ...]
```

#### Problema: Multicolinealidad (Dummy Variable Trap)

```mermaid
graph TD
    A[3 CategorÃ­as:<br/>Rojo, Verde, Azul] --> B{OneHotEncoder<br/>completo}
    B --> C[3 columnas:<br/>Rojo, Verde, Azul]

    C --> D[âŒ Problema:<br/>Multicolinealidad]
    D --> E["Si Rojo=0 y Verde=0<br/>entonces Azul=1<br/>(info redundante)"]

    A --> F{OneHotEncoder<br/>con drop='first'}
    F --> G[2 columnas:<br/>Verde, Azul]
    G --> H[âœ… SoluciÃ³n:<br/>Sin redundancia]

    style D fill:#ffcccc
    style H fill:#ccffcc
```

#### CÃ³digo con drop='first'

```python
# Evitar multicolinealidad eliminando la primera categorÃ­a
enc_drop = OneHotEncoder(drop='first', sparse_output=False)
X_encoded = enc_drop.fit_transform(X)
# Si antes tenÃ­as 3 columnas por categorÃ­a, ahora tendrÃ¡s 2
```

#### Manejo de CategorÃ­as Poco Frecuentes

```python
# Agrupar categorÃ­as poco frecuentes (aparecen < 6 veces)
X = np.array([['dog'] * 5 + ['cat'] * 20 + ['rabbit'] * 10 + ['snake'] * 3],
             dtype=object).T

enc_infrequent = OneHotEncoder(min_frequency=6, sparse_output=False)
enc_infrequent.fit(X)

# Ver categorÃ­as poco frecuentes
print(enc_infrequent.infrequent_categories_)
# [array(['dog', 'snake'], dtype=object)]

# Resultado: dog y snake se agrupan en una categorÃ­a "infrequent"
```

#### Manejo de CategorÃ­as Desconocidas

```python
# OpciÃ³n 1: Error (default)
enc_error = OneHotEncoder(handle_unknown='error')

# OpciÃ³n 2: Ignorar (codificar como todos ceros)
enc_ignore = OneHotEncoder(handle_unknown='ignore')
X_test = [['female', 'from Asia', 'uses Chrome']]  # 'from Asia' es nueva
X_encoded = enc_ignore.transform(X_test)
# from Asia â†’ [0, 0] (ni Europe ni US)

# OpciÃ³n 3: Tratar como categorÃ­a poco frecuente
enc_infrequent_unknown = OneHotEncoder(
    handle_unknown='infrequent_if_exist',
    min_frequency=5
)
```

#### Ventajas y Desventajas

| âœ… Ventajas | âŒ Desventajas |
|------------|----------------|
| No introduce orden artificial | Aumenta dimensionalidad |
| Cada categorÃ­a es independiente | No funciona bien con alta cardinalidad |
| Funciona con categorÃ­as nominales | Puede crear sparse matrices grandes |
| Interpretable | Curse of dimensionality |

---

### 4. TargetEncoder (CodificaciÃ³n por Objetivo)

**Concepto**: Reemplaza cada categorÃ­a por la **media de la variable objetivo** para esa categorÃ­a. Ãštil para variables categÃ³ricas con **alta cardinalidad** (muchas categorÃ­as Ãºnicas).

#### Â¿CÃ³mo Funciona?

```
Dataset:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ciudad â”‚ Survived â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Paris  â”‚    1     â”‚
â”‚ London â”‚    0     â”‚
â”‚ Paris  â”‚    1     â”‚
â”‚ London â”‚    0     â”‚
â”‚ Paris  â”‚    0     â”‚
â”‚ Berlin â”‚    1     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TargetEncoder:
- Paris  â†’ (1 + 1 + 0) / 3 = 0.667
- London â†’ (0 + 0) / 2 = 0.000
- Berlin â†’ 1 / 1 = 1.000

Resultado:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Ciudad_enc â”‚ Survived â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   0.667    â”‚    1     â”‚
â”‚   0.000    â”‚    0     â”‚
â”‚   0.667    â”‚    1     â”‚
â”‚   0.000    â”‚    0     â”‚
â”‚   0.667    â”‚    0     â”‚
â”‚   1.000    â”‚    1     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### âš ï¸ Riesgo: Target Leakage y Overfitting

```mermaid
graph TD
    A[TargetEncoder sin<br/>validaciÃ³n cruzada] --> B[âŒ Target Leakage]
    B --> C[El modelo memoriza<br/>los encodings]
    C --> D[Overfitting en<br/>entrenamiento]
    D --> E[Bajo rendimiento<br/>en test]

    F[TargetEncoder con<br/>cross-validation] --> G[âœ… Previene Leakage]
    G --> H[Encodings separados<br/>por fold]
    H --> I[GeneralizaciÃ³n<br/>correcta]

    style B fill:#ffcccc
    style G fill:#ccffcc
```

#### CÃ³digo con scikit-learn

```python
from sklearn.preprocessing import TargetEncoder

X = [['dog'], ['cat'], ['dog'], ['cat'], ['dog']]
y = [90, 80, 75, 60, 70]

# TargetEncoder bÃ¡sico
enc = TargetEncoder()
X_encoded = enc.fit_transform(X, y)
# dog â†’ (90 + 75 + 70) / 3 = 78.33
# cat â†’ (80 + 60) / 2 = 70.00

# Con cross-validation para evitar overfitting (RECOMENDADO)
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

pipeline = Pipeline([
    ('encoder', TargetEncoder()),
    ('classifier', LogisticRegression())
])

# El encoder se ajustarÃ¡ correctamente en cada fold
scores = cross_val_score(pipeline, X, y, cv=5)
```

#### Ventajas y Desventajas

| âœ… Ventajas | âŒ Desventajas |
|------------|----------------|
| Funciona con alta cardinalidad | Riesgo de target leakage |
| No aumenta dimensionalidad | Puede overfit |
| Captura relaciÃ³n con el target | Solo para target numÃ©rico |
| Eficiente en memoria | Requiere cross-validation |

---

### ComparaciÃ³n de MÃ©todos de CodificaciÃ³n

#### Tabla Comparativa

| MÃ©todo | Variables | Dimensiones | Orden | Cardinalidad | Uso Principal |
|--------|-----------|-------------|-------|--------------|---------------|
| **LabelEncoder** | Target (y) | âœ… No aumenta | âš ï¸ Introduce | Cualquiera | Variable objetivo |
| **OrdinalEncoder** | Ordinales | âœ… No aumenta | âœ… Respeta | Cualquiera | EducaciÃ³n, satisfacciÃ³n |
| **OneHotEncoder** | Nominales | âŒ Aumenta mucho | âœ… No introduce | Baja-Media | Color, ciudad, gÃ©nero |
| **TargetEncoder** | Nominales | âœ… No aumenta | N/A | Alta | CÃ³digos postales, IDs |

#### Diagrama de DecisiÃ³n

```mermaid
graph TD
    A{Â¿Variable objetivo<br/>o caracterÃ­stica?} -->|Target y| B[LabelEncoder]
    A -->|CaracterÃ­stica X| C{Â¿Tiene orden<br/>natural?}

    C -->|SÃ­| D[OrdinalEncoder]
    C -->|No| E{Â¿CuÃ¡ntas<br/>categorÃ­as?}

    E -->|Pocas<br/><10| F[OneHotEncoder]
    E -->|Muchas<br/>>50| G[TargetEncoder]
    E -->|Media<br/>10-50| H{Â¿Importa la<br/>dimensionalidad?}

    H -->|No| F
    H -->|SÃ­| G

    style B fill:#ccffcc
    style D fill:#cce5ff
    style F fill:#ffffcc
    style G fill:#ffcccc
```

---

## Escalado y NormalizaciÃ³n

### Â¿Por quÃ© escalar?

Muchos algoritmos de ML son **sensibles a la escala** de las caracterÃ­sticas:

```
Sin Escalar:
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Age  â”‚ Salary â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  25  â”‚ 50000  â”‚
â”‚  30  â”‚ 60000  â”‚
â”‚  45  â”‚ 100000 â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Problema: Salary domina porque sus valores son ~1000x mÃ¡s grandes
```

### Algoritmos Sensibles a la Escala

| Sensibles | No Sensibles |
|-----------|--------------|
| K-Nearest Neighbors (KNN) | Ãrboles de DecisiÃ³n |
| Support Vector Machines (SVM) | Random Forest |
| RegresiÃ³n LogÃ­stica/Lineal | XGBoost |
| Redes Neuronales | LightGBM |
| K-Means Clustering | CatBoost |
| PCA | Naive Bayes |

---

### 1. StandardScaler (EstandarizaciÃ³n Z-score)

**Concepto**: Transforma datos para que tengan **media = 0** y **desviaciÃ³n estÃ¡ndar = 1**.

**FÃ³rmula**:
```
z = (x - Î¼) / Ïƒ

donde:
- x = valor original
- Î¼ = media de la columna
- Ïƒ = desviaciÃ³n estÃ¡ndar
```

#### Ejemplo Visual

```
Datos Originales:
[10, 20, 30, 40, 50]
Î¼ = 30, Ïƒ = 15.81

DespuÃ©s de StandardScaler:
[-1.26, -0.63, 0.00, 0.63, 1.26]
Î¼ = 0, Ïƒ = 1
```

#### VisualizaciÃ³n

```mermaid
graph LR
    A[DistribuciÃ³n<br/>Original] -->|StandardScaler| B[DistribuciÃ³n<br/>Estandarizada]

    A1[Î¼ = 100<br/>Ïƒ = 15] -.-> A
    B1[Î¼ = 0<br/>Ïƒ = 1] -.-> B

    style A fill:#ffcccc
    style B fill:#ccffcc
```

#### CÃ³digo con scikit-learn

```python
from sklearn.preprocessing import StandardScaler
import numpy as np

X = np.array([[1., -1., 2.],
              [2., 0., 0.],
              [0., 1., -1.]])

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Ver media y desviaciÃ³n estÃ¡ndar aprendidas
print(scaler.mean_)  # [1., 0., 0.33]
print(scaler.scale_) # [0.816, 0.816, 1.247]

# Aplicar a datos nuevos
X_test = np.array([[-1., 1., 0.]])
X_test_scaled = scaler.transform(X_test)
```

#### CuÃ¡ndo Usar

| âœ… Usar | âŒ No Usar |
|--------|-----------|
| SVM, RegresiÃ³n LogÃ­stica | Ãrboles, Random Forest |
| Redes Neuronales | Datos con muchos outliers |
| PCA | Variables ya en [0, 1] |
| Datos con distribuciÃ³n normal | |

---

### 2. MinMaxScaler (NormalizaciÃ³n Min-Max)

**Concepto**: Escala datos al rango **[0, 1]** (o cualquier rango personalizado).

**FÃ³rmula**:
```
x_scaled = (x - x_min) / (x_max - x_min)
```

#### Ejemplo Visual

```
Datos Originales:
[10, 20, 30, 40, 50]
min = 10, max = 50

DespuÃ©s de MinMaxScaler:
[0.0, 0.25, 0.5, 0.75, 1.0]
min = 0, max = 1
```

#### CÃ³digo con scikit-learn

```python
from sklearn.preprocessing import MinMaxScaler

X = np.array([[1., -1., 2.],
              [2., 0., 0.],
              [0., 1., -1.]])

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Ver mÃ­n y mÃ¡x aprendidos
print(scaler.data_min_)  # [0., -1., -1.]
print(scaler.data_max_)  # [2., 1., 2.]

# Escalar a rango personalizado [0, 10]
scaler_custom = MinMaxScaler(feature_range=(0, 10))
X_scaled_custom = scaler_custom.fit_transform(X)
```

#### Ventajas y Desventajas

| âœ… Ventajas | âŒ Desventajas |
|------------|----------------|
| Mantiene forma de distribuciÃ³n | Muy sensible a outliers |
| Todos los valores en [0, 1] | No mantiene media = 0 |
| Interpretable | Afecta mucho la escala |
| Funciona bien con redes neuronales | |

---

### 3. MaxAbsScaler

**Concepto**: Escala dividiendo por el **valor absoluto mÃ¡ximo**. Resultado: rango **[-1, 1]**.

**FÃ³rmula**:
```
x_scaled = x / max(|x|)
```

#### Ventaja Principal

âœ… **Preserva la esparsidad** (mantiene los ceros como ceros)
âœ… Ãštil para **datos sparse** (matrices con muchos ceros)

#### CÃ³digo con scikit-learn

```python
from sklearn.preprocessing import MaxAbsScaler

X = np.array([[1., -1., 2.],
              [2., 0., 0.],
              [0., 1., -1.]])

scaler = MaxAbsScaler()
X_scaled = scaler.fit_transform(X)
# Resultado: [[0.5, -1., 1.],
#             [1., 0., 0.],
#             [0., 1., -0.5]]

# Ver el mÃ¡ximo absoluto por columna
print(scaler.scale_)  # [2., 1., 2.]
```

#### CuÃ¡ndo Usar

- Datos ya centrados en cero
- Matrices sparse (muchos ceros)
- Datos de sensores (positivos y negativos)

---

### 4. RobustScaler

**Concepto**: Usa la **mediana** y el **rango intercuartÃ­lico (IQR)** en lugar de media y desviaciÃ³n estÃ¡ndar. **Robusto a outliers**.

**FÃ³rmula**:
```
x_scaled = (x - mediana) / IQR

donde IQR = Q3 - Q1
```

#### Â¿Por quÃ© es robusto?

```
Datos con outlier:
[1, 2, 3, 4, 5, 100]

StandardScaler:
- Afectado por 100 (outlier)
- Î¼ = 19.17, Ïƒ = 38.7

RobustScaler:
- No afectado por 100
- mediana = 3.5, IQR = 3
```

#### CÃ³digo con scikit-learn

```python
from sklearn.preprocessing import RobustScaler

X = np.array([[1., -1., 2.],
              [2., 0., 0.],
              [0., 1., -1.]])

scaler = RobustScaler()
X_scaled = scaler.fit_transform(X)

# Ver mediana y IQR
print(scaler.center_)  # Mediana por columna
print(scaler.scale_)   # IQR por columna
```

#### CuÃ¡ndo Usar

| âœ… Usar | âŒ No Usar |
|--------|-----------|
| Datos con outliers | Datos limpios sin outliers |
| Distribuciones sesgadas | Cuando necesitas media = 0 |
| Datos financieros | Datasets pequeÃ±os |

---

### 5. Normalizer

**Concepto**: Normaliza **cada muestra (fila)** para que tenga **norma unitaria**. Diferente a los demÃ¡s (escala por fila, no por columna).

**FÃ³rmula**:
```
Para norma L2:
x_normalized = x / ||x||

donde ||x|| = sqrt(xâ‚Â² + xâ‚‚Â² + ... + xâ‚™Â²)
```

#### Ejemplo Visual

```
Muestra original: [3, 4]
Norma L2: sqrt(3Â² + 4Â²) = sqrt(25) = 5

Normalizada: [3/5, 4/5] = [0.6, 0.8]
Nueva norma: sqrt(0.6Â² + 0.8Â²) = 1.0 âœ…
```

#### CÃ³digo con scikit-learn

```python
from sklearn.preprocessing import Normalizer

X = np.array([[4, 1, 2, 2],
              [1, 3, 9, 3],
              [5, 7, 5, 1]])

# Norma L2 (default)
normalizer_l2 = Normalizer(norm='l2')
X_normalized_l2 = normalizer_l2.fit_transform(X)

# Norma L1
normalizer_l1 = Normalizer(norm='l1')
X_normalized_l1 = normalizer_l1.fit_transform(X)

# Norma Max
normalizer_max = Normalizer(norm='max')
X_normalized_max = normalizer_max.fit_transform(X)
```

#### CuÃ¡ndo Usar

- Sistemas de recomendaciÃ³n
- Similitud de coseno
- AnÃ¡lisis de texto (TF-IDF)
- Cuando la magnitud no importa, solo la direcciÃ³n

---

### ComparaciÃ³n de MÃ©todos de Escalado

```mermaid
graph TD
    A{Â¿Tus datos tienen outliers?} -->|SÃ­| B[RobustScaler]
    A -->|No| C{Â¿Necesitas valores en rango 0-1?}

    C -->|SÃ­| D[MinMaxScaler]
    C -->|No| E{Â¿Datos sparse?}

    E -->|SÃ­| F[MaxAbsScaler]
    E -->|No| G[StandardScaler]

    H{Â¿Escalado por fila?} -->|SÃ­| I[Normalizer]

    style B fill:#ffffcc
    style D fill:#cce5ff
    style F fill:#ffcccc
    style G fill:#ccffcc
    style I fill:#e5ccff
```

### Tabla Resumen

| MÃ©todo | Rango Resultado | Outliers | Sparse | Uso Principal |
|--------|----------------|----------|--------|---------------|
| **StandardScaler** | (-âˆ, +âˆ), Î¼=0, Ïƒ=1 | âŒ Sensible | âŒ No preserva | SVM, Redes Neuronales, PCA |
| **MinMaxScaler** | [0, 1] o custom | âŒ Muy sensible | âŒ No preserva | ImÃ¡genes, Redes Neuronales |
| **MaxAbsScaler** | [-1, 1] | âŒ Sensible | âœ… Preserva | Datos sparse, centrados en 0 |
| **RobustScaler** | (-âˆ, +âˆ) | âœ… Robusto | âŒ No preserva | Datos financieros, outliers |
| **Normalizer** | Norma = 1 | N/A | âœ… Puede preservar | Similitud, texto, recomendaciÃ³n |

---

## ReducciÃ³n de Dimensionalidad: PCA

### Â¿QuÃ© es PCA?

**PCA (Principal Component Analysis)** es una tÃ©cnica que reduce el nÃºmero de caracterÃ­sticas transformÃ¡ndolas en **componentes principales** que capturan la mayor varianza posible.

### Problema: Curse of Dimensionality

```mermaid
graph TD
    A[Muchas Dimensiones<br/>100+ caracterÃ­sticas] --> B[Problemas]

    B --> C[Overfitting]
    B --> D[Lento para entrenar]
    B --> E[DifÃ­cil de visualizar]
    B --> F[Datos sparse]

    G[PCA] --> H[Pocas Dimensiones<br/>10-20 caracterÃ­sticas]
    H --> I[Beneficios]

    I --> J[Menos overfitting]
    I --> K[Entrenamiento rÃ¡pido]
    I --> L[Visualizable en 2D/3D]
    I --> M[Datos mÃ¡s densos]

    style A fill:#ffcccc
    style H fill:#ccffcc
```

### Â¿CÃ³mo Funciona PCA Intuitivamente?

#### AnalogÃ­a Visual

```
Imagina que tienes puntos en 2D:

    â†‘ y
    â”‚     *
    â”‚   *   *
    â”‚ *       *
    â”‚*          *
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x

PCA encuentra:
1. La direcciÃ³n de MÃXIMA variaciÃ³n (PC1)
2. La direcciÃ³n PERPENDICULAR con la siguiente mayor variaciÃ³n (PC2)

    â†‘ PC2
    â”‚
    â”‚    / (PC1)
    â”‚   /
    â”‚  /
    â”‚ /
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’

ProyecciÃ³n en PC1:
*----*--*---*----*-â†’ (1D, mantiene 90% de la info)
```

### Pasos del Algoritmo PCA

```mermaid
graph TD
    A[1. Estandarizar<br/>los datos] --> B[2. Calcular matriz<br/>de covarianza]
    B --> C[3. Calcular<br/>eigenvectores<br/>y eigenvalores]
    C --> D[4. Ordenar por<br/>eigenvalor<br/>descendente]
    D --> E[5. Seleccionar<br/>k componentes]
    E --> F[6. Transformar<br/>datos]

    style A fill:#cce5ff
    style F fill:#ccffcc
```

### CÃ³digo con scikit-learn

```python
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import numpy as np

# Datos de ejemplo: 4 caracterÃ­sticas
X = np.array([[2.5, 2.4, 0.5, 1.0],
              [0.5, 0.7, 1.2, 0.8],
              [2.2, 2.9, 0.9, 1.5],
              [1.9, 2.2, 0.4, 0.9],
              [3.1, 3.0, 0.8, 1.8]])

# Paso 1: SIEMPRE estandarizar antes de PCA
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Paso 2: Aplicar PCA
pca = PCA(n_components=2)  # Reducir a 2 dimensiones
X_pca = pca.fit_transform(X_scaled)

# Ver cuÃ¡nta varianza explica cada componente
print("Varianza explicada:", pca.explained_variance_ratio_)
# Ej: [0.73, 0.22] â†’ PC1 explica 73%, PC2 explica 22%

# Varianza acumulada
print("Varianza acumulada:", np.cumsum(pca.explained_variance_ratio_))
# Ej: [0.73, 0.95] â†’ Con 2 componentes capturas 95% de la info

# Ver los componentes principales (direcciones)
print("Componentes:", pca.components_)
```

### Elegir el NÃºmero de Componentes

#### MÃ©todo 1: Varianza Explicada

```python
# Ver varianza explicada por TODOS los componentes
pca_full = PCA()
pca_full.fit(X_scaled)

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))

# GrÃ¡fico de codo (Scree Plot)
plt.subplot(1, 2, 1)
plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),
         pca_full.explained_variance_ratio_, 'bo-')
plt.xlabel('Componente')
plt.ylabel('Varianza Explicada')
plt.title('Scree Plot')

# GrÃ¡fico acumulado
plt.subplot(1, 2, 2)
plt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),
         np.cumsum(pca_full.explained_variance_ratio_), 'ro-')
plt.axhline(y=0.95, color='g', linestyle='--', label='95% varianza')
plt.xlabel('Componente')
plt.ylabel('Varianza Acumulada')
plt.title('Varianza Acumulada')
plt.legend()

plt.show()
```

#### MÃ©todo 2: Especificar Varianza Deseada

```python
# Mantener 95% de la varianza
pca_95 = PCA(n_components=0.95)
X_pca_95 = pca_95.fit_transform(X_scaled)

print(f"Dimensiones originales: {X.shape[1]}")
print(f"Dimensiones despuÃ©s de PCA: {X_pca_95.shape[1]}")
# AutomÃ¡ticamente selecciona el nÃºmero mÃ­nimo de componentes
```

### VisualizaciÃ³n de Resultados

```python
import matplotlib.pyplot as plt

# Visualizar en 2D
plt.figure(figsize=(8, 6))
plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)
plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} varianza)')
plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} varianza)')
plt.title('Datos proyectados en 2 componentes principales')
plt.grid(True)
plt.show()
```

### Invertir la TransformaciÃ³n

```python
# Reconstruir datos aproximados desde PCA
X_reconstructed = pca.inverse_transform(X_pca)

# Comparar con originales
reconstruction_error = np.mean((X_scaled - X_reconstructed) ** 2)
print(f"Error de reconstrucciÃ³n: {reconstruction_error:.6f}")
```

### Ventajas y Desventajas de PCA

| âœ… Ventajas | âŒ Desventajas |
|------------|----------------|
| Reduce dimensionalidad | PÃ©rdida de informaciÃ³n |
| Elimina multicolinealidad | Componentes no interpretables |
| Acelera entrenamiento | Asume linealidad |
| Reduce overfitting | Sensible a escala (requiere estandarizaciÃ³n) |
| VisualizaciÃ³n en 2D/3D | No funciona con datos categÃ³ricos |

### CuÃ¡ndo Usar PCA

```mermaid
graph TD
    A{Â¿CuÃ¡ntas<br/>caracterÃ­sticas?} -->|<10| B[âŒ No necesitas PCA<br/>Pocas features]
    A -->|10-50| C{Â¿Hay<br/>correlaciÃ³n?}
    A -->|>50| D[âœ… Considera PCA]

    C -->|SÃ­| E[âœ… PCA puede ayudar]
    C -->|No| F[âš ï¸ PCA puede no ayudar]

    G{Â¿Necesitas<br/>interpretabilidad?} -->|SÃ­| H[âŒ Evita PCA<br/>o usa pocas componentes]
    G -->|No| I[âœ… PCA es buena opciÃ³n]

    style B fill:#ffcccc
    style D fill:#ccffcc
    style E fill:#ccffcc
    style F fill:#ffffcc
    style H fill:#ffcccc
    style I fill:#ccffcc
```

### Casos de Uso Comunes

1. **VisiÃ³n por Computadora**: Reducir imÃ¡genes de 784 pÃ­xeles (28Ã—28) a 50 componentes
2. **AnÃ¡lisis GenÃ©tico**: Miles de genes â†’ decenas de componentes
3. **Finanzas**: MÃºltiples indicadores econÃ³micos â†’ pocos factores
4. **Procesamiento de SeÃ±ales**: CompresiÃ³n de datos de sensores

---

## Feature Engineering

### Â¿QuÃ© es Feature Engineering?

**Feature Engineering** es el proceso de **crear, transformar o seleccionar** caracterÃ­sticas para mejorar el rendimiento de los modelos.

```mermaid
graph LR
    A[Datos<br/>Crudos] --> B[Feature<br/>Engineering]
    B --> C[CaracterÃ­sticas<br/>Mejoradas]
    C --> D[Modelo ML]
    D --> E[Mejor<br/>Rendimiento]

    style A fill:#ffcccc
    style C fill:#ccffcc
    style E fill:#aaffaa
```

### Tipos de Feature Engineering

```mermaid
mindmap
  root((Feature<br/>Engineering))
    CreaciÃ³n
      Interacciones
      Polinomiales
      Agregaciones
      Binning
    TransformaciÃ³n
      LogarÃ­tmica
      Box-Cox
      RaÃ­z cuadrada
    SelecciÃ³n
      Filter Methods
      Wrapper Methods
      Embedded Methods
    ExtracciÃ³n
      PCA
      LDA
      Embeddings
```

---

### 1. CreaciÃ³n de Features: Interacciones

**Concepto**: Combinar dos o mÃ¡s caracterÃ­sticas para capturar relaciones.

#### Ejemplo

```python
# Dataset: Precio de casas
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Area â”‚ Rooms  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 100  â”‚   3    â”‚
â”‚ 150  â”‚   4    â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# Feature Engineering: Crear "Area por habitaciÃ³n"
â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Area â”‚ Rooms  â”‚ Area_per_roomâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 100  â”‚   3    â”‚   33.33     â”‚
â”‚ 150  â”‚   4    â”‚   37.50     â”‚
â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### CÃ³digo

```python
import pandas as pd

# Crear interacciones manualmente
df['Area_per_room'] = df['Area'] / df['Rooms']
df['Total_space'] = df['Area'] + df['Garden_area']
df['Price_per_sqm'] = df['Price'] / df['Area']

# Interacciones automÃ¡ticas con PolynomialFeatures
from sklearn.preprocessing import PolynomialFeatures

X = df[['Area', 'Rooms']]
poly = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly.fit_transform(X)

# Genera: [Area, Rooms, AreaÂ², AreaÃ—Rooms, RoomsÂ²]
print(poly.get_feature_names_out())
```

---

### 2. Binning (DiscretizaciÃ³n)

**Concepto**: Convertir variables continuas en **grupos** o **bins**.

#### Â¿Por quÃ©?

- Captura relaciones no lineales
- Reduce efecto de outliers
- A veces mejora interpretabilidad

#### Ejemplo Visual

```
Edad continua: [18, 25, 32, 45, 67, 72]

DespuÃ©s de Binning:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Rango Edad  â”‚ Bin     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 18-30       â”‚ Joven   â”‚
â”‚ 31-50       â”‚ Adulto  â”‚
â”‚ 51+         â”‚ Senior  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### CÃ³digo con scikit-learn

```python
from sklearn.preprocessing import KBinsDiscretizer
import numpy as np

X = np.array([[18], [25], [32], [45], [67], [72]])

# Binning en 3 grupos con estrategia "uniform" (rangos iguales)
discretizer = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')
X_binned = discretizer.fit_transform(X)

# Estrategias disponibles:
# - 'uniform': Rangos de igual ancho
# - 'quantile': Bins con igual nÃºmero de muestras
# - 'kmeans': Bins basados en K-means
```

---

### 3. Transformaciones MatemÃ¡ticas

#### LogarÃ­tmica

**CuÃ¡ndo usar**: Datos con **distribuciÃ³n sesgada** (skewed) o con valores muy grandes.

```python
# Variable con distribuciÃ³n exponencial
df['Income_log'] = np.log1p(df['Income'])  # log1p = log(1 + x)
```

#### Box-Cox

**CuÃ¡ndo usar**: Para hacer datos **mÃ¡s normales**.

```python
from scipy.stats import boxcox

# Transforma datos a distribuciÃ³n mÃ¡s normal
df['Feature_boxcox'], lambda_param = boxcox(df['Feature'] + 1)
# Nota: Feature debe ser > 0
```

#### VisualizaciÃ³n del Efecto

```
Antes (Skewed):        DespuÃ©s (Normalizada):
    â–‚â–„â–‡â–ˆâ–…â–ƒâ–‚â–               â–â–ƒâ–…â–‡â–ˆâ–‡â–…â–ƒâ–
```

---

### 4. Features Temporales

**Concepto**: Extraer informaciÃ³n de fechas.

```python
import pandas as pd

df['Date'] = pd.to_datetime(df['Date'])

# Extraer componentes
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['DayOfWeek'] = df['Date'].dt.dayofweek
df['Quarter'] = df['Date'].dt.quarter
df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)

# Features cÃ­clicas (mantienen continuidad)
df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)
df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)
```

**Â¿Por quÃ© cÃ­clicas?**

```
Problema: Diciembre (12) y Enero (1) estÃ¡n lejos numÃ©ricamente
SoluciÃ³n: sin/cos hacen que 12 y 1 estÃ©n cerca en el cÃ­rculo

    12 â† â†’ 1
    â†‘     â†“
    11 â†’ 2
```

---

### 5. Agregaciones

**Concepto**: Crear estadÃ­sticas resumidas.

```python
# Por grupos
df['Avg_price_by_city'] = df.groupby('City')['Price'].transform('mean')
df['Max_price_by_city'] = df.groupby('City')['Price'].transform('max')
df['Count_by_city'] = df.groupby('City')['Price'].transform('count')

# Rolling (ventanas mÃ³viles)
df['Sales_rolling_7d'] = df['Sales'].rolling(window=7).mean()
df['Sales_rolling_30d'] = df['Sales'].rolling(window=30).mean()
```

---

### 6. Target Encoding (ya visto)

Ver secciÃ³n de [TargetEncoder](#4-targetencoder-codificaciÃ³n-por-objetivo).

---

### SelecciÃ³n de Features

#### Â¿Por quÃ© seleccionar?

- âŒ MÃ¡s features â‰  mejor modelo
- âœ… Menos features = menos overfitting, mÃ¡s rÃ¡pido

#### MÃ©todos

```mermaid
graph TD
    A[SelecciÃ³n de<br/>Features] --> B[Filter Methods]
    A --> C[Wrapper Methods]
    A --> D[Embedded Methods]

    B --> B1[CorrelaciÃ³n<br/>Varianza<br/>Chi-cuadrado]
    C --> C1[RFE<br/>Forward Selection<br/>Backward Elimination]
    D --> D1[Lasso<br/>Random Forest<br/>XGBoost]

    style B fill:#cce5ff
    style C fill:#ffffcc
    style D fill:#ccffcc
```

#### 1. Filter Methods: CorrelaciÃ³n

```python
# Eliminar features con baja correlaciÃ³n con el target
correlation = df.corr()['target'].abs().sort_values(ascending=False)
selected_features = correlation[correlation > 0.1].index.tolist()

# Eliminar features altamente correlacionadas entre sÃ­
correlation_matrix = df.corr().abs()
upper_triangle = correlation_matrix.where(
    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
)
to_drop = [col for col in upper_triangle.columns if any(upper_triangle[col] > 0.95)]
df_filtered = df.drop(columns=to_drop)
```

#### 2. Wrapper Methods: RFE

```python
from sklearn.feature_selection import RFE
from sklearn.ensemble import RandomForestClassifier

# SelecciÃ³n recursiva de features
model = RandomForestClassifier()
rfe = RFE(estimator=model, n_features_to_select=10)
rfe.fit(X, y)

# Ver features seleccionadas
selected_features = X.columns[rfe.support_]
print(selected_features)
```

#### 3. Embedded Methods: Feature Importance

```python
from sklearn.ensemble import RandomForestClassifier

# Entrenar modelo
model = RandomForestClassifier()
model.fit(X, y)

# Ver importancias
importances = pd.DataFrame({
    'feature': X.columns,
    'importance': model.feature_importances_
}).sort_values('importance', ascending=False)

# Seleccionar top 10
top_features = importances.head(10)['feature'].tolist()
```

---

### Pipeline Completo de Feature Engineering

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

# Pipeline completo
pipeline = Pipeline([
    ('poly', PolynomialFeatures(degree=2)),
    ('scaler', StandardScaler()),
    ('pca', PCA(n_components=0.95)),
    ('classifier', RandomForestClassifier())
])

pipeline.fit(X_train, y_train)
predictions = pipeline.predict(X_test)
```

---

## Preprocesamiento de Texto

### Flujo TÃ­pico

```mermaid
graph LR
    A[Texto Crudo] --> B[Limpieza]
    B --> C[TokenizaciÃ³n]
    C --> D[NormalizaciÃ³n]
    D --> E[VectorizaciÃ³n]
    E --> F[Modelo ML]

    style A fill:#ffcccc
    style F fill:#ccffcc
```

### TÃ©cnicas Comunes

#### 1. Limpieza

```python
import re

def clean_text(text):
    # MinÃºsculas
    text = text.lower()

    # Eliminar URLs
    text = re.sub(r'http\S+|www\S+', '', text)

    # Eliminar menciones y hashtags
    text = re.sub(r'@\w+|#\w+', '', text)

    # Eliminar puntuaciÃ³n
    text = re.sub(r'[^\w\s]', '', text)

    # Eliminar nÃºmeros
    text = re.sub(r'\d+', '', text)

    # Eliminar espacios mÃºltiples
    text = re.sub(r'\s+', ' ', text).strip()

    return text

text = "Check out https://example.com! @user #AI is awesome!!!"
clean_text(text)
# Resultado: "check out is awesome"
```

#### 2. TokenizaciÃ³n

```python
from sklearn.feature_extraction.text import CountVectorizer

# TokenizaciÃ³n simple
text = "El machine learning es fascinante"
tokens = text.split()  # ['El', 'machine', 'learning', 'es', 'fascinante']

# Con CountVectorizer
vectorizer = CountVectorizer()
X = vectorizer.fit_transform([text])
print(vectorizer.get_feature_names_out())
```

#### 3. Stopwords (Palabras VacÃ­as)

```python
from sklearn.feature_extraction.text import CountVectorizer

# Eliminar stopwords en espaÃ±ol
vectorizer = CountVectorizer(stop_words='english')  # Para inglÃ©s

# Stopwords personalizadas
custom_stopwords = ['el', 'la', 'de', 'en', 'es', 'un', 'una']
vectorizer = CountVectorizer(stop_words=custom_stopwords)
```

#### 4. N-gramas

```python
from sklearn.feature_extraction.text import CountVectorizer

# Unigramas (palabras individuales)
vectorizer_1 = CountVectorizer(ngram_range=(1, 1))

# Bigramas
vectorizer_2 = CountVectorizer(ngram_range=(2, 2))

# Unigramas + Bigramas
vectorizer_12 = CountVectorizer(ngram_range=(1, 2))

text = ["machine learning is great"]
X = vectorizer_2.fit_transform(text)
print(vectorizer_2.get_feature_names_out())
# ['machine learning', 'learning is', 'is great']
```

#### 5. TF-IDF

**Concepto**: Da mÃ¡s peso a palabras **importantes** y menos a palabras **comunes**.

```
TF (Term Frequency): Frecuencia de palabra en documento
IDF (Inverse Document Frequency): Rareza de palabra en corpus

TF-IDF = TF Ã— IDF
```

```python
from sklearn.feature_extraction.text import TfidfVectorizer

documents = [
    "machine learning is great",
    "deep learning is powerful",
    "learning is fun"
]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(documents)

# Ver matriz TF-IDF
import pandas as pd
df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
print(df)
```

### LibrerÃ­as Populares

| LibrerÃ­a | Uso Principal |
|----------|---------------|
| **NLTK** | TokenizaciÃ³n, stemming, POS tagging |
| **spaCy** | NLP avanzado, rÃ¡pido, eficiente |
| **TextBlob** | AnÃ¡lisis de sentimientos, simple |
| **Gensim** | Topic modeling, Word2Vec |
| **Transformers** | BERT, GPT, modelos pre-entrenados |

---

## Preprocesamiento de ImÃ¡genes

### Flujo TÃ­pico

```mermaid
graph LR
    A[Imagen Cruda] --> B[Carga]
    B --> C[Redimensionamiento]
    C --> D[NormalizaciÃ³n]
    D --> E[Aumento de Datos]
    E --> F[Modelo ML]

    style A fill:#ffcccc
    style F fill:#ccffcc
```

### TÃ©cnicas Comunes

#### 1. Carga y Redimensionamiento

```python
from PIL import Image
import numpy as np

# Cargar imagen
img = Image.open('image.jpg')

# Redimensionar
img_resized = img.resize((224, 224))  # TamaÃ±o comÃºn para CNNs

# Convertir a array numpy
img_array = np.array(img_resized)
print(img_array.shape)  # (224, 224, 3) para RGB
```

#### 2. NormalizaciÃ³n

```python
# Normalizar a [0, 1]
img_normalized = img_array / 255.0

# Estandarizar (ImageNet stats)
mean = np.array([0.485, 0.456, 0.406])
std = np.array([0.229, 0.224, 0.225])
img_standardized = (img_normalized - mean) / std
```

#### 3. Data Augmentation (Aumento de Datos)

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=20,        # Rotar Â±20 grados
    width_shift_range=0.2,    # Desplazar horizontalmente
    height_shift_range=0.2,   # Desplazar verticalmente
    horizontal_flip=True,     # Voltear horizontalmente
    zoom_range=0.2,           # Zoom in/out
    shear_range=0.2           # TransformaciÃ³n de corte
)

# Aplicar a imÃ¡genes
for batch in datagen.flow(images, batch_size=32):
    # Entrenar modelo con batch augmentado
    pass
```

### LibrerÃ­as Populares

| LibrerÃ­a | Uso Principal |
|----------|---------------|
| **PIL/Pillow** | Carga, manipulaciÃ³n bÃ¡sica |
| **OpenCV** | Procesamiento avanzado, rÃ¡pido |
| **scikit-image** | Filtros, transformaciones |
| **imgaug** | Data augmentation potente |
| **Albumentations** | Data augmentation rÃ¡pido |
| **torchvision** | Transformaciones para PyTorch |

---

## Pipeline Completo

### Ejemplo Integrado: Titanic

```python
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Cargar datos
df = pd.read_csv('titanic.csv')

# Definir tipos de features
numeric_features = ['Age', 'Fare']
categorical_features = ['Sex', 'Embarked']

# Pipeline para features numÃ©ricas
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Pipeline para features categÃ³ricas
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(drop='first', handle_unknown='ignore'))
])

# Combinar transformadores
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Pipeline completo con modelo
clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])

# Entrenar
X = df[['Age', 'Fare', 'Sex', 'Embarked']]
y = df['Survived']

clf.fit(X_train, y_train)
predictions = clf.predict(X_test)
```

### Diagrama del Pipeline

```mermaid
graph TD
    A[Datos Crudos] --> B{ColumnTransformer}

    B --> C[Numeric Pipeline]
    B --> D[Categorical Pipeline]

    C --> C1[SimpleImputer<br/>median]
    C1 --> C2[StandardScaler]

    D --> D1[SimpleImputer<br/>most_frequent]
    D1 --> D2[OneHotEncoder<br/>drop=first]

    C2 --> E[Concatenar]
    D2 --> E

    E --> F[RandomForest<br/>Classifier]
    F --> G[Predicciones]

    style A fill:#ffcccc
    style G fill:#ccffcc
```

---

## Mejores PrÃ¡cticas

### 1. Orden del Preprocesamiento

```mermaid
graph TD
    A[1. Limpieza<br/>Eliminar duplicados,<br/>errores obvios] --> B[2. ImputaciÃ³n<br/>Valores faltantes]
    B --> C[3. CodificaciÃ³n<br/>Variables categÃ³ricas]
    C --> D[4. Feature Engineering<br/>Crear nuevas features]
    D --> E[5. Escalado<br/>Estandarizar/Normalizar]
    E --> F[6. ReducciÃ³n<br/>PCA si es necesario]
```

### 2. Train-Test Split

âš ï¸ **IMPORTANTE**: Siempre divide ANTES de preprocesar.

```python
from sklearn.model_selection import train_test_split

# âœ… CORRECTO
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Ajustar preprocesador solo en train
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)  # Solo transform, NO fit

# âŒ INCORRECTO
scaler.fit(X)  # No uses TODOS los datos
X_scaled = scaler.transform(X)
X_train, X_test = train_test_split(X_scaled)  # Data leakage!
```

### 3. Usar Pipelines

âœ… **Ventajas**:
- Evita data leakage
- CÃ³digo mÃ¡s limpio
- FÃ¡cil de reproducir
- Compatible con GridSearchCV

### 4. Guardar Transformadores

```python
import joblib

# Guardar pipeline completo
joblib.dump(pipeline, 'model_pipeline.pkl')

# Cargar y usar
loaded_pipeline = joblib.load('model_pipeline.pkl')
predictions = loaded_pipeline.predict(new_data)
```

---

## Resumen Visual

```mermaid
mindmap
  root((Preprocesamiento<br/>ML))
    Valores Faltantes
      SimpleImputer
      KNNImputer
      IterativeImputer
      MissingIndicator
    CategÃ³ricas
      LabelEncoder
      OrdinalEncoder
      OneHotEncoder
      TargetEncoder
    Escalado
      StandardScaler
      MinMaxScaler
      RobustScaler
      MaxAbsScaler
      Normalizer
    Dimensionalidad
      PCA
      Feature Selection
      Feature Importance
    Feature Engineering
      Interacciones
      Binning
      Transformaciones
      Agregaciones
```

---

## Referencias

### DocumentaciÃ³n Oficial

1. **scikit-learn Preprocessing**: https://scikit-learn.org/stable/modules/preprocessing.html
2. **scikit-learn Imputation**: https://scikit-learn.org/stable/modules/impute.html
3. **scikit-learn Feature Selection**: https://scikit-learn.org/stable/modules/feature_selection.html
4. **scikit-learn PCA**: https://scikit-learn.org/stable/modules/decomposition.html#pca

### LibrerÃ­as Mencionadas

5. **NLTK**: https://www.nltk.org/
6. **spaCy**: https://spacy.io/
7. **OpenCV**: https://opencv.org/
8. **Pillow**: https://python-pillow.org/
9. **TensorFlow Image Preprocessing**: https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image
10. **Albumentations**: https://albumentations.ai/

### ArtÃ­culos y Tutoriales

11. **Feature Engineering for Machine Learning** - Jason Brownlee: https://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/
12. **A Comprehensive Guide to Data Imputation** - Towards Data Science
13. **Understanding PCA** - StatQuest: https://www.youtube.com/watch?v=FgakZw6K1QQ
14. **Target Encoding Done The Right Way** - Kaggle

### CÃ³digo de Ejemplos

15. **scikit-learn GitHub Repository**: https://github.com/scikit-learn/scikit-learn
16. **Context7 scikit-learn Documentation**: Usado para obtener ejemplos actualizados de cÃ³digo

---

**Licencia**: MIT License
**Autor**: David Palacio JimÃ©nez
**Fecha**: 2025
**VersiÃ³n**: 1.0
