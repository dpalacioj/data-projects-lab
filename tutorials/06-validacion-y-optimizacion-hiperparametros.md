# Tutorial: Validaci√≥n Cruzada y Optimizaci√≥n de Hiperpar√°metros

## √çndice

1. [Introducci√≥n](#introducci√≥n)
2. [Par√°metros vs Hiperpar√°metros](#par√°metros-vs-hiperpar√°metros)
3. [Validaci√≥n Cruzada (Cross-Validation)](#validaci√≥n-cruzada-cross-validation)
4. [Optimizaci√≥n de Hiperpar√°metros con scikit-learn](#optimizaci√≥n-de-hiperpar√°metros-con-scikit-learn)
5. [Optimizaci√≥n Avanzada con Optuna](#optimizaci√≥n-avanzada-con-optuna)
6. [Comparaci√≥n de M√©todos](#comparaci√≥n-de-m√©todos)
7. [Mejores Pr√°cticas](#mejores-pr√°cticas)
8. [Referencias](#referencias)

---

## Introducci√≥n

El **ajuste de hiperpar√°metros** (hyperparameter tuning o fine-tuning) es el proceso de encontrar la configuraci√≥n √≥ptima de un modelo de machine learning. Este tutorial explica:

- ‚úÖ Qu√© son los hiperpar√°metros y c√≥mo difieren de los par√°metros
- ‚úÖ C√≥mo validar modelos correctamente
- ‚úÖ Diferentes estrategias para buscar los mejores hiperpar√°metros
- ‚úÖ Herramientas modernas para optimizaci√≥n eficiente

### El Problema

```mermaid
graph TD
    A[Entrenar modelo con<br/>hiperpar√°metros aleatorios] --> B{¬øBuen<br/>rendimiento?}
    B -->|No| C[‚ùå Modelo pobre]
    B -->|Por suerte| D[‚ö†Ô∏è Puede ser overfitting]

    E[Optimizar hiperpar√°metros<br/>sistem√°ticamente] --> F[Validaci√≥n cruzada]
    F --> G[Buscar mejor<br/>configuraci√≥n]
    G --> H[‚úÖ Modelo robusto<br/>y generalizable]

    style C fill:#ffcccc
    style D fill:#ffffcc
    style H fill:#ccffcc
```

---

## Par√°metros vs Hiperpar√°metros

### Definiciones

```mermaid
mindmap
  root((Configuraci√≥n<br/>del Modelo))
    Par√°metros
      Se aprenden del entrenamiento
      Ejemplos: pesos, coeficientes
      El modelo los ajusta autom√°ticamente
    Hiperpar√°metros
      Se definen ANTES del entrenamiento
      Ejemplos: learning rate, max_depth
      El usuario debe configurarlos
```

---

### Comparaci√≥n Detallada

| Aspecto | Par√°metros | Hiperpar√°metros |
|---------|-----------|-----------------|
| **Definici√≥n** | Variables **internas** del modelo que se aprenden durante el entrenamiento | Variables **externas** que controlan el proceso de aprendizaje |
| **C√≥mo se obtienen** | El algoritmo los **ajusta autom√°ticamente** usando los datos | El usuario debe **especificarlos manualmente** |
| **¬øCu√°ndo se definen?** | **Durante** el entrenamiento | **Antes** del entrenamiento |
| **Ejemplos** | Pesos de red neuronal, coeficientes de regresi√≥n, vectores de soporte en SVM | Learning rate, n√∫mero de √°rboles, profundidad m√°xima, C en SVM |
| **¬øSe guardan en el modelo?** | ‚úÖ S√≠ | ‚ö†Ô∏è Se guardan pero como configuraci√≥n |
| **¬øSe pueden modificar despu√©s?** | ‚ùå No sin reentrenar | ‚úÖ S√≠, pero requiere reentrenar |

---

### Ejemplos por Algoritmo

#### 1. Regresi√≥n Lineal

```python
from sklearn.linear_model import Ridge

# Crear modelo
model = Ridge(alpha=1.0)  # alpha es un HIPERPAR√ÅMETRO
model.fit(X_train, y_train)

# Los coeficientes son PAR√ÅMETROS (aprendidos)
print("Par√°metros (coeficientes):", model.coef_)
print("Par√°metro (intercepto):", model.intercept_)
```

**Tabla de Par√°metros vs Hiperpar√°metros**:

| Tipo | Nombre | Descripci√≥n |
|------|--------|-------------|
| **Hiperpar√°metro** | `alpha` | Fuerza de regularizaci√≥n (lo defines t√∫) |
| **Par√°metro** | `coef_` | Pesos de cada caracter√≠stica (aprendidos) |
| **Par√°metro** | `intercept_` | T√©rmino independiente (aprendido) |

---

#### 2. Random Forest

```python
from sklearn.ensemble import RandomForestClassifier

# Crear modelo con HIPERPAR√ÅMETROS
model = RandomForestClassifier(
    n_estimators=100,        # Hiperpar√°metro
    max_depth=10,           # Hiperpar√°metro
    min_samples_split=5,    # Hiperpar√°metro
    random_state=42
)

model.fit(X_train, y_train)

# Los √°rboles internos son PAR√ÅMETROS (aprendidos)
# Cada nodo de cada √°rbol tiene condiciones aprendidas
```

**Hiperpar√°metros comunes**:

| Hiperpar√°metro | Qu√© Controla | Valores T√≠picos |
|----------------|--------------|-----------------|
| `n_estimators` | N√∫mero de √°rboles | 50, 100, 200, 500 |
| `max_depth` | Profundidad m√°xima de √°rboles | 5, 10, 20, None |
| `min_samples_split` | M√≠nimo de muestras para dividir | 2, 5, 10 |
| `min_samples_leaf` | M√≠nimo de muestras en hoja | 1, 2, 4 |
| `max_features` | Features a considerar en cada split | 'sqrt', 'log2', None |

---

#### 3. Red Neuronal

```python
from sklearn.neural_network import MLPClassifier

# HIPERPAR√ÅMETROS de arquitectura y entrenamiento
model = MLPClassifier(
    hidden_layer_sizes=(100, 50),  # Hiperpar√°metro: arquitectura
    activation='relu',             # Hiperpar√°metro: funci√≥n de activaci√≥n
    learning_rate_init=0.001,      # Hiperpar√°metro: tasa de aprendizaje
    max_iter=200,                  # Hiperpar√°metro: √©pocas
    random_state=42
)

model.fit(X_train, y_train)

# PAR√ÅMETROS (pesos y sesgos de cada capa)
print("Capas:", len(model.coefs_))  # N√∫mero de capas
print("Pesos capa 1:", model.coefs_[0].shape)
print("Sesgos capa 1:", model.intercepts_[0].shape)
```

---

### Visualizaci√≥n del Proceso

```mermaid
graph LR
    A[Usuario define<br/>HIPERPAR√ÅMETROS] --> B[Crear modelo]
    B --> C[Entrenar con datos]
    C --> D[Modelo aprende<br/>PAR√ÅMETROS]
    D --> E[Modelo entrenado]

    style A fill:#ffffcc
    style D fill:#ccffcc
```

**Ejemplo Conceptual**:

```
üéØ HIPERPAR√ÅMETROS (t√∫ decides):
   - Profundidad del √°rbol: 10
   - N√∫mero de √°rboles: 100
   - M√≠nimo de muestras: 5

       ‚¨á ENTRENAR ‚¨á

üìä PAR√ÅMETROS (el modelo aprende):
   - √Årbol 1: Si edad > 35 ‚Üí Si salario > 50k ‚Üí Clase A
   - √Årbol 2: Si ciudad == "NYC" ‚Üí Si edad < 25 ‚Üí Clase B
   - ... (98 √°rboles m√°s con sus reglas)
```

---

### ¬øPor qu√© importa esta distinci√≥n?

```mermaid
graph TD
    A{¬øQuiero mejorar<br/>el modelo?} --> B[Ajustar<br/>HIPERPAR√ÅMETROS]
    B --> C[Requiere buscar<br/>mejor configuraci√≥n]
    C --> D[GridSearch,<br/>RandomSearch, Optuna]

    A --> E[Entrenar con<br/>m√°s datos]
    E --> F[Aprende mejores<br/>PAR√ÅMETROS]

    style B fill:#ffffcc
    style E fill:#ccffcc
```

| Objetivo | Acci√≥n |
|----------|--------|
| **Reducir overfitting** | Ajustar hiperpar√°metros de regularizaci√≥n (`alpha`, `max_depth`) |
| **Mejorar precisi√≥n** | Buscar mejores hiperpar√°metros con validaci√≥n cruzada |
| **Acelerar entrenamiento** | Ajustar `learning_rate`, `batch_size` |
| **Modelo m√°s simple** | Reducir `n_estimators`, `hidden_layers` |

---

## Validaci√≥n Cruzada (Cross-Validation)

### ¬øPor qu√© NO basta con Train/Test Split?

```python
# ‚ùå PROBLEMA: Split simple
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model.fit(X_train, y_train)
score = model.score(X_test, y_test)
```

**Problemas**:

1. ‚ö†Ô∏è El resultado depende de la **suerte** del split
2. ‚ö†Ô∏è 20% de datos "desperdiciados" (solo para test)
3. ‚ö†Ô∏è No sabes si el modelo **generaliza bien** o tuvo suerte

```mermaid
graph LR
    A[Dataset completo] --> B[Split 1]
    A --> C[Split 2]
    A --> D[Split 3]

    B --> B1[Train: 80%<br/>Test: 20%<br/>Score: 0.85]
    C --> C1[Train: 80%<br/>Test: 20%<br/>Score: 0.78]
    D --> D1[Train: 80%<br/>Test: 20%<br/>Score: 0.92]

    B1 --> E{¬øCu√°l es<br/>el verdadero<br/>rendimiento?}
    C1 --> E
    D1 --> E

    style E fill:#ffcccc
```

---

### Soluci√≥n: Validaci√≥n Cruzada

**Concepto**: Dividir los datos en **m√∫ltiples particiones** (folds) y usar cada partici√≥n como test una vez.

```mermaid
graph TD
    A[Dataset completo] --> B[Dividir en K folds]
    B --> C[Fold 1]
    B --> D[Fold 2]
    B --> E[Fold 3]
    B --> F[Fold K]

    C --> C1[Train: F2+F3+...Fk<br/>Test: F1<br/>Score 1]
    D --> D1[Train: F1+F3+...Fk<br/>Test: F2<br/>Score 2]
    E --> E1[Train: F1+F2+F4+...Fk<br/>Test: F3<br/>Score 3]
    F --> F1[Train: F1+F2+...Fk-1<br/>Test: Fk<br/>Score K]

    C1 --> G[Promedio de<br/>todos los scores]
    D1 --> G
    E1 --> G
    F1 --> G

    style G fill:#ccffcc
```

**Ventajas**:

‚úÖ Usa **todos los datos** para entrenamiento y validaci√≥n
‚úÖ Resultados m√°s **confiables** (promedio de K experimentos)
‚úÖ Detecta **overfitting** si hay gran varianza entre folds

---

### Tipos de Validaci√≥n Cruzada

```mermaid
mindmap
  root((Validaci√≥n<br/>Cruzada))
    K-Fold CV
      Datos balanceados
      Clasificaci√≥n/Regresi√≥n
    Stratified K-Fold
      Clasificaci√≥n
      Mantiene proporci√≥n de clases
    Leave-One-Out LOO
      Datasets peque√±os
      Muy costoso
    Time Series Split
      Series temporales
      Respeta orden temporal
    Group K-Fold
      Datos agrupados
      Evita data leakage
```

---

### 1. K-Fold Cross-Validation

**Concepto**: Divide datos en K particiones iguales. Cada partici√≥n se usa como test una vez.

#### Visualizaci√≥n

```
Dataset: [‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†] (100 muestras)

K=5 folds:

Iteraci√≥n 1: [‚ñà‚ñà‚ñà‚ñà|¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑]  Test: 20 muestras
             Train ---------> (80 muestras)

Iteraci√≥n 2: [¬∑¬∑¬∑¬∑|‚ñà‚ñà‚ñà‚ñà|¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑]  Test: 20 muestras
              ----  Train ---------> (80 muestras)

Iteraci√≥n 3: [¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑|‚ñà‚ñà‚ñà‚ñà|¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑]  Test: 20 muestras
              --------  Train --> (80 muestras)

Iteraci√≥n 4: [¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑|‚ñà‚ñà‚ñà‚ñà|¬∑¬∑¬∑¬∑]  Test: 20 muestras
              ----------  Train --> (80 muestras)

Iteraci√≥n 5: [¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑|¬∑¬∑¬∑¬∑|‚ñà‚ñà‚ñà‚ñà]  Test: 20 muestras
              ----------  Train --> (80 muestras)

Resultado: Promedio de 5 scores
```

#### C√≥digo con scikit-learn

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier

# Crear modelo
model = RandomForestClassifier(n_estimators=100)

# K-Fold con 5 particiones
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Realizar validaci√≥n cruzada
scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')

print(f"Scores por fold: {scores}")
print(f"Accuracy promedio: {scores.mean():.3f} (+/- {scores.std():.3f})")

# Ejemplo de salida:
# Scores por fold: [0.85, 0.82, 0.88, 0.84, 0.86]
# Accuracy promedio: 0.850 (+/- 0.021)
```

#### Par√°metros de KFold

| Par√°metro | Descripci√≥n | Valores Comunes |
|-----------|-------------|-----------------|
| `n_splits` | N√∫mero de folds (K) | 5, 10 |
| `shuffle` | Mezclar datos antes de dividir | `True` (recomendado) |
| `random_state` | Semilla para reproducibilidad | 42, 0 |

---

### 2. Stratified K-Fold

**Problema con K-Fold regular**: En clasificaci√≥n desbalanceada, algunos folds pueden no tener muestras de ciertas clases.

```
Dataset desbalanceado: 90% Clase A, 10% Clase B

‚ùå K-Fold regular puede crear:
   Fold 1: 100% Clase A (sin Clase B)
   Fold 2: 95% Clase A, 5% Clase B
   ...

‚úÖ Stratified K-Fold mantiene proporciones:
   Fold 1: 90% Clase A, 10% Clase B
   Fold 2: 90% Clase A, 10% Clase B
   ...
```

#### Visualizaci√≥n

```
Dataset Original:
Clase A: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (90%)
Clase B: ‚ñà‚ñà‚ñà‚ñà (10%)

Stratified K-Fold (K=5):
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Fold 1: 90% A + 10% B                ‚îÇ
‚îÇ Fold 2: 90% A + 10% B                ‚îÇ
‚îÇ Fold 3: 90% A + 10% B                ‚îÇ
‚îÇ Fold 4: 90% A + 10% B                ‚îÇ
‚îÇ Fold 5: 90% A + 10% B                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### C√≥digo con scikit-learn

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score

# Crear modelo
model = RandomForestClassifier(n_estimators=100)

# Stratified K-Fold
stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Validaci√≥n cruzada estratificada
scores = cross_val_score(model, X, y, cv=stratified_kfold, scoring='accuracy')

print(f"Scores por fold: {scores}")
print(f"Accuracy promedio: {scores.mean():.3f}")
```

#### Cu√°ndo Usar

| Usar Stratified K-Fold | Usar K-Fold Regular |
|------------------------|---------------------|
| ‚úÖ Clasificaci√≥n | ‚úÖ Regresi√≥n |
| ‚úÖ Clases desbalanceadas | ‚úÖ Clases balanceadas |
| ‚úÖ Pocas muestras por clase | - |

---

### 3. Leave-One-Out (LOO)

**Concepto**: Caso extremo de K-Fold donde **K = n√∫mero de muestras**. Cada muestra es un fold.

```
Dataset: 10 muestras [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

Iteraci√≥n 1:  Test: [1]    Train: [2,3,4,5,6,7,8,9,10]
Iteraci√≥n 2:  Test: [2]    Train: [1,3,4,5,6,7,8,9,10]
Iteraci√≥n 3:  Test: [3]    Train: [1,2,4,5,6,7,8,9,10]
...
Iteraci√≥n 10: Test: [10]   Train: [1,2,3,4,5,6,7,8,9]

Total: 10 modelos entrenados
```

#### C√≥digo con scikit-learn

```python
from sklearn.model_selection import LeaveOneOut, cross_val_score

# Crear modelo
model = RandomForestClassifier(n_estimators=50)

# Leave-One-Out
loo = LeaveOneOut()

# Validaci√≥n cruzada (¬°LENTO para datasets grandes!)
scores = cross_val_score(model, X, y, cv=loo)

print(f"N√∫mero de iteraciones: {len(scores)}")  # = n√∫mero de muestras
print(f"Accuracy promedio: {scores.mean():.3f}")
```

#### Ventajas y Desventajas

| ‚úÖ Ventajas | ‚ùå Desventajas |
|------------|----------------|
| Usa TODOS los datos | **MUY lento** (n entrenamientos) |
| Sin sesgo por split | Alta varianza en resultados |
| M√°xima utilizaci√≥n de datos | No pr√°ctico para datasets grandes |

**Cu√°ndo usar**: Solo con **datasets muy peque√±os** (< 100 muestras).

---

### 4. Time Series Split

**Problema**: Con series temporales, **NO puedes mezclar los datos**. El futuro no puede predecir el pasado.

```
‚ùå K-Fold regular (mezcla datos):
[Ene, Feb, Mar, Abr, May, Jun, Jul, Ago, Sep, Oct, Nov, Dic]
Fold 1: Train [Ene, Mar, May, Jul, Sep, Nov]  Test [Feb, Abr, Jun, Ago, Oct, Dic]
                     ‚Üë Usa datos del futuro para predecir el pasado ‚ùå

‚úÖ Time Series Split (respeta orden):
[Ene, Feb, Mar, Abr, May, Jun, Jul, Ago, Sep, Oct, Nov, Dic]
Fold 1: Train [Ene, Feb, Mar]          Test [Abr]
Fold 2: Train [Ene, Feb, Mar, Abr]     Test [May]
Fold 3: Train [Ene, ..., May]          Test [Jun]
...
```

#### Visualizaci√≥n

```mermaid
graph TD
    A[Serie Temporal] --> B[Split 1]
    A --> C[Split 2]
    A --> D[Split 3]
    A --> E[Split N]

    B --> B1["Train: ‚ñà‚ñà‚ñà‚ñà<br/>Test: ‚ñà<br/>(solo futuro)"]
    C --> C1["Train: ‚ñà‚ñà‚ñà‚ñà‚ñà<br/>Test: ‚ñà<br/>(solo futuro)"]
    D --> D1["Train: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà<br/>Test: ‚ñà<br/>(solo futuro)"]
    E --> E1["Train: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà<br/>Test: ‚ñà<br/>(solo futuro)"]

    style B1 fill:#ccffcc
    style C1 fill:#ccffcc
    style D1 fill:#ccffcc
    style E1 fill:#ccffcc
```

#### C√≥digo con scikit-learn

```python
from sklearn.model_selection import TimeSeriesSplit, cross_val_score

# Crear modelo
model = RandomForestRegressor()

# Time Series Split con 5 splits
tscv = TimeSeriesSplit(n_splits=5)

# Validaci√≥n cruzada temporal
scores = cross_val_score(model, X, y, cv=tscv, scoring='neg_mean_squared_error')

print(f"MSE por split: {-scores}")  # Negativo porque scoring es 'neg_mse'
print(f"MSE promedio: {-scores.mean():.3f}")

# Visualizar los splits
for i, (train_index, test_index) in enumerate(tscv.split(X)):
    print(f"Split {i+1}:")
    print(f"  Train: √≠ndices {train_index[0]} a {train_index[-1]}")
    print(f"  Test:  √≠ndices {test_index[0]} a {test_index[-1]}")
```

---

### 5. Group K-Fold

**Problema**: Cuando tienes **grupos** en tus datos (ej: m√∫ltiples mediciones del mismo paciente), debes asegurar que muestras del mismo grupo NO est√©n en train y test simult√°neamente.

```
Dataset: Pacientes con m√∫ltiples visitas

‚ùå K-Fold regular:
   Paciente A: Visita 1 (Train), Visita 2 (Test)  ‚Üê Data leakage!

‚úÖ Group K-Fold:
   Paciente A: Todas sus visitas en Train
   Paciente B: Todas sus visitas en Test
```

#### C√≥digo con scikit-learn

```python
from sklearn.model_selection import GroupKFold

# IDs de grupo (ej: ID de paciente)
groups = [1, 1, 1, 2, 2, 3, 3, 3, 4, 4]
#         ‚îî‚îÄA‚îÄ‚îò  ‚îîB‚îò  ‚îî‚îÄC‚îÄ‚îò  ‚îîD‚îò

# Group K-Fold
gkf = GroupKFold(n_splits=3)

for i, (train_idx, test_idx) in enumerate(gkf.split(X, y, groups=groups)):
    print(f"Fold {i+1}:")
    print(f"  Train groups: {set([groups[i] for i in train_idx])}")
    print(f"  Test groups:  {set([groups[i] for i in test_idx])}")

# Salida:
# Fold 1:
#   Train groups: {2, 3, 4}
#   Test groups:  {1}
```

---

### Comparaci√≥n de M√©todos de Validaci√≥n

```mermaid
graph TD
    A{¬øQu√© tipo<br/>de datos?} -->|Clasificaci√≥n| B{¬øBalanceado?}
    A -->|Regresi√≥n| C[K-Fold]
    A -->|Series Temporales| D[TimeSeriesSplit]
    A -->|Con grupos| E[GroupKFold]

    B -->|S√≠| C
    B -->|No| F[StratifiedKFold]

    G{¬øCu√°ntos datos?} -->|Muy pocos<br/><100| H[LeaveOneOut]
    G -->|Normal| C

    style C fill:#ccffcc
    style F fill:#ffffcc
    style D fill:#cce5ff
    style E fill:#ffcccc
    style H fill:#ffeecc
```

### Tabla Comparativa Completa

| M√©todo | Tipo de Datos | Ventajas | Desventajas | Cu√°ndo Usar |
|--------|---------------|----------|-------------|-------------|
| **K-Fold** | Regresi√≥n, Clasificaci√≥n balanceada | Simple, eficiente | Puede desbalancear clases | General purpose |
| **Stratified K-Fold** | Clasificaci√≥n | Mantiene proporci√≥n de clases | Solo para clasificaci√≥n | Clases desbalanceadas |
| **Leave-One-Out** | Cualquiera | M√°ximo uso de datos | Muy lento, alta varianza | Datasets muy peque√±os |
| **Time Series Split** | Series temporales | Respeta temporalidad | Requiere orden | Forecasting, finanzas |
| **Group K-Fold** | Datos agrupados | Evita data leakage | Requiere definir grupos | M√∫ltiples mediciones |

---

## Optimizaci√≥n de Hiperpar√°metros con scikit-learn

### Flujo General

```mermaid
graph LR
    A[Definir espacio<br/>de b√∫squeda] --> B[Elegir m√©todo<br/>de b√∫squeda]
    B --> C[GridSearchCV o<br/>RandomizedSearchCV]
    C --> D[Ejecutar b√∫squeda<br/>con CV]
    D --> E[Obtener mejores<br/>hiperpar√°metros]
    E --> F[Entrenar modelo<br/>final]

    style A fill:#ffffcc
    style E fill:#ccffcc
    style F fill:#aaffaa
```

---

### 1. GridSearchCV (B√∫squeda Exhaustiva)

**Concepto**: Prueba **todas las combinaciones posibles** de hiperpar√°metros.

#### ¬øC√≥mo Funciona?

```
Espacio de b√∫squeda:
- n_estimators: [50, 100, 200]
- max_depth: [5, 10, 15]
- min_samples_split: [2, 5]

Grid completo:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Combinaci√≥n 1: n=50,  d=5,  s=2                ‚îÇ
‚îÇ Combinaci√≥n 2: n=50,  d=5,  s=5                ‚îÇ
‚îÇ Combinaci√≥n 3: n=50,  d=10, s=2                ‚îÇ
‚îÇ Combinaci√≥n 4: n=50,  d=10, s=5                ‚îÇ
‚îÇ Combinaci√≥n 5: n=50,  d=15, s=2                ‚îÇ
‚îÇ Combinaci√≥n 6: n=50,  d=15, s=5                ‚îÇ
‚îÇ Combinaci√≥n 7: n=100, d=5,  s=2                ‚îÇ
‚îÇ ... (18 combinaciones en total = 3√ó3√ó2)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Cada combinaci√≥n se eval√∫a con K-Fold CV (ej: K=5)
Total de entrenamientos: 18 √ó 5 = 90
```

#### C√≥digo con scikit-learn

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# 1. Definir modelo base
model = RandomForestClassifier(random_state=42)

# 2. Definir espacio de b√∫squeda (grid)
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# 3. Crear GridSearchCV
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=5,                    # 5-fold cross-validation
    scoring='accuracy',      # M√©trica a optimizar
    n_jobs=-1,              # Usar todos los CPUs
    verbose=2                # Mostrar progreso
)

# 4. Ejecutar b√∫squeda
grid_search.fit(X_train, y_train)

# 5. Mejores hiperpar√°metros
print("Mejores hiperpar√°metros:", grid_search.best_params_)
print("Mejor score:", grid_search.best_score_)

# 6. Usar mejor modelo
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test)

# Ejemplo de salida:
# Mejores hiperpar√°metros: {'max_depth': 10, 'min_samples_leaf': 1,
#                            'min_samples_split': 2, 'n_estimators': 200}
# Mejor score: 0.8765
```

#### Analizar Resultados

```python
import pandas as pd

# Ver todos los resultados
results = pd.DataFrame(grid_search.cv_results_)

# Columnas importantes
results[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values('rank_test_score')

# Top 5 configuraciones
print(results.nsmallest(5, 'rank_test_score')[['params', 'mean_test_score']])
```

#### Ventajas y Desventajas

| ‚úÖ Ventajas | ‚ùå Desventajas |
|------------|----------------|
| Prueba TODAS las combinaciones | **Muy lento** con muchos hiperpar√°metros |
| Garantiza encontrar el mejor dentro del grid | Crece exponencialmente |
| Reproducible | No escala bien |
| Simple de entender | Puede no explorar bien el espacio |

**Complejidad**: Si tienes **n** hiperpar√°metros con **m** valores cada uno:
- Combinaciones: $m^n$
- Con K-Fold CV: $m^n \times K$ entrenamientos

Ejemplo: 5 hiperpar√°metros con 4 valores = $4^5 = 1024$ combinaciones √ó 5 folds = **5,120 entrenamientos** üò±

---

### 2. RandomizedSearchCV (B√∫squeda Aleatoria)

**Concepto**: En lugar de probar TODAS las combinaciones, prueba un **n√∫mero fijo de combinaciones aleatorias**.

#### Comparaci√≥n Visual

```
GridSearchCV (exhaustivo):
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ  Prueba TODAS
‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ  las celdas
‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ  (25 combinaciones)
‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ
‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ ‚úì ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò

RandomizedSearchCV (muestreo):
‚îå‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   ‚îÇ ‚úì ‚îÇ   ‚îÇ   ‚îÇ ‚úì ‚îÇ  Prueba solo n_iter
‚îÇ ‚úì ‚îÇ   ‚îÇ   ‚îÇ ‚úì ‚îÇ   ‚îÇ  combinaciones aleatorias
‚îÇ   ‚îÇ   ‚îÇ ‚úì ‚îÇ   ‚îÇ   ‚îÇ  (10 combinaciones)
‚îÇ   ‚îÇ ‚úì ‚îÇ   ‚îÇ   ‚îÇ ‚úì ‚îÇ
‚îÇ ‚úì ‚îÇ   ‚îÇ   ‚îÇ ‚úì ‚îÇ   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îò
M√°s r√°pido pero puede perder el √≥ptimo global
```

#### C√≥digo con scikit-learn

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# 1. Definir modelo
model = RandomForestClassifier(random_state=42)

# 2. Definir distribuciones de hiperpar√°metros
param_distributions = {
    'n_estimators': randint(50, 500),           # Enteros entre 50-500
    'max_depth': randint(5, 50),                # Enteros entre 5-50
    'min_samples_split': randint(2, 20),        # Enteros entre 2-20
    'min_samples_leaf': randint(1, 10),         # Enteros entre 1-10
    'max_features': uniform(0.1, 0.9)           # Floats entre 0.1-1.0
}

# 3. Crear RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_distributions,
    n_iter=50,               # N√∫mero de combinaciones a probar
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=2
)

# 4. Ejecutar b√∫squeda
random_search.fit(X_train, y_train)

# 5. Mejores hiperpar√°metros
print("Mejores hiperpar√°metros:", random_search.best_params_)
print("Mejor score:", random_search.best_score_)
```

#### Distribuciones Comunes

| Distribuci√≥n | Uso | Ejemplo |
|--------------|-----|---------|
| `randint(low, high)` | Enteros discretos | `n_estimators`, `max_depth` |
| `uniform(low, width)` | Floats continuos | `learning_rate`, `alpha` |
| `loguniform(low, high)` | Floats en escala logar√≠tmica | `C` en SVM, regularizaci√≥n |
| Lista | Categ√≥ricos | `['gini', 'entropy']` |

```python
from scipy.stats import randint, uniform, loguniform

param_dist = {
    # Entero: 10-100
    'n_estimators': randint(10, 100),

    # Float lineal: 0.0-1.0
    'subsample': uniform(0.5, 0.5),  # uniform(loc, scale) ‚Üí [0.5, 1.0]

    # Float logar√≠tmico: 0.0001-0.1
    'learning_rate': loguniform(1e-4, 1e-1),

    # Categ√≥rico
    'criterion': ['gini', 'entropy']
}
```

#### Ventajas y Desventajas

| ‚úÖ Ventajas | ‚ùå Desventajas |
|------------|----------------|
| **Mucho m√°s r√°pido** que GridSearch | No garantiza encontrar el √≥ptimo |
| Explora mejor espacios continuos | Puede repetir combinaciones |
| Puedes controlar el tiempo (n_iter) | Requiere entender distribuciones |
| Bueno para muchos hiperpar√°metros | Menos reproducible |

**Cu√°ndo usar**: Espacio de b√∫squeda **grande** o hiperpar√°metros **continuos**.

---

### 3. HalvingGridSearchCV / HalvingRandomSearchCV

**Concepto**: Variante **eficiente** que elimina candidatos pobres progresivamente usando **Successive Halving**.

#### ¬øC√≥mo Funciona?

```mermaid
graph TD
    A[Iteraci√≥n 1:<br/>Todos los candidatos<br/>Pocos recursos] --> B[Evaluar todos]
    B --> C[Eliminar peores<br/>50%]
    C --> D[Iteraci√≥n 2:<br/>Mejores 50%<br/>M√°s recursos]
    D --> E[Evaluar supervivientes]
    E --> F[Eliminar peores<br/>50%]
    F --> G[Iteraci√≥n 3:<br/>Mejores 25%<br/>Todos los recursos]
    G --> H[Mejor candidato]

    style A fill:#ffcccc
    style D fill:#ffffcc
    style G fill:#ccffcc
    style H fill:#aaffaa
```

#### Ejemplo Visual

```
64 candidatos iniciales
‚îú‚îÄ Iteraci√≥n 1: Entrenar con 100 muestras
‚îÇ  ‚îî‚îÄ Eliminar 32 peores ‚Üí Quedan 32
‚îÇ
‚îú‚îÄ Iteraci√≥n 2: Entrenar con 400 muestras
‚îÇ  ‚îî‚îÄ Eliminar 16 peores ‚Üí Quedan 16
‚îÇ
‚îú‚îÄ Iteraci√≥n 3: Entrenar con 1600 muestras
‚îÇ  ‚îî‚îÄ Eliminar 8 peores ‚Üí Quedan 8
‚îÇ
‚îî‚îÄ Iteraci√≥n 4: Entrenar con 6400 muestras
   ‚îî‚îÄ Elegir el mejor

‚úÖ Ahorro: ~75% de entrenamientos vs GridSearch
```

#### C√≥digo con scikit-learn

```python
from sklearn.experimental import enable_halving_search_cv  # ¬°Necesario!
from sklearn.model_selection import HalvingGridSearchCV

# Definir modelo y param_grid igual que GridSearchCV
model = RandomForestClassifier(random_state=42)

param_grid = {
    'n_estimators': [50, 100, 200, 500],
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10]
}

# HalvingGridSearchCV
halving_search = HalvingGridSearchCV(
    estimator=model,
    param_grid=param_grid,
    factor=3,                # Factor de reducci√≥n (elimina 2/3 en cada iter)
    resource='n_samples',    # Recurso a aumentar: muestras o n_estimators
    max_resources='auto',    # M√°ximo de recursos disponibles
    cv=5,
    random_state=42,
    verbose=1
)

halving_search.fit(X_train, y_train)

print("Mejores hiperpar√°metros:", halving_search.best_params_)
```

#### Par√°metros Importantes

| Par√°metro | Descripci√≥n | Valores |
|-----------|-------------|---------|
| `factor` | Factor de eliminaci√≥n | 2, 3 (elimina 1/2 o 2/3) |
| `resource` | Qu√© aumentar en cada iter | `'n_samples'`, `'n_estimators'` |
| `max_resources` | M√°ximo de recursos | `'auto'` o n√∫mero |
| `aggressive_elimination` | Forzar factor en √∫ltima iter | `True`, `False` |

---

### Comparaci√≥n: Grid vs Randomized vs Halving

```mermaid
graph LR
    A[Espacio de<br/>Hiperpar√°metros] --> B[GridSearchCV<br/>Exhaustivo]
    A --> C[RandomizedSearchCV<br/>Muestreo]
    A --> D[HalvingSearchCV<br/>Eliminaci√≥n Progresiva]

    B --> B1[Lento ‚è±Ô∏è‚è±Ô∏è‚è±Ô∏è<br/>Completo ‚úÖ]
    C --> C1[R√°pido ‚è±Ô∏è<br/>Bueno ‚ö†Ô∏è]
    D --> D1[Medio ‚è±Ô∏è‚è±Ô∏è<br/>Eficiente üöÄ]

    style B fill:#ffcccc
    style C fill:#ffffcc
    style D fill:#ccffcc
```

### Tabla Comparativa

| M√©todo | Velocidad | Cobertura | Mejor Para | Complejidad |
|--------|-----------|-----------|------------|-------------|
| **GridSearchCV** | ‚è±Ô∏è‚è±Ô∏è‚è±Ô∏è Muy lento | ‚úÖ 100% del grid | Espacios peque√±os, pocos hiper | üü¢ Simple |
| **RandomizedSearchCV** | ‚è±Ô∏è‚è±Ô∏è R√°pido | ‚ö†Ô∏è Muestreo aleatorio | Espacios grandes, continuos | üü° Media |
| **HalvingGridSearchCV** | ‚è±Ô∏è‚è±Ô∏è Medio | ‚úÖ 100% del grid (eficiente) | Espacios medianos | üü° Media |
| **HalvingRandomSearchCV** | ‚è±Ô∏è Muy r√°pido | ‚ö†Ô∏è Muestreo aleatorio (eficiente) | Espacios muy grandes | üü° Media |

### Ejemplo de Tiempos

```
Dataset: 10,000 muestras
Hiperpar√°metros: 5 par√°metros con 4 valores cada uno = 1024 combinaciones
K-Fold: 5 folds

GridSearchCV:        1024 √ó 5 = 5,120 entrenamientos  (~8 horas)
RandomizedSearchCV:   100 √ó 5 =   500 entrenamientos  (~50 min)
HalvingGridSearchCV:  ~300 √ó 5 = 1,500 entrenamientos (~2.5 horas)
```

---

## Optimizaci√≥n Avanzada con Optuna

### ¬øQu√© es Optuna?

**Optuna** es un framework de optimizaci√≥n de hiperpar√°metros que usa **algoritmos inteligentes** para encontrar mejores configuraciones **m√°s r√°pido** que Grid/Random Search.

### Grid/Random Search vs Optuna

```mermaid
graph TD
    A[GridSearch/<br/>RandomSearch] --> B[B√∫squeda ciega]
    B --> C[No aprende de<br/>intentos previos]

    D[Optuna] --> E[B√∫squeda inteligente]
    E --> F[Aprende de cada trial]
    F --> G[Sugiere mejores<br/>hiperpar√°metros]

    style A fill:#ffcccc
    style D fill:#ccffcc
```

### Diferencias Clave

| Aspecto | GridSearch/RandomSearch | Optuna |
|---------|------------------------|--------|
| **Estrategia** | B√∫squeda ciega (no aprende) | B√∫squeda bayesiana (aprende) |
| **Eficiencia** | Muchas evaluaciones desperdiciadas | Menos evaluaciones necesarias |
| **Flexibilidad** | Grid est√°tico | Espacio din√°mico |
| **Algoritmo** | Fuerza bruta / Aleatorio | Tree-structured Parzen Estimator (TPE) |
| **Early Stopping** | ‚ùå No soportado | ‚úÖ Pruning de trials pobres |
| **Distribuciones** | Listas fijas | Continuas, categ√≥ricas, log-uniform |

---

### Conceptos B√°sicos de Optuna

```mermaid
graph LR
    A[Study] --> B[Trial 1]
    A --> C[Trial 2]
    A --> D[Trial N]

    B --> B1[Suggest<br/>hiperpar√°metros]
    B1 --> B2[Entrenar<br/>modelo]
    B2 --> B3[Devolver<br/>score]

    C --> C1[Suggest mejores<br/>hiperpar√°metros]
    C1 --> C2[Entrenar]
    C2 --> C3[Devolver<br/>score]

    D --> D1[Encontrar<br/>mejor trial]

    style A fill:#ffffcc
    style D1 fill:#ccffcc
```

**Terminolog√≠a**:

| T√©rmino | Definici√≥n |
|---------|------------|
| **Study** | Proceso de optimizaci√≥n completo |
| **Trial** | Una evaluaci√≥n individual (un set de hiperpar√°metros) |
| **Objective Function** | Funci√≥n que entrena el modelo y devuelve score |
| **Suggest** | M√©todo para proponer valores de hiperpar√°metros |
| **Sampler** | Algoritmo que sugiere valores (TPE, Random, CMA-ES) |
| **Pruner** | Detiene trials pobres tempranamente |

---

### Ejemplo B√°sico con Optuna

```python
import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_iris

# 1. Cargar datos
X, y = load_iris(return_X_y=True)

# 2. Definir funci√≥n objetivo
def objective(trial):
    # Sugerir hiperpar√°metros
    n_estimators = trial.suggest_int('n_estimators', 50, 500)
    max_depth = trial.suggest_int('max_depth', 2, 32)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)

    # Crear modelo con hiperpar√°metros sugeridos
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        random_state=42
    )

    # Evaluar con cross-validation
    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()

    return score  # Optuna MAXIMIZA o MINIMIZA este valor

# 3. Crear study
study = optuna.create_study(direction='maximize')  # Maximizar accuracy

# 4. Optimizar
study.optimize(objective, n_trials=100)

# 5. Mejores hiperpar√°metros
print("Mejores hiperpar√°metros:", study.best_params)
print("Mejor score:", study.best_value)
print("Mejor trial:", study.best_trial)

# Ejemplo de salida:
# Mejores hiperpar√°metros: {'n_estimators': 324, 'max_depth': 18,
#                            'min_samples_split': 3, 'min_samples_leaf': 2}
# Mejor score: 0.9733
```

---

### Tipos de Suggest

#### 1. Suggest Integer

```python
def objective(trial):
    # Entero uniforme
    n_estimators = trial.suggest_int('n_estimators', 10, 1000)

    # Entero logar√≠tmico (√∫til para b√∫squeda exponencial)
    n_estimators_log = trial.suggest_int('n_estimators', 10, 1000, log=True)

    # Entero con step
    batch_size = trial.suggest_int('batch_size', 16, 256, step=16)
    # Valores posibles: 16, 32, 48, 64, ..., 256
```

#### 2. Suggest Float

```python
def objective(trial):
    # Float uniforme
    learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.1)

    # Float logar√≠tmico (para learning rates, regularizaci√≥n)
    lr_log = trial.suggest_float('lr', 1e-5, 1e-1, log=True)

    # Float con step
    dropout = trial.suggest_float('dropout', 0.1, 0.9, step=0.1)
    # Valores: 0.1, 0.2, 0.3, ..., 0.9
```

#### 3. Suggest Categorical

```python
def objective(trial):
    # Categ√≥rico (para opciones discretas)
    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd', 'rmsprop'])

    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid'])

    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])
```

---

### Ejemplo Avanzado: Multiple Modelos

```python
import optuna
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

def objective(trial):
    # Sugerir qu√© modelo usar
    classifier_name = trial.suggest_categorical('classifier', ['SVC', 'RandomForest', 'LogisticRegression'])

    if classifier_name == 'SVC':
        # Hiperpar√°metros espec√≠ficos de SVC
        C = trial.suggest_float('svc_C', 1e-3, 1e3, log=True)
        kernel = trial.suggest_categorical('svc_kernel', ['linear', 'rbf', 'poly'])

        if kernel == 'rbf' or kernel == 'poly':
            gamma = trial.suggest_float('svc_gamma', 1e-4, 1e-1, log=True)
            classifier = SVC(C=C, kernel=kernel, gamma=gamma, random_state=42)
        else:
            classifier = SVC(C=C, kernel=kernel, random_state=42)

    elif classifier_name == 'RandomForest':
        # Hiperpar√°metros de Random Forest
        n_estimators = trial.suggest_int('rf_n_estimators', 50, 500)
        max_depth = trial.suggest_int('rf_max_depth', 2, 32)
        classifier = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42
        )

    else:  # LogisticRegression
        C = trial.suggest_float('lr_C', 1e-3, 1e3, log=True)
        classifier = LogisticRegression(C=C, random_state=42, max_iter=1000)

    # Evaluar
    score = cross_val_score(classifier, X, y, cv=5, scoring='accuracy').mean()
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=200)

print("Mejor modelo:", study.best_params['classifier'])
print("Mejores hiperpar√°metros:", study.best_params)
```

---

### Pruning: Detener Trials Pobres

**Concepto**: Si un trial est√° dando malos resultados en las primeras √©pocas/folds, **detenerlo temprano** para ahorrar tiempo.

```mermaid
graph TD
    A[Trial 1] --> B[√âpoca 1: Score bajo]
    B --> C{Pruner:<br/>¬øContinuar?}
    C -->|No promisorio| D[‚ùå DETENER<br/>Ahorrar tiempo]
    C -->|Promisorio| E[Continuar<br/>entrenamiento]

    F[Trial 2] --> G[√âpoca 1: Score alto]
    G --> H[Continuar hasta<br/>el final]

    style D fill:#ffcccc
    style H fill:#ccffcc
```

#### C√≥digo con Pruning

```python
import optuna
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_validate

def objective(trial):
    # Sugerir hiperpar√°metros
    n_estimators = trial.suggest_int('n_estimators', 50, 500)
    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)
    max_depth = trial.suggest_int('max_depth', 3, 10)

    model = GradientBoostingClassifier(
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_depth=max_depth,
        random_state=42
    )

    # Cross-validation con reporte de scores intermedios
    cv_results = cross_validate(model, X, y, cv=5, scoring='accuracy', return_train_score=False)

    # Reportar score de cada fold para pruning
    for i, score in enumerate(cv_results['test_score']):
        # Report intermediate value
        trial.report(score, i)

        # Check if trial should be pruned
        if trial.should_prune():
            raise optuna.TrialPruned()

    return cv_results['test_score'].mean()

# Study con pruner
study = optuna.create_study(
    direction='maximize',
    pruner=optuna.pruners.MedianPruner(  # Detiene si score < mediana
        n_startup_trials=10,  # No prunar los primeros 10 trials
        n_warmup_steps=2      # No prunar en los primeros 2 folds
    )
)

study.optimize(objective, n_trials=100)

print(f"Trials completados: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
print(f"Trials podados: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
```

#### Tipos de Pruners

| Pruner | Descripci√≥n | Cu√°ndo Usar |
|--------|-------------|-------------|
| **MedianPruner** | Poda si score < mediana de otros trials | General purpose |
| **PercentilePruner** | Poda si score < percentil X | Control fino |
| **HyperbandPruner** | Successive Halving avanzado | Muchos hiperpar√°metros |
| **ThresholdPruner** | Poda si score < umbral fijo | Requisito m√≠nimo conocido |

---

### Visualizaci√≥n de Resultados

Optuna incluye herramientas de visualizaci√≥n muy √∫tiles:

```python
import optuna.visualization as vis

# 1. Historia de optimizaci√≥n
fig = vis.plot_optimization_history(study)
fig.show()

# 2. Importancia de hiperpar√°metros
fig = vis.plot_param_importances(study)
fig.show()

# 3. Slice plot (efecto individual de cada hiperpar√°metro)
fig = vis.plot_slice(study)
fig.show()

# 4. Contour plot (interacci√≥n entre 2 hiperpar√°metros)
fig = vis.plot_contour(study, params=['n_estimators', 'max_depth'])
fig.show()

# 5. Parallel coordinate plot
fig = vis.plot_parallel_coordinate(study)
fig.show()
```

---

### Integraci√≥n con scikit-learn

Optuna tiene una integraci√≥n directa con scikit-learn:

```python
import optuna
from optuna.integration import OptunaSearchCV
from sklearn.ensemble import RandomForestClassifier

# Similar a GridSearchCV pero con Optuna
param_distributions = {
    'n_estimators': optuna.distributions.IntDistribution(50, 500),
    'max_depth': optuna.distributions.IntDistribution(2, 32),
    'min_samples_split': optuna.distributions.IntDistribution(2, 20)
}

# OptunaSearchCV (API similar a GridSearchCV)
optuna_search = OptunaSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_trials=100,
    cv=5,
    scoring='accuracy',
    random_state=42
)

optuna_search.fit(X_train, y_train)

print("Mejores hiperpar√°metros:", optuna_search.best_params_)
print("Mejor score:", optuna_search.best_score_)

# Usar como un modelo normal
predictions = optuna_search.predict(X_test)
```

---

## Comparaci√≥n de M√©todos

### Tabla Comparativa Completa

| M√©todo | Estrategia | Velocidad | Eficiencia | Flexibilidad | Complejidad | Mejor Para |
|--------|-----------|-----------|-----------|--------------|-------------|------------|
| **GridSearchCV** | Exhaustiva | ‚è±Ô∏è‚è±Ô∏è‚è±Ô∏è | ‚≠ê‚≠ê | üîß | üü¢ Simple | Espacios peque√±os, reproducibilidad |
| **RandomizedSearchCV** | Aleatoria | ‚è±Ô∏è‚è±Ô∏è | ‚≠ê‚≠ê‚≠ê | üîßüîß | üü¢ Simple | Espacios grandes, exploraci√≥n |
| **HalvingSearchCV** | Eliminaci√≥n | ‚è±Ô∏è‚è±Ô∏è | ‚≠ê‚≠ê‚≠ê‚≠ê | üîß | üü° Media | Balance velocidad/cobertura |
| **Optuna** | Bayesiana | ‚è±Ô∏è | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | üîßüîßüîß | üü° Media | Espacios complejos, m√°xima eficiencia |

### Diagrama de Decisi√≥n

```mermaid
graph TD
    A{¬øCu√°ntos<br/>hiperpar√°metros?} -->|1-2| B[GridSearchCV]
    A -->|3-4| C{¬øTiempo<br/>limitado?}
    A -->|5+| D[Optuna o<br/>RandomizedSearchCV]

    C -->|No| E[HalvingGridSearchCV]
    C -->|S√≠| F[RandomizedSearchCV]

    G{¬øNecesitas<br/>m√°xima<br/>eficiencia?} -->|S√≠| H[Optuna]
    G -->|No| I[RandomizedSearchCV]

    J{¬øConoces bien<br/>el espacio?} -->|S√≠| B
    J -->|No| K[Optuna o<br/>RandomizedSearchCV]

    style B fill:#ffffcc
    style D fill:#ccffcc
    style E fill:#cce5ff
    style F fill:#ffcccc
    style H fill:#aaffaa
```

### Ejemplo de Performance

```
Dataset: 50,000 muestras, 100 features
Tarea: Optimizar Random Forest con 5 hiperpar√°metros
Budget: Encontrar la mejor configuraci√≥n

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ M√©todo                 ‚îÇ Tiempo     ‚îÇ Trials       ‚îÇ Best Score ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ GridSearchCV           ‚îÇ 24 horas   ‚îÇ 1,024 (100%) ‚îÇ   0.8765   ‚îÇ
‚îÇ RandomizedSearchCV     ‚îÇ 2 horas    ‚îÇ   100 (10%)  ‚îÇ   0.8720   ‚îÇ
‚îÇ HalvingGridSearchCV    ‚îÇ 8 horas    ‚îÇ   300 (30%)  ‚îÇ   0.8750   ‚îÇ
‚îÇ Optuna (TPE)           ‚îÇ 1.5 horas  ‚îÇ   150 (15%)  ‚îÇ   0.8780   ‚îÇ ‚úÖ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Conclusi√≥n: Optuna encontr√≥ MEJOR score en MENOS tiempo
```

---

## Mejores Pr√°cticas

### 1. Siempre Usar Validaci√≥n Cruzada

```python
# ‚ùå MAL: Sin validaci√≥n
model.fit(X_train, y_train)
score = model.score(X_test, y_test)  # Solo un n√∫mero, puede ser suerte

# ‚úÖ BIEN: Con validaci√≥n cruzada
scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"Score: {scores.mean():.3f} (+/- {scores.std():.3f})")
```

### 2. Separar Test Set ANTES de Optimizar

```python
# ‚ùå MAL: Test set usado durante optimizaci√≥n
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X, y)  # Usa TODOS los datos
score = grid_search.score(X, y)  # Data leakage!

# ‚úÖ BIEN: Test set separado
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_train, y_train)  # Solo usa train

# Evaluar en test set NO VISTO
test_score = grid_search.score(X_test, y_test)
```

### 3. Definir Rangos Razonables

```python
# ‚ùå MAL: Rango muy amplio e ineficiente
param_grid = {
    'n_estimators': list(range(1, 1000)),  # 1000 valores!
    'learning_rate': [10**i for i in range(-10, 10)]  # Valores absurdos
}

# ‚úÖ BIEN: Rango enfocado basado en experiencia
param_distributions = {
    'n_estimators': randint(50, 500),
    'learning_rate': loguniform(1e-4, 1e-1)  # Escala log para LR
}
```

### 4. Usar M√©tricas Apropiadas

```python
# Para clasificaci√≥n desbalanceada
grid_search = GridSearchCV(
    model, param_grid,
    scoring='f1',  # No usar accuracy si clases desbalanceadas
    cv=StratifiedKFold(5)
)

# Para regresi√≥n
grid_search = GridSearchCV(
    model, param_grid,
    scoring='neg_mean_squared_error',
    cv=5
)

# M√∫ltiples m√©tricas
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1'
}
grid_search = GridSearchCV(model, param_grid, scoring=scoring, refit='f1')
```

### 5. Guardar Resultados

```python
import joblib

# Guardar mejor modelo
joblib.dump(grid_search.best_estimator_, 'best_model.pkl')

# Guardar todo el estudio de Optuna
import pickle
with open('study.pkl', 'wb') as f:
    pickle.dump(study, f)

# Cargar
study = pickle.load(open('study.pkl', 'rb'))
```

### 6. Pipeline Completo

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Pipeline con preprocesamiento
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA()),
    ('classifier', RandomForestClassifier())
])

# Optimizar hiperpar√°metros del pipeline completo
param_grid = {
    'pca__n_components': [10, 20, 30],
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [5, 10, None]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)
```

---

## Flujo de Trabajo Recomendado

```mermaid
graph TD
    A[1. Split datos<br/>Train/Test] --> B[2. Exploraci√≥n inicial<br/>con defaults]
    B --> C{¬øBuen<br/>rendimiento?}
    C -->|S√≠| D[3. Optimizaci√≥n<br/>con RandomSearch]
    C -->|No| E[Revisar features<br/>y preprocesamiento]
    E --> B

    D --> F[4. Refinamiento<br/>con Optuna]
    F --> G[5. Validar en<br/>test set]
    G --> H{¬øSatisfactorio?}
    H -->|S√≠| I[‚úÖ Modelo final]
    H -->|No| J[Analizar errores]
    J --> E

    style I fill:#ccffcc
```

**Pasos**:

1. **Split inicial**: 80/20 o 70/30
2. **Baseline**: Entrenar con hiperpar√°metros default
3. **Primera optimizaci√≥n**: RandomizedSearchCV (n_iter=50-100)
4. **Refinamiento**: Optuna (n_trials=100-200) en rango reducido
5. **Validaci√≥n final**: Test set NO VISTO
6. **An√°lisis**: Importancia de features, errores, curvas de aprendizaje

---

## Referencias

### Documentaci√≥n Oficial

1. **scikit-learn Model Selection**: https://scikit-learn.org/stable/model_selection.html
2. **scikit-learn GridSearchCV**: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
3. **scikit-learn RandomizedSearchCV**: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html
4. **scikit-learn Cross-Validation**: https://scikit-learn.org/stable/modules/cross_validation.html

### Optuna

5. **Optuna Documentation**: https://optuna.readthedocs.io/
6. **Optuna GitHub**: https://github.com/optuna/optuna
7. **Optuna Examples**: https://github.com/optuna/optuna-examples

### Art√≠culos y Tutoriales

8. **Hyperparameter Tuning the Random Forest** - Towards Data Science
9. **A Conceptual Explanation of Bayesian Hyperparameter Optimization** - Machine Learning Mastery
10. **Optuna: A Next-generation Hyperparameter Optimization Framework** - Paper KDD 2019

### Otros Frameworks

11. **Hyperopt**: http://hyperopt.github.io/hyperopt/
12. **Ray Tune**: https://docs.ray.io/en/latest/tune/index.html
13. **Keras Tuner**: https://keras.io/keras_tuner/

---

**Licencia**: MIT License
**Autor**: David Palacio Jim√©nez
**Fecha**: 2025
**Versi√≥n**: 1.0
