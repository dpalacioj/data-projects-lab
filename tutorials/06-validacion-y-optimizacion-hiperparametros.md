# Tutorial: ValidaciÃ³n Cruzada y OptimizaciÃ³n de HiperparÃ¡metros

## Ãndice

1. [IntroducciÃ³n](#introducciÃ³n)
2. [ParÃ¡metros vs HiperparÃ¡metros](#parÃ¡metros-vs-hiperparÃ¡metros)
3. [ValidaciÃ³n Cruzada (Cross-Validation)](#validaciÃ³n-cruzada-cross-validation)
4. [OptimizaciÃ³n de HiperparÃ¡metros con scikit-learn](#optimizaciÃ³n-de-hiperparÃ¡metros-con-scikit-learn)
5. [OptimizaciÃ³n Avanzada con Optuna](#optimizaciÃ³n-avanzada-con-optuna)
6. [ComparaciÃ³n de MÃ©todos](#comparaciÃ³n-de-mÃ©todos)
7. [Mejores PrÃ¡cticas](#mejores-prÃ¡cticas)
8. [Referencias](#referencias)

---

## IntroducciÃ³n

El **ajuste de hiperparÃ¡metros** (hyperparameter tuning o fine-tuning) es el proceso de encontrar la configuraciÃ³n Ã³ptima de un modelo de machine learning. Este tutorial explica:

- âœ… QuÃ© son los hiperparÃ¡metros y cÃ³mo difieren de los parÃ¡metros
- âœ… CÃ³mo validar modelos correctamente
- âœ… Diferentes estrategias para buscar los mejores hiperparÃ¡metros
- âœ… Herramientas modernas para optimizaciÃ³n eficiente

### El Problema

```mermaid
graph TD
    A[Entrenar modelo con<br/>hiperparÃ¡metros aleatorios] --> B{Â¿Buen<br/>rendimiento?}
    B -->|No| C[âŒ Modelo pobre]
    B -->|Por suerte| D[âš ï¸ Puede ser overfitting]

    E[Optimizar hiperparÃ¡metros<br/>sistemÃ¡ticamente] --> F[ValidaciÃ³n cruzada]
    F --> G[Buscar mejor<br/>configuraciÃ³n]
    G --> H[âœ… Modelo robusto<br/>y generalizable]

    style C fill:#ffcccc
    style D fill:#ffffcc
    style H fill:#ccffcc
```

---

## ParÃ¡metros vs HiperparÃ¡metros

### Definiciones

```mermaid
mindmap
  root((ConfiguraciÃ³n<br/>del Modelo))
    ParÃ¡metros
      Se aprenden del entrenamiento
      Ejemplos: pesos, coeficientes
      El modelo los ajusta automÃ¡ticamente
    HiperparÃ¡metros
      Se definen ANTES del entrenamiento
      Ejemplos: learning rate, max_depth
      El usuario debe configurarlos
```

---

### ComparaciÃ³n Detallada

| Aspecto | ParÃ¡metros | HiperparÃ¡metros |
|---------|-----------|-----------------|
| **DefiniciÃ³n** | Variables **internas** del modelo que se aprenden durante el entrenamiento | Variables **externas** que controlan el proceso de aprendizaje |
| **CÃ³mo se obtienen** | El algoritmo los **ajusta automÃ¡ticamente** usando los datos | El usuario debe **especificarlos manualmente** |
| **Â¿CuÃ¡ndo se definen?** | **Durante** el entrenamiento | **Antes** del entrenamiento |
| **Ejemplos** | Pesos de red neuronal, coeficientes de regresiÃ³n, vectores de soporte en SVM | Learning rate, nÃºmero de Ã¡rboles, profundidad mÃ¡xima, C en SVM |
| **Â¿Se guardan en el modelo?** | âœ… SÃ­ | âš ï¸ Se guardan pero como configuraciÃ³n |
| **Â¿Se pueden modificar despuÃ©s?** | âŒ No sin reentrenar | âœ… SÃ­, pero requiere reentrenar |

---

### Ejemplos por Algoritmo

#### 1. RegresiÃ³n Lineal

```python
from sklearn.linear_model import Ridge

# Crear modelo
model = Ridge(alpha=1.0)  # alpha es un HIPERPARÃMETRO
model.fit(X_train, y_train)

# Los coeficientes son PARÃMETROS (aprendidos)
print("ParÃ¡metros (coeficientes):", model.coef_)
print("ParÃ¡metro (intercepto):", model.intercept_)
```

**Tabla de ParÃ¡metros vs HiperparÃ¡metros**:

| Tipo | Nombre | DescripciÃ³n |
|------|--------|-------------|
| **HiperparÃ¡metro** | `alpha` | Fuerza de regularizaciÃ³n (lo defines tÃº) |
| **ParÃ¡metro** | `coef_` | Pesos de cada caracterÃ­stica (aprendidos) |
| **ParÃ¡metro** | `intercept_` | TÃ©rmino independiente (aprendido) |

---

#### 2. Random Forest

```python
from sklearn.ensemble import RandomForestClassifier

# Crear modelo con HIPERPARÃMETROS
model = RandomForestClassifier(
    n_estimators=100,        # HiperparÃ¡metro
    max_depth=10,           # HiperparÃ¡metro
    min_samples_split=5,    # HiperparÃ¡metro
    random_state=42
)

model.fit(X_train, y_train)

# Los Ã¡rboles internos son PARÃMETROS (aprendidos)
# Cada nodo de cada Ã¡rbol tiene condiciones aprendidas
```

**HiperparÃ¡metros comunes**:

| HiperparÃ¡metro | QuÃ© Controla | Valores TÃ­picos |
|----------------|--------------|-----------------|
| `n_estimators` | NÃºmero de Ã¡rboles | 50, 100, 200, 500 |
| `max_depth` | Profundidad mÃ¡xima de Ã¡rboles | 5, 10, 20, None |
| `min_samples_split` | MÃ­nimo de muestras para dividir | 2, 5, 10 |
| `min_samples_leaf` | MÃ­nimo de muestras en hoja | 1, 2, 4 |
| `max_features` | Features a considerar en cada split | 'sqrt', 'log2', None |

---

#### 3. Red Neuronal

```python
from sklearn.neural_network import MLPClassifier

# HIPERPARÃMETROS de arquitectura y entrenamiento
model = MLPClassifier(
    hidden_layer_sizes=(100, 50),  # HiperparÃ¡metro: arquitectura
    activation='relu',             # HiperparÃ¡metro: funciÃ³n de activaciÃ³n
    learning_rate_init=0.001,      # HiperparÃ¡metro: tasa de aprendizaje
    max_iter=200,                  # HiperparÃ¡metro: Ã©pocas
    random_state=42
)

model.fit(X_train, y_train)

# PARÃMETROS (pesos y sesgos de cada capa)
print("Capas:", len(model.coefs_))  # NÃºmero de capas
print("Pesos capa 1:", model.coefs_[0].shape)
print("Sesgos capa 1:", model.intercepts_[0].shape)
```

---

### VisualizaciÃ³n del Proceso

```mermaid
graph LR
    A[Usuario define<br/>HIPERPARÃMETROS] --> B[Crear modelo]
    B --> C[Entrenar con datos]
    C --> D[Modelo aprende<br/>PARÃMETROS]
    D --> E[Modelo entrenado]

    style A fill:#ffffcc
    style D fill:#ccffcc
```

**Ejemplo Conceptual**:

```
ğŸ¯ HIPERPARÃMETROS (tÃº decides):
   - Profundidad del Ã¡rbol: 10
   - NÃºmero de Ã¡rboles: 100
   - MÃ­nimo de muestras: 5

       â¬‡ ENTRENAR â¬‡

ğŸ“Š PARÃMETROS (el modelo aprende):
   - Ãrbol 1: Si edad > 35 â†’ Si salario > 50k â†’ Clase A
   - Ãrbol 2: Si ciudad == "NYC" â†’ Si edad < 25 â†’ Clase B
   - ... (98 Ã¡rboles mÃ¡s con sus reglas)
```

---

### Â¿Por quÃ© importa esta distinciÃ³n?

```mermaid
graph TD
    A{Â¿Quiero mejorar<br/>el modelo?} --> B[Ajustar<br/>HIPERPARÃMETROS]
    B --> C[Requiere buscar<br/>mejor configuraciÃ³n]
    C --> D[GridSearch,<br/>RandomSearch, Optuna]

    A --> E[Entrenar con<br/>mÃ¡s datos]
    E --> F[Aprende mejores<br/>PARÃMETROS]

    style B fill:#ffffcc
    style E fill:#ccffcc
```

| Objetivo | AcciÃ³n |
|----------|--------|
| **Reducir overfitting** | Ajustar hiperparÃ¡metros de regularizaciÃ³n (`alpha`, `max_depth`) |
| **Mejorar precisiÃ³n** | Buscar mejores hiperparÃ¡metros con validaciÃ³n cruzada |
| **Acelerar entrenamiento** | Ajustar `learning_rate`, `batch_size` |
| **Modelo mÃ¡s simple** | Reducir `n_estimators`, `hidden_layers` |

---

## ValidaciÃ³n Cruzada (Cross-Validation)

### Â¿Por quÃ© NO basta con Train/Test Split?

```python
# âŒ PROBLEMA: Split simple
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

model.fit(X_train, y_train)
score = model.score(X_test, y_test)
```

**Problemas**:

1. âš ï¸ El resultado depende de la **suerte** del split
2. âš ï¸ 20% de datos "desperdiciados" (solo para test)
3. âš ï¸ No sabes si el modelo **generaliza bien** o tuvo suerte

```mermaid
graph LR
    A[Dataset completo] --> B[Split 1]
    A --> C[Split 2]
    A --> D[Split 3]

    B --> B1[Train: 80%<br/>Test: 20%<br/>Score: 0.85]
    C --> C1[Train: 80%<br/>Test: 20%<br/>Score: 0.78]
    D --> D1[Train: 80%<br/>Test: 20%<br/>Score: 0.92]

    B1 --> E{Â¿CuÃ¡l es<br/>el verdadero<br/>rendimiento?}
    C1 --> E
    D1 --> E

    style E fill:#ffcccc
```

---

### SoluciÃ³n: ValidaciÃ³n Cruzada

**Concepto**: Dividir los datos en **mÃºltiples particiones** (folds) y usar cada particiÃ³n como test una vez.

```mermaid
graph TD
    A[Dataset completo] --> B[Dividir en K folds]
    B --> C[Fold 1]
    B --> D[Fold 2]
    B --> E[Fold 3]
    B --> F[Fold K]

    C --> C1[Train: F2+F3+...Fk<br/>Test: F1<br/>Score 1]
    D --> D1[Train: F1+F3+...Fk<br/>Test: F2<br/>Score 2]
    E --> E1[Train: F1+F2+F4+...Fk<br/>Test: F3<br/>Score 3]
    F --> F1[Train: F1+F2+...Fk-1<br/>Test: Fk<br/>Score K]

    C1 --> G[Promedio de<br/>todos los scores]
    D1 --> G
    E1 --> G
    F1 --> G

    style G fill:#ccffcc
```

**Ventajas**:

âœ… Usa **todos los datos** para entrenamiento y validaciÃ³n
âœ… Resultados mÃ¡s **confiables** (promedio de K experimentos)
âœ… Detecta **overfitting** si hay gran varianza entre folds

---

### Tipos de ValidaciÃ³n Cruzada

```mermaid
mindmap
  root((ValidaciÃ³n<br/>Cruzada))
    K-Fold CV
      Datos balanceados
      ClasificaciÃ³n/RegresiÃ³n
    Stratified K-Fold
      ClasificaciÃ³n
      Mantiene proporciÃ³n de clases
    Leave-One-Out LOO
      Datasets pequeÃ±os
      Muy costoso
    Time Series Split
      Series temporales
      Respeta orden temporal
    Group K-Fold
      Datos agrupados
      Evita data leakage
```

---

### 1. K-Fold Cross-Validation

**Concepto**: Divide datos en K particiones iguales. Cada particiÃ³n se usa como test una vez.

#### VisualizaciÃ³n

```
Dataset: [â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– â– ] (100 muestras)

K=5 folds:

IteraciÃ³n 1: [â–ˆâ–ˆâ–ˆâ–ˆ|Â·Â·Â·Â·|Â·Â·Â·Â·|Â·Â·Â·Â·|Â·Â·Â·Â·]  Test: 20 muestras
             Train ---------> (80 muestras)

IteraciÃ³n 2: [Â·Â·Â·Â·|â–ˆâ–ˆâ–ˆâ–ˆ|Â·Â·Â·Â·|Â·Â·Â·Â·|Â·Â·Â·Â·]  Test: 20 muestras
              ----  Train ---------> (80 muestras)

IteraciÃ³n 3: [Â·Â·Â·Â·|Â·Â·Â·Â·|â–ˆâ–ˆâ–ˆâ–ˆ|Â·Â·Â·Â·|Â·Â·Â·Â·]  Test: 20 muestras
              --------  Train --> (80 muestras)

IteraciÃ³n 4: [Â·Â·Â·Â·|Â·Â·Â·Â·|Â·Â·Â·Â·|â–ˆâ–ˆâ–ˆâ–ˆ|Â·Â·Â·Â·]  Test: 20 muestras
              ----------  Train --> (80 muestras)

IteraciÃ³n 5: [Â·Â·Â·Â·|Â·Â·Â·Â·|Â·Â·Â·Â·|Â·Â·Â·Â·|â–ˆâ–ˆâ–ˆâ–ˆ]  Test: 20 muestras
              ----------  Train --> (80 muestras)

Resultado: Promedio de 5 scores
```

#### CÃ³digo con scikit-learn

```python
from sklearn.model_selection import cross_val_score, KFold
from sklearn.ensemble import RandomForestClassifier

# Crear modelo
model = RandomForestClassifier(n_estimators=100)

# K-Fold con 5 particiones
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

# Realizar validaciÃ³n cruzada
scores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')

print(f"Scores por fold: {scores}")
print(f"Accuracy promedio: {scores.mean():.3f} (+/- {scores.std():.3f})")

# Ejemplo de salida:
# Scores por fold: [0.85, 0.82, 0.88, 0.84, 0.86]
# Accuracy promedio: 0.850 (+/- 0.021)
```

#### ParÃ¡metros de KFold

| ParÃ¡metro | DescripciÃ³n | Valores Comunes |
|-----------|-------------|-----------------|
| `n_splits` | NÃºmero de folds (K) | 5, 10 |
| `shuffle` | Mezclar datos antes de dividir | `True` (recomendado) |
| `random_state` | Semilla para reproducibilidad | 42, 0 |

---

### 2. Stratified K-Fold

**Problema con K-Fold regular**: En clasificaciÃ³n desbalanceada, algunos folds pueden no tener muestras de ciertas clases.

```
Dataset desbalanceado: 90% Clase A, 10% Clase B

âŒ K-Fold regular puede crear:
   Fold 1: 100% Clase A (sin Clase B)
   Fold 2: 95% Clase A, 5% Clase B
   ...

âœ… Stratified K-Fold mantiene proporciones:
   Fold 1: 90% Clase A, 10% Clase B
   Fold 2: 90% Clase A, 10% Clase B
   ...
```

#### VisualizaciÃ³n

```
Dataset Original:
Clase A: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ (90%)
Clase B: â–ˆâ–ˆâ–ˆâ–ˆ (10%)

Stratified K-Fold (K=5):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Fold 1: 90% A + 10% B                â”‚
â”‚ Fold 2: 90% A + 10% B                â”‚
â”‚ Fold 3: 90% A + 10% B                â”‚
â”‚ Fold 4: 90% A + 10% B                â”‚
â”‚ Fold 5: 90% A + 10% B                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### CÃ³digo con scikit-learn

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score

# Crear modelo
model = RandomForestClassifier(n_estimators=100)

# Stratified K-Fold
stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# ValidaciÃ³n cruzada estratificada
scores = cross_val_score(model, X, y, cv=stratified_kfold, scoring='accuracy')

print(f"Scores por fold: {scores}")
print(f"Accuracy promedio: {scores.mean():.3f}")
```

#### CuÃ¡ndo Usar

| Usar Stratified K-Fold | Usar K-Fold Regular |
|------------------------|---------------------|
| âœ… ClasificaciÃ³n | âœ… RegresiÃ³n |
| âœ… Clases desbalanceadas | âœ… Clases balanceadas |
| âœ… Pocas muestras por clase | - |

---

### 3. Leave-One-Out (LOO)

**Concepto**: Caso extremo de K-Fold donde **K = nÃºmero de muestras**. Cada muestra es un fold.

```
Dataset: 10 muestras [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]

IteraciÃ³n 1:  Test: [1]    Train: [2,3,4,5,6,7,8,9,10]
IteraciÃ³n 2:  Test: [2]    Train: [1,3,4,5,6,7,8,9,10]
IteraciÃ³n 3:  Test: [3]    Train: [1,2,4,5,6,7,8,9,10]
...
IteraciÃ³n 10: Test: [10]   Train: [1,2,3,4,5,6,7,8,9]

Total: 10 modelos entrenados
```

#### CÃ³digo con scikit-learn

```python
from sklearn.model_selection import LeaveOneOut, cross_val_score

# Crear modelo
model = RandomForestClassifier(n_estimators=50)

# Leave-One-Out
loo = LeaveOneOut()

# ValidaciÃ³n cruzada (Â¡LENTO para datasets grandes!)
scores = cross_val_score(model, X, y, cv=loo)

print(f"NÃºmero de iteraciones: {len(scores)}")  # = nÃºmero de muestras
print(f"Accuracy promedio: {scores.mean():.3f}")
```

#### Ventajas y Desventajas

| âœ… Ventajas | âŒ Desventajas |
|------------|----------------|
| Usa TODOS los datos | **MUY lento** (n entrenamientos) |
| Sin sesgo por split | Alta varianza en resultados |
| MÃ¡xima utilizaciÃ³n de datos | No prÃ¡ctico para datasets grandes |

**CuÃ¡ndo usar**: Solo con **datasets muy pequeÃ±os** (< 100 muestras).

---

### 4. Time Series Split

**Problema**: Con series temporales, **NO puedes mezclar los datos**. El futuro no puede predecir el pasado.

```
âŒ K-Fold regular (mezcla datos):
[Ene, Feb, Mar, Abr, May, Jun, Jul, Ago, Sep, Oct, Nov, Dic]
Fold 1: Train [Ene, Mar, May, Jul, Sep, Nov]  Test [Feb, Abr, Jun, Ago, Oct, Dic]
                     â†‘ Usa datos del futuro para predecir el pasado âŒ

âœ… Time Series Split (respeta orden):
[Ene, Feb, Mar, Abr, May, Jun, Jul, Ago, Sep, Oct, Nov, Dic]
Fold 1: Train [Ene, Feb, Mar]          Test [Abr]
Fold 2: Train [Ene, Feb, Mar, Abr]     Test [May]
Fold 3: Train [Ene, ..., May]          Test [Jun]
...
```

#### VisualizaciÃ³n

```mermaid
graph TD
    A[Serie Temporal] --> B[Split 1]
    A --> C[Split 2]
    A --> D[Split 3]
    A --> E[Split N]

    B --> B1["Train: â–ˆâ–ˆâ–ˆâ–ˆ<br/>Test: â–ˆ<br/>(solo futuro)"]
    C --> C1["Train: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ<br/>Test: â–ˆ<br/>(solo futuro)"]
    D --> D1["Train: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ<br/>Test: â–ˆ<br/>(solo futuro)"]
    E --> E1["Train: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ<br/>Test: â–ˆ<br/>(solo futuro)"]

    style B1 fill:#ccffcc
    style C1 fill:#ccffcc
    style D1 fill:#ccffcc
    style E1 fill:#ccffcc
```

#### CÃ³digo con scikit-learn

```python
from sklearn.model_selection import TimeSeriesSplit, cross_val_score

# Crear modelo
model = RandomForestRegressor()

# Time Series Split con 5 splits
tscv = TimeSeriesSplit(n_splits=5)

# ValidaciÃ³n cruzada temporal
scores = cross_val_score(model, X, y, cv=tscv, scoring='neg_mean_squared_error')

print(f"MSE por split: {-scores}")  # Negativo porque scoring es 'neg_mse'
print(f"MSE promedio: {-scores.mean():.3f}")

# Visualizar los splits
for i, (train_index, test_index) in enumerate(tscv.split(X)):
    print(f"Split {i+1}:")
    print(f"  Train: Ã­ndices {train_index[0]} a {train_index[-1]}")
    print(f"  Test:  Ã­ndices {test_index[0]} a {test_index[-1]}")
```

---

### 5. Group K-Fold

**Problema**: Cuando tienes **grupos** en tus datos (ej: mÃºltiples mediciones del mismo paciente), debes asegurar que muestras del mismo grupo NO estÃ©n en train y test simultÃ¡neamente.

```
Dataset: Pacientes con mÃºltiples visitas

âŒ K-Fold regular:
   Paciente A: Visita 1 (Train), Visita 2 (Test)  â† Data leakage!

âœ… Group K-Fold:
   Paciente A: Todas sus visitas en Train
   Paciente B: Todas sus visitas en Test
```

#### CÃ³digo con scikit-learn

```python
from sklearn.model_selection import GroupKFold

# IDs de grupo (ej: ID de paciente)
groups = [1, 1, 1, 2, 2, 3, 3, 3, 4, 4]
#         â””â”€Aâ”€â”˜  â””Bâ”˜  â””â”€Câ”€â”˜  â””Dâ”˜

# Group K-Fold
gkf = GroupKFold(n_splits=3)

for i, (train_idx, test_idx) in enumerate(gkf.split(X, y, groups=groups)):
    print(f"Fold {i+1}:")
    print(f"  Train groups: {set([groups[i] for i in train_idx])}")
    print(f"  Test groups:  {set([groups[i] for i in test_idx])}")

# Salida:
# Fold 1:
#   Train groups: {2, 3, 4}
#   Test groups:  {1}
```

---

### ComparaciÃ³n de MÃ©todos de ValidaciÃ³n

```mermaid
graph TD
    A{Â¿QuÃ© tipo<br/>de datos?} -->|ClasificaciÃ³n| B{Â¿Balanceado?}
    A -->|RegresiÃ³n| C[K-Fold]
    A -->|Series Temporales| D[TimeSeriesSplit]
    A -->|Con grupos| E[GroupKFold]

    B -->|SÃ­| C
    B -->|No| F[StratifiedKFold]

    G{Â¿CuÃ¡ntos datos?} -->|Muy pocos<br/><100| H[LeaveOneOut]
    G -->|Normal| C

    style C fill:#ccffcc
    style F fill:#ffffcc
    style D fill:#cce5ff
    style E fill:#ffcccc
    style H fill:#ffeecc
```

### Tabla Comparativa Completa

| MÃ©todo | Tipo de Datos | Ventajas | Desventajas | CuÃ¡ndo Usar |
|--------|---------------|----------|-------------|-------------|
| **K-Fold** | RegresiÃ³n, ClasificaciÃ³n balanceada | Simple, eficiente | Puede desbalancear clases | General purpose |
| **Stratified K-Fold** | ClasificaciÃ³n | Mantiene proporciÃ³n de clases | Solo para clasificaciÃ³n | Clases desbalanceadas |
| **Leave-One-Out** | Cualquiera | MÃ¡ximo uso de datos | Muy lento, alta varianza | Datasets muy pequeÃ±os |
| **Time Series Split** | Series temporales | Respeta temporalidad | Requiere orden | Forecasting, finanzas |
| **Group K-Fold** | Datos agrupados | Evita data leakage | Requiere definir grupos | MÃºltiples mediciones |

---

## OptimizaciÃ³n de HiperparÃ¡metros con scikit-learn

### Flujo General

```mermaid
graph LR
    A[Definir espacio<br/>de bÃºsqueda] --> B[Elegir mÃ©todo<br/>de bÃºsqueda]
    B --> C[GridSearchCV o<br/>RandomizedSearchCV]
    C --> D[Ejecutar bÃºsqueda<br/>con CV]
    D --> E[Obtener mejores<br/>hiperparÃ¡metros]
    E --> F[Entrenar modelo<br/>final]

    style A fill:#ffffcc
    style E fill:#ccffcc
    style F fill:#aaffaa
```

---

### 1. GridSearchCV (BÃºsqueda Exhaustiva)

**Concepto**: Prueba **todas las combinaciones posibles** de hiperparÃ¡metros.

#### Â¿CÃ³mo Funciona?

```
Espacio de bÃºsqueda:
- n_estimators: [50, 100, 200]
- max_depth: [5, 10, 15]
- min_samples_split: [2, 5]

Grid completo:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CombinaciÃ³n 1: n=50,  d=5,  s=2                â”‚
â”‚ CombinaciÃ³n 2: n=50,  d=5,  s=5                â”‚
â”‚ CombinaciÃ³n 3: n=50,  d=10, s=2                â”‚
â”‚ CombinaciÃ³n 4: n=50,  d=10, s=5                â”‚
â”‚ CombinaciÃ³n 5: n=50,  d=15, s=2                â”‚
â”‚ CombinaciÃ³n 6: n=50,  d=15, s=5                â”‚
â”‚ CombinaciÃ³n 7: n=100, d=5,  s=2                â”‚
â”‚ ... (18 combinaciones en total = 3Ã—3Ã—2)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Cada combinaciÃ³n se evalÃºa con K-Fold CV (ej: K=5)
Total de entrenamientos: 18 Ã— 5 = 90
```

#### CÃ³digo con scikit-learn

```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

# 1. Definir modelo base
model = RandomForestClassifier(random_state=42)

# 2. Definir espacio de bÃºsqueda (grid)
param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# 3. Crear GridSearchCV
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    cv=5,                    # 5-fold cross-validation
    scoring='accuracy',      # MÃ©trica a optimizar
    n_jobs=-1,              # Usar todos los CPUs
    verbose=2                # Mostrar progreso
)

# 4. Ejecutar bÃºsqueda
grid_search.fit(X_train, y_train)

# 5. Mejores hiperparÃ¡metros
print("Mejores hiperparÃ¡metros:", grid_search.best_params_)
print("Mejor score:", grid_search.best_score_)

# 6. Usar mejor modelo
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test)

# Ejemplo de salida:
# Mejores hiperparÃ¡metros: {'max_depth': 10, 'min_samples_leaf': 1,
#                            'min_samples_split': 2, 'n_estimators': 200}
# Mejor score: 0.8765
```

#### Analizar Resultados

```python
import pandas as pd

# Ver todos los resultados
results = pd.DataFrame(grid_search.cv_results_)

# Columnas importantes
results[['params', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values('rank_test_score')

# Top 5 configuraciones
print(results.nsmallest(5, 'rank_test_score')[['params', 'mean_test_score']])
```

#### Ventajas y Desventajas

| âœ… Ventajas | âŒ Desventajas |
|------------|----------------|
| Prueba TODAS las combinaciones | **Muy lento** con muchos hiperparÃ¡metros |
| Garantiza encontrar el mejor dentro del grid | Crece exponencialmente |
| Reproducible | No escala bien |
| Simple de entender | Puede no explorar bien el espacio |

**Complejidad**: Si tienes **n** hiperparÃ¡metros con **m** valores cada uno:
- Combinaciones: $m^n$
- Con K-Fold CV: $m^n \times K$ entrenamientos

Ejemplo: 5 hiperparÃ¡metros con 4 valores = $4^5 = 1024$ combinaciones Ã— 5 folds = **5,120 entrenamientos** ğŸ˜±

---

### 2. RandomizedSearchCV (BÃºsqueda Aleatoria)

**Concepto**: En lugar de probar TODAS las combinaciones, prueba un **nÃºmero fijo de combinaciones aleatorias**.

#### ComparaciÃ³n Visual

```
GridSearchCV (exhaustivo):
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚  Prueba TODAS
â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚  las celdas
â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚  (25 combinaciones)
â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚
â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚ âœ“ â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜

RandomizedSearchCV (muestreo):
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚   â”‚ âœ“ â”‚   â”‚   â”‚ âœ“ â”‚  Prueba solo n_iter
â”‚ âœ“ â”‚   â”‚   â”‚ âœ“ â”‚   â”‚  combinaciones aleatorias
â”‚   â”‚   â”‚ âœ“ â”‚   â”‚   â”‚  (10 combinaciones)
â”‚   â”‚ âœ“ â”‚   â”‚   â”‚ âœ“ â”‚
â”‚ âœ“ â”‚   â”‚   â”‚ âœ“ â”‚   â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
MÃ¡s rÃ¡pido pero puede perder el Ã³ptimo global
```

#### CÃ³digo con scikit-learn

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import randint, uniform

# 1. Definir modelo
model = RandomForestClassifier(random_state=42)

# 2. Definir distribuciones de hiperparÃ¡metros
param_distributions = {
    'n_estimators': randint(50, 500),           # Enteros entre 50-500
    'max_depth': randint(5, 50),                # Enteros entre 5-50
    'min_samples_split': randint(2, 20),        # Enteros entre 2-20
    'min_samples_leaf': randint(1, 10),         # Enteros entre 1-10
    'max_features': uniform(0.1, 0.9)           # Floats entre 0.1-1.0
}

# 3. Crear RandomizedSearchCV
random_search = RandomizedSearchCV(
    estimator=model,
    param_distributions=param_distributions,
    n_iter=50,               # NÃºmero de combinaciones a probar
    cv=5,
    scoring='accuracy',
    n_jobs=-1,
    random_state=42,
    verbose=2
)

# 4. Ejecutar bÃºsqueda
random_search.fit(X_train, y_train)

# 5. Mejores hiperparÃ¡metros
print("Mejores hiperparÃ¡metros:", random_search.best_params_)
print("Mejor score:", random_search.best_score_)
```

#### Distribuciones Comunes

| DistribuciÃ³n | Uso | Ejemplo |
|--------------|-----|---------|
| `randint(low, high)` | Enteros discretos | `n_estimators`, `max_depth` |
| `uniform(low, width)` | Floats continuos | `learning_rate`, `alpha` |
| `loguniform(low, high)` | Floats en escala logarÃ­tmica | `C` en SVM, regularizaciÃ³n |
| Lista | CategÃ³ricos | `['gini', 'entropy']` |

```python
from scipy.stats import randint, uniform, loguniform

param_dist = {
    # Entero: 10-100
    'n_estimators': randint(10, 100),

    # Float lineal: 0.0-1.0
    'subsample': uniform(0.5, 0.5),  # uniform(loc, scale) â†’ [0.5, 1.0]

    # Float logarÃ­tmico: 0.0001-0.1
    'learning_rate': loguniform(1e-4, 1e-1),

    # CategÃ³rico
    'criterion': ['gini', 'entropy']
}
```

#### Ventajas y Desventajas

| âœ… Ventajas | âŒ Desventajas |
|------------|----------------|
| **Mucho mÃ¡s rÃ¡pido** que GridSearch | No garantiza encontrar el Ã³ptimo |
| Explora mejor espacios continuos | Puede repetir combinaciones |
| Puedes controlar el tiempo (n_iter) | Requiere entender distribuciones |
| Bueno para muchos hiperparÃ¡metros | Menos reproducible |

**CuÃ¡ndo usar**: Espacio de bÃºsqueda **grande** o hiperparÃ¡metros **continuos**.

---

### 3. HalvingGridSearchCV / HalvingRandomSearchCV

**Concepto**: Variante **eficiente** que elimina candidatos pobres progresivamente usando **Successive Halving**.

#### Â¿CÃ³mo Funciona?

```mermaid
graph TD
    A[IteraciÃ³n 1:<br/>Todos los candidatos<br/>Pocos recursos] --> B[Evaluar todos]
    B --> C[Eliminar peores<br/>50%]
    C --> D[IteraciÃ³n 2:<br/>Mejores 50%<br/>MÃ¡s recursos]
    D --> E[Evaluar supervivientes]
    E --> F[Eliminar peores<br/>50%]
    F --> G[IteraciÃ³n 3:<br/>Mejores 25%<br/>Todos los recursos]
    G --> H[Mejor candidato]

    style A fill:#ffcccc
    style D fill:#ffffcc
    style G fill:#ccffcc
    style H fill:#aaffaa
```

#### Ejemplo Visual

```
64 candidatos iniciales
â”œâ”€ IteraciÃ³n 1: Entrenar con 100 muestras
â”‚  â””â”€ Eliminar 32 peores â†’ Quedan 32
â”‚
â”œâ”€ IteraciÃ³n 2: Entrenar con 400 muestras
â”‚  â””â”€ Eliminar 16 peores â†’ Quedan 16
â”‚
â”œâ”€ IteraciÃ³n 3: Entrenar con 1600 muestras
â”‚  â””â”€ Eliminar 8 peores â†’ Quedan 8
â”‚
â””â”€ IteraciÃ³n 4: Entrenar con 6400 muestras
   â””â”€ Elegir el mejor

âœ… Ahorro: ~75% de entrenamientos vs GridSearch
```

#### CÃ³digo con scikit-learn

```python
from sklearn.experimental import enable_halving_search_cv  # Â¡Necesario!
from sklearn.model_selection import HalvingGridSearchCV

# Definir modelo y param_grid igual que GridSearchCV
model = RandomForestClassifier(random_state=42)

param_grid = {
    'n_estimators': [50, 100, 200, 500],
    'max_depth': [5, 10, 15, 20, None],
    'min_samples_split': [2, 5, 10]
}

# HalvingGridSearchCV
halving_search = HalvingGridSearchCV(
    estimator=model,
    param_grid=param_grid,
    factor=3,                # Factor de reducciÃ³n (elimina 2/3 en cada iter)
    resource='n_samples',    # Recurso a aumentar: muestras o n_estimators
    max_resources='auto',    # MÃ¡ximo de recursos disponibles
    cv=5,
    random_state=42,
    verbose=1
)

halving_search.fit(X_train, y_train)

print("Mejores hiperparÃ¡metros:", halving_search.best_params_)
```

#### ParÃ¡metros Importantes

| ParÃ¡metro | DescripciÃ³n | Valores |
|-----------|-------------|---------|
| `factor` | Factor de eliminaciÃ³n | 2, 3 (elimina 1/2 o 2/3) |
| `resource` | QuÃ© aumentar en cada iter | `'n_samples'`, `'n_estimators'` |
| `max_resources` | MÃ¡ximo de recursos | `'auto'` o nÃºmero |
| `aggressive_elimination` | Forzar factor en Ãºltima iter | `True`, `False` |

---

### ComparaciÃ³n: Grid vs Randomized vs Halving

```mermaid
graph LR
    A[Espacio de<br/>HiperparÃ¡metros] --> B[GridSearchCV<br/>Exhaustivo]
    A --> C[RandomizedSearchCV<br/>Muestreo]
    A --> D[HalvingSearchCV<br/>EliminaciÃ³n Progresiva]

    B --> B1[Lento â±ï¸â±ï¸â±ï¸<br/>Completo âœ…]
    C --> C1[RÃ¡pido â±ï¸<br/>Bueno âš ï¸]
    D --> D1[Medio â±ï¸â±ï¸<br/>Eficiente ğŸš€]

    style B fill:#ffcccc
    style C fill:#ffffcc
    style D fill:#ccffcc
```

### Tabla Comparativa

| MÃ©todo | Velocidad | Cobertura | Mejor Para | Complejidad |
|--------|-----------|-----------|------------|-------------|
| **GridSearchCV** | â±ï¸â±ï¸â±ï¸ Muy lento | âœ… 100% del grid | Espacios pequeÃ±os, pocos hiper | ğŸŸ¢ Simple |
| **RandomizedSearchCV** | â±ï¸â±ï¸ RÃ¡pido | âš ï¸ Muestreo aleatorio | Espacios grandes, continuos | ğŸŸ¡ Media |
| **HalvingGridSearchCV** | â±ï¸â±ï¸ Medio | âœ… 100% del grid (eficiente) | Espacios medianos | ğŸŸ¡ Media |
| **HalvingRandomSearchCV** | â±ï¸ Muy rÃ¡pido | âš ï¸ Muestreo aleatorio (eficiente) | Espacios muy grandes | ğŸŸ¡ Media |

### Ejemplo de Tiempos

```
Dataset: 10,000 muestras
HiperparÃ¡metros: 5 parÃ¡metros con 4 valores cada uno = 1024 combinaciones
K-Fold: 5 folds

GridSearchCV:        1024 Ã— 5 = 5,120 entrenamientos  (~8 horas)
RandomizedSearchCV:   100 Ã— 5 =   500 entrenamientos  (~50 min)
HalvingGridSearchCV:  ~300 Ã— 5 = 1,500 entrenamientos (~2.5 horas)
```

---

## OptimizaciÃ³n Avanzada con Optuna

### Â¿QuÃ© es Optuna?

**Optuna** es un framework de optimizaciÃ³n de hiperparÃ¡metros que usa **algoritmos inteligentes** para encontrar mejores configuraciones **mÃ¡s rÃ¡pido** que Grid/Random Search.

### Grid/Random Search vs Optuna

```mermaid
graph TD
    A[GridSearch/<br/>RandomSearch] --> B[BÃºsqueda ciega]
    B --> C[No aprende de<br/>intentos previos]

    D[Optuna] --> E[BÃºsqueda inteligente]
    E --> F[Aprende de cada trial]
    F --> G[Sugiere mejores<br/>hiperparÃ¡metros]

    style A fill:#ffcccc
    style D fill:#ccffcc
```

### Diferencias Clave

| Aspecto | GridSearch/RandomSearch | Optuna |
|---------|------------------------|--------|
| **Estrategia** | BÃºsqueda ciega (no aprende) | BÃºsqueda bayesiana (aprende) |
| **Eficiencia** | Muchas evaluaciones desperdiciadas | Menos evaluaciones necesarias |
| **Flexibilidad** | Grid estÃ¡tico | Espacio dinÃ¡mico |
| **Algoritmo** | Fuerza bruta / Aleatorio | Tree-structured Parzen Estimator (TPE) |
| **Early Stopping** | âŒ No soportado | âœ… Pruning de trials pobres |
| **Distribuciones** | Listas fijas | Continuas, categÃ³ricas, log-uniform |

---

### Conceptos BÃ¡sicos de Optuna

```mermaid
graph LR
    A[Study] --> B[Trial 1]
    A --> C[Trial 2]
    A --> D[Trial N]

    B --> B1[Suggest<br/>hiperparÃ¡metros]
    B1 --> B2[Entrenar<br/>modelo]
    B2 --> B3[Devolver<br/>score]

    C --> C1[Suggest mejores<br/>hiperparÃ¡metros]
    C1 --> C2[Entrenar]
    C2 --> C3[Devolver<br/>score]

    D --> D1[Encontrar<br/>mejor trial]

    style A fill:#ffffcc
    style D1 fill:#ccffcc
```

**TerminologÃ­a**:

| TÃ©rmino | DefiniciÃ³n |
|---------|------------|
| **Study** | Proceso de optimizaciÃ³n completo |
| **Trial** | Una evaluaciÃ³n individual (un set de hiperparÃ¡metros) |
| **Objective Function** | FunciÃ³n que entrena el modelo y devuelve score |
| **Suggest** | MÃ©todo para proponer valores de hiperparÃ¡metros |
| **Sampler** | Algoritmo que sugiere valores (TPE, Random, CMA-ES) |
| **Pruner** | Detiene trials pobres tempranamente |

---

### Ejemplo BÃ¡sico con Optuna

```python
import optuna
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.datasets import load_iris

# 1. Cargar datos
X, y = load_iris(return_X_y=True)

# 2. Definir funciÃ³n objetivo
def objective(trial):
    # Sugerir hiperparÃ¡metros
    n_estimators = trial.suggest_int('n_estimators', 50, 500)
    max_depth = trial.suggest_int('max_depth', 2, 32)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)

    # Crear modelo con hiperparÃ¡metros sugeridos
    model = RandomForestClassifier(
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        random_state=42
    )

    # Evaluar con cross-validation
    score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()

    return score  # Optuna MAXIMIZA o MINIMIZA este valor

# 3. Crear study
study = optuna.create_study(direction='maximize')  # Maximizar accuracy

# 4. Optimizar
study.optimize(objective, n_trials=100)

# 5. Mejores hiperparÃ¡metros
print("Mejores hiperparÃ¡metros:", study.best_params)
print("Mejor score:", study.best_value)
print("Mejor trial:", study.best_trial)

# Ejemplo de salida:
# Mejores hiperparÃ¡metros: {'n_estimators': 324, 'max_depth': 18,
#                            'min_samples_split': 3, 'min_samples_leaf': 2}
# Mejor score: 0.9733
```

---

### Tipos de Suggest

#### 1. Suggest Integer

```python
def objective(trial):
    # Entero uniforme
    n_estimators = trial.suggest_int('n_estimators', 10, 1000)

    # Entero logarÃ­tmico (Ãºtil para bÃºsqueda exponencial)
    n_estimators_log = trial.suggest_int('n_estimators', 10, 1000, log=True)

    # Entero con step
    batch_size = trial.suggest_int('batch_size', 16, 256, step=16)
    # Valores posibles: 16, 32, 48, 64, ..., 256
```

#### 2. Suggest Float

```python
def objective(trial):
    # Float uniforme
    learning_rate = trial.suggest_float('learning_rate', 0.0001, 0.1)

    # Float logarÃ­tmico (para learning rates, regularizaciÃ³n)
    lr_log = trial.suggest_float('lr', 1e-5, 1e-1, log=True)

    # Float con step
    dropout = trial.suggest_float('dropout', 0.1, 0.9, step=0.1)
    # Valores: 0.1, 0.2, 0.3, ..., 0.9
```

#### 3. Suggest Categorical

```python
def objective(trial):
    # CategÃ³rico (para opciones discretas)
    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd', 'rmsprop'])

    activation = trial.suggest_categorical('activation', ['relu', 'tanh', 'sigmoid'])

    kernel = trial.suggest_categorical('kernel', ['linear', 'rbf', 'poly'])
```

---

### Ejemplo Avanzado: Multiple Modelos

```python
import optuna
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression

def objective(trial):
    # Sugerir quÃ© modelo usar
    classifier_name = trial.suggest_categorical('classifier', ['SVC', 'RandomForest', 'LogisticRegression'])

    if classifier_name == 'SVC':
        # HiperparÃ¡metros especÃ­ficos de SVC
        C = trial.suggest_float('svc_C', 1e-3, 1e3, log=True)
        kernel = trial.suggest_categorical('svc_kernel', ['linear', 'rbf', 'poly'])

        if kernel == 'rbf' or kernel == 'poly':
            gamma = trial.suggest_float('svc_gamma', 1e-4, 1e-1, log=True)
            classifier = SVC(C=C, kernel=kernel, gamma=gamma, random_state=42)
        else:
            classifier = SVC(C=C, kernel=kernel, random_state=42)

    elif classifier_name == 'RandomForest':
        # HiperparÃ¡metros de Random Forest
        n_estimators = trial.suggest_int('rf_n_estimators', 50, 500)
        max_depth = trial.suggest_int('rf_max_depth', 2, 32)
        classifier = RandomForestClassifier(
            n_estimators=n_estimators,
            max_depth=max_depth,
            random_state=42
        )

    else:  # LogisticRegression
        C = trial.suggest_float('lr_C', 1e-3, 1e3, log=True)
        classifier = LogisticRegression(C=C, random_state=42, max_iter=1000)

    # Evaluar
    score = cross_val_score(classifier, X, y, cv=5, scoring='accuracy').mean()
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=200)

print("Mejor modelo:", study.best_params['classifier'])
print("Mejores hiperparÃ¡metros:", study.best_params)
```

---

### Pruning: Detener Trials Pobres

**Concepto**: Si un trial estÃ¡ dando malos resultados en las primeras Ã©pocas/folds, **detenerlo temprano** para ahorrar tiempo.

```mermaid
graph TD
    A[Trial 1] --> B[Ã‰poca 1: Score bajo]
    B --> C{Pruner:<br/>Â¿Continuar?}
    C -->|No promisorio| D[âŒ DETENER<br/>Ahorrar tiempo]
    C -->|Promisorio| E[Continuar<br/>entrenamiento]

    F[Trial 2] --> G[Ã‰poca 1: Score alto]
    G --> H[Continuar hasta<br/>el final]

    style D fill:#ffcccc
    style H fill:#ccffcc
```

#### CÃ³digo con Pruning

```python
import optuna
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_validate

def objective(trial):
    # Sugerir hiperparÃ¡metros
    n_estimators = trial.suggest_int('n_estimators', 50, 500)
    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)
    max_depth = trial.suggest_int('max_depth', 3, 10)

    model = GradientBoostingClassifier(
        n_estimators=n_estimators,
        learning_rate=learning_rate,
        max_depth=max_depth,
        random_state=42
    )

    # Cross-validation con reporte de scores intermedios
    cv_results = cross_validate(model, X, y, cv=5, scoring='accuracy', return_train_score=False)

    # Reportar score de cada fold para pruning
    for i, score in enumerate(cv_results['test_score']):
        # Report intermediate value
        trial.report(score, i)

        # Check if trial should be pruned
        if trial.should_prune():
            raise optuna.TrialPruned()

    return cv_results['test_score'].mean()

# Study con pruner
study = optuna.create_study(
    direction='maximize',
    pruner=optuna.pruners.MedianPruner(  # Detiene si score < mediana
        n_startup_trials=10,  # No prunar los primeros 10 trials
        n_warmup_steps=2      # No prunar en los primeros 2 folds
    )
)

study.optimize(objective, n_trials=100)

print(f"Trials completados: {len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])}")
print(f"Trials podados: {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}")
```

#### Tipos de Pruners

| Pruner | DescripciÃ³n | CuÃ¡ndo Usar |
|--------|-------------|-------------|
| **MedianPruner** | Poda si score < mediana de otros trials | General purpose |
| **PercentilePruner** | Poda si score < percentil X | Control fino |
| **HyperbandPruner** | Successive Halving avanzado | Muchos hiperparÃ¡metros |
| **ThresholdPruner** | Poda si score < umbral fijo | Requisito mÃ­nimo conocido |

---

### VisualizaciÃ³n de Resultados

Optuna incluye herramientas de visualizaciÃ³n muy Ãºtiles:

```python
import optuna.visualization as vis

# 1. Historia de optimizaciÃ³n
fig = vis.plot_optimization_history(study)
fig.show()

# 2. Importancia de hiperparÃ¡metros
fig = vis.plot_param_importances(study)
fig.show()

# 3. Slice plot (efecto individual de cada hiperparÃ¡metro)
fig = vis.plot_slice(study)
fig.show()

# 4. Contour plot (interacciÃ³n entre 2 hiperparÃ¡metros)
fig = vis.plot_contour(study, params=['n_estimators', 'max_depth'])
fig.show()

# 5. Parallel coordinate plot
fig = vis.plot_parallel_coordinate(study)
fig.show()
```

---

### IntegraciÃ³n con scikit-learn

Optuna tiene una integraciÃ³n directa con scikit-learn:

```python
import optuna
from optuna.integration import OptunaSearchCV
from sklearn.ensemble import RandomForestClassifier

# Similar a GridSearchCV pero con Optuna
param_distributions = {
    'n_estimators': optuna.distributions.IntDistribution(50, 500),
    'max_depth': optuna.distributions.IntDistribution(2, 32),
    'min_samples_split': optuna.distributions.IntDistribution(2, 20)
}

# OptunaSearchCV (API similar a GridSearchCV)
optuna_search = OptunaSearchCV(
    estimator=RandomForestClassifier(random_state=42),
    param_distributions=param_distributions,
    n_trials=100,
    cv=5,
    scoring='accuracy',
    random_state=42
)

optuna_search.fit(X_train, y_train)

print("Mejores hiperparÃ¡metros:", optuna_search.best_params_)
print("Mejor score:", optuna_search.best_score_)

# Usar como un modelo normal
predictions = optuna_search.predict(X_test)
```

---

## ComparaciÃ³n de MÃ©todos

### Tabla Comparativa Completa

| MÃ©todo | Estrategia | Velocidad | Eficiencia | Flexibilidad | Complejidad | Mejor Para |
|--------|-----------|-----------|-----------|--------------|-------------|------------|
| **GridSearchCV** | Exhaustiva | â±ï¸â±ï¸â±ï¸ | â­â­ | ğŸ”§ | ğŸŸ¢ Simple | Espacios pequeÃ±os, reproducibilidad |
| **RandomizedSearchCV** | Aleatoria | â±ï¸â±ï¸ | â­â­â­ | ğŸ”§ğŸ”§ | ğŸŸ¢ Simple | Espacios grandes, exploraciÃ³n |
| **HalvingSearchCV** | EliminaciÃ³n | â±ï¸â±ï¸ | â­â­â­â­ | ğŸ”§ | ğŸŸ¡ Media | Balance velocidad/cobertura |
| **Optuna** | Bayesiana | â±ï¸ | â­â­â­â­â­ | ğŸ”§ğŸ”§ğŸ”§ | ğŸŸ¡ Media | Espacios complejos, mÃ¡xima eficiencia |

### Diagrama de DecisiÃ³n

```mermaid
graph TD
    A{Â¿CuÃ¡ntos<br/>hiperparÃ¡metros?} -->|1-2| B[GridSearchCV]
    A -->|3-4| C{Â¿Tiempo<br/>limitado?}
    A -->|5+| D[Optuna o<br/>RandomizedSearchCV]

    C -->|No| E[HalvingGridSearchCV]
    C -->|SÃ­| F[RandomizedSearchCV]

    G{Â¿Necesitas<br/>mÃ¡xima<br/>eficiencia?} -->|SÃ­| H[Optuna]
    G -->|No| I[RandomizedSearchCV]

    J{Â¿Conoces bien<br/>el espacio?} -->|SÃ­| B
    J -->|No| K[Optuna o<br/>RandomizedSearchCV]

    style B fill:#ffffcc
    style D fill:#ccffcc
    style E fill:#cce5ff
    style F fill:#ffcccc
    style H fill:#aaffaa
```

### Ejemplo de Performance

```
Dataset: 50,000 muestras, 100 features
Tarea: Optimizar Random Forest con 5 hiperparÃ¡metros
Budget: Encontrar la mejor configuraciÃ³n

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MÃ©todo                 â”‚ Tiempo     â”‚ Trials       â”‚ Best Score â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GridSearchCV           â”‚ 24 horas   â”‚ 1,024 (100%) â”‚   0.8765   â”‚
â”‚ RandomizedSearchCV     â”‚ 2 horas    â”‚   100 (10%)  â”‚   0.8720   â”‚
â”‚ HalvingGridSearchCV    â”‚ 8 horas    â”‚   300 (30%)  â”‚   0.8750   â”‚
â”‚ Optuna (TPE)           â”‚ 1.5 horas  â”‚   150 (15%)  â”‚   0.8780   â”‚ âœ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ConclusiÃ³n: Optuna encontrÃ³ MEJOR score en MENOS tiempo
```

---

## Mejores PrÃ¡cticas

### 1. Siempre Usar ValidaciÃ³n Cruzada

```python
# âŒ MAL: Sin validaciÃ³n
model.fit(X_train, y_train)
score = model.score(X_test, y_test)  # Solo un nÃºmero, puede ser suerte

# âœ… BIEN: Con validaciÃ³n cruzada
scores = cross_val_score(model, X_train, y_train, cv=5)
print(f"Score: {scores.mean():.3f} (+/- {scores.std():.3f})")
```

### 2. Separar Test Set ANTES de Optimizar

```python
# âŒ MAL: Test set usado durante optimizaciÃ³n
grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X, y)  # Usa TODOS los datos
score = grid_search.score(X, y)  # Data leakage!

# âœ… BIEN: Test set separado
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

grid_search = GridSearchCV(model, param_grid, cv=5)
grid_search.fit(X_train, y_train)  # Solo usa train

# Evaluar en test set NO VISTO
test_score = grid_search.score(X_test, y_test)
```

### 3. Definir Rangos Razonables

```python
# âŒ MAL: Rango muy amplio e ineficiente
param_grid = {
    'n_estimators': list(range(1, 1000)),  # 1000 valores!
    'learning_rate': [10**i for i in range(-10, 10)]  # Valores absurdos
}

# âœ… BIEN: Rango enfocado basado en experiencia
param_distributions = {
    'n_estimators': randint(50, 500),
    'learning_rate': loguniform(1e-4, 1e-1)  # Escala log para LR
}
```

### 4. Usar MÃ©tricas Apropiadas

```python
# Para clasificaciÃ³n desbalanceada
grid_search = GridSearchCV(
    model, param_grid,
    scoring='f1',  # No usar accuracy si clases desbalanceadas
    cv=StratifiedKFold(5)
)

# Para regresiÃ³n
grid_search = GridSearchCV(
    model, param_grid,
    scoring='neg_mean_squared_error',
    cv=5
)

# MÃºltiples mÃ©tricas
scoring = {
    'accuracy': 'accuracy',
    'precision': 'precision',
    'recall': 'recall',
    'f1': 'f1'
}
grid_search = GridSearchCV(model, param_grid, scoring=scoring, refit='f1')
```

### 5. Guardar Resultados

```python
import joblib

# Guardar mejor modelo
joblib.dump(grid_search.best_estimator_, 'best_model.pkl')

# Guardar todo el estudio de Optuna
import pickle
with open('study.pkl', 'wb') as f:
    pickle.dump(study, f)

# Cargar
study = pickle.load(open('study.pkl', 'rb'))
```

### 6. Pipeline Completo

```python
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Pipeline con preprocesamiento
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('pca', PCA()),
    ('classifier', RandomForestClassifier())
])

# Optimizar hiperparÃ¡metros del pipeline completo
param_grid = {
    'pca__n_components': [10, 20, 30],
    'classifier__n_estimators': [50, 100, 200],
    'classifier__max_depth': [5, 10, None]
}

grid_search = GridSearchCV(pipeline, param_grid, cv=5)
grid_search.fit(X_train, y_train)
```

---

## Flujo de Trabajo Recomendado

```mermaid
graph TD
    A[1. Split datos<br/>Train/Test] --> B[2. ExploraciÃ³n inicial<br/>con defaults]
    B --> C{Â¿Buen<br/>rendimiento?}
    C -->|SÃ­| D[3. OptimizaciÃ³n<br/>con RandomSearch]
    C -->|No| E[Revisar features<br/>y preprocesamiento]
    E --> B

    D --> F[4. Refinamiento<br/>con Optuna]
    F --> G[5. Validar en<br/>test set]
    G --> H{Â¿Satisfactorio?}
    H -->|SÃ­| I[âœ… Modelo final]
    H -->|No| J[Analizar errores]
    J --> E

    style I fill:#ccffcc
```

**Pasos**:

1. **Split inicial**: 80/20 o 70/30
2. **Baseline**: Entrenar con hiperparÃ¡metros default
3. **Primera optimizaciÃ³n**: RandomizedSearchCV (n_iter=50-100)
4. **Refinamiento**: Optuna (n_trials=100-200) en rango reducido
5. **ValidaciÃ³n final**: Test set NO VISTO
6. **AnÃ¡lisis**: Importancia de features, errores, curvas de aprendizaje

---

## Referencias

### DocumentaciÃ³n Oficial

1. **scikit-learn Model Selection**: https://scikit-learn.org/stable/model_selection.html
2. **scikit-learn GridSearchCV**: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html
3. **scikit-learn RandomizedSearchCV**: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html
4. **scikit-learn Cross-Validation**: https://scikit-learn.org/stable/modules/cross_validation.html

### Optuna

5. **Optuna Documentation**: https://optuna.readthedocs.io/
6. **Optuna GitHub**: https://github.com/optuna/optuna
7. **Optuna Examples**: https://github.com/optuna/optuna-examples

### ArtÃ­culos y Tutoriales

8. **Hyperparameter Tuning the Random Forest** - Towards Data Science
9. **A Conceptual Explanation of Bayesian Hyperparameter Optimization** - Machine Learning Mastery
10. **Optuna: A Next-generation Hyperparameter Optimization Framework** - Paper KDD 2019

### Otros Frameworks

11. **Hyperopt**: http://hyperopt.github.io/hyperopt/
12. **Ray Tune**: https://docs.ray.io/en/latest/tune/index.html
13. **Keras Tuner**: https://keras.io/keras_tuner/

---

**Licencia**: MIT License
**Autor**: David Palacio JimÃ©nez
**Fecha**: 2025
**VersiÃ³n**: 1.0
