"""
Aplicaci√≥n Gradio para Comparaci√≥n de Modelos de Predicci√≥n de Precios

Esta aplicaci√≥n permite comparar el desempe√±o de diferentes modelos de ML
lado a lado, mostrando tanto las predicciones como las m√©tricas de evaluaci√≥n.

Para ejecutar:
    python app_gradio2.py
"""

import gradio as gr
import sys
import pandas as pd
import numpy as np
import joblib
import json
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Agregar src/ al path para importar m√≥dulos
sys.path.append(str(Path(__file__).parent / "src"))

from config import MODELS_DIR, RANDOM_STATE, TEST_SIZE
from preprocessing import load_data, prepare_features
from predict import predict_single, format_prediction


# =============================================================================
# CARGA DE MODELOS Y C√ÅLCULO DE M√âTRICAS
# =============================================================================

def load_all_models():
    """
    Carga todos los modelos disponibles en el directorio de modelos.

    Returns:
        dict: Diccionario con nombre_modelo: (modelo, metadata)
    """
    models = {}
    metadata_files = list(MODELS_DIR.glob("*_metadata.json"))

    print("Cargando modelos disponibles...")

    for metadata_file in metadata_files:
        # Evitar el archivo model_metadata.json (es un duplicado)
        if metadata_file.name == "model_metadata.json":
            continue

        with open(metadata_file, 'r') as f:
            metadata = json.load(f)

        model_file = metadata.get('model_file')
        if not model_file:
            continue

        model_path = MODELS_DIR / model_file
        if not model_path.exists():
            continue

        # Cargar modelo
        try:
            model = joblib.load(model_path)
            model_name = metadata.get('model_type', model_file.replace('.pkl', ''))
            models[model_name] = {
                'model': model,
                'metadata': metadata,
                'file': model_file
            }
            print(f"  ‚úì {model_name} cargado")
        except Exception as e:
            print(f"  ‚úó Error cargando {model_file}: {e}")

    return models


def compute_all_metrics(models):
    """
    Calcula m√©tricas de test para todos los modelos.

    Args:
        models (dict): Diccionario de modelos cargados

    Returns:
        dict: Diccionario con nombre_modelo: {r2, mae, rmse}
    """
    print("\nCalculando m√©tricas en conjunto de prueba...")

    # Cargar datos y hacer split (mismo que en entrenamiento)
    df = load_data()
    X, y = prepare_features(df, include_target=True)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
    )

    metrics = {}

    for model_name, model_info in models.items():
        model = model_info['model']

        try:
            # Predecir en test set
            y_pred = model.predict(X_test)

            # Calcular m√©tricas
            r2 = r2_score(y_test, y_pred)
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))

            metrics[model_name] = {
                'r2': r2,
                'mae': mae,
                'rmse': rmse
            }

            print(f"  ‚úì {model_name}: R¬≤={r2:.4f}, MAE=${mae:,.0f}")

        except Exception as e:
            print(f"  ‚úó Error calculando m√©tricas para {model_name}: {e}")
            metrics[model_name] = {
                'r2': 0.0,
                'mae': 0.0,
                'rmse': 0.0
            }

    return metrics


# Cargar modelos y m√©tricas al iniciar
print("="*60)
print("INICIALIZANDO APLICACI√ìN DE COMPARACI√ìN DE MODELOS")
print("="*60)

try:
    AVAILABLE_MODELS = load_all_models()
    MODEL_METRICS = compute_all_metrics(AVAILABLE_MODELS)
    print(f"\n‚úì {len(AVAILABLE_MODELS)} modelos cargados exitosamente\n")
    print("="*60)
except Exception as e:
    print(f"\n‚úó Error en inicializaci√≥n: {e}")
    AVAILABLE_MODELS = {}
    MODEL_METRICS = {}


# =============================================================================
# FUNCI√ìN DE COMPARACI√ìN
# =============================================================================

def compare_models(
    model1_name, model2_name,
    marca, tipo_carroceria, a√±o, kilometraje, tipo_combustible,
    transmision, cilindrada, potencia, peso, consumo,
    color, edad_propietarios, calificacion_estado, region_venta
):
    """
    Compara dos modelos realizando predicciones y mostrando m√©tricas.

    Returns:
        str: Reporte comparativo formateado
    """
    if not AVAILABLE_MODELS:
        return "ERROR: No se han cargado modelos. Verifica que existan modelos entrenados."

    if model1_name == model2_name:
        return "‚ö†Ô∏è Por favor selecciona dos modelos diferentes para comparar."

    try:
        # Crear diccionario con los datos de entrada
        input_data = {
            'marca': marca,
            'tipo_carroceria': tipo_carroceria,
            'a√±o': int(a√±o),
            'kilometraje': int(kilometraje),
            'tipo_combustible': tipo_combustible,
            'transmision': transmision,
            'cilindrada': int(cilindrada),
            'potencia': int(potencia),
            'peso': int(peso),
            'consumo': float(consumo),
            'color': color,
            'edad_propietarios': int(edad_propietarios),
            'calificacion_estado': float(calificacion_estado),
            'region_venta': region_venta
        }

        # Obtener modelos
        model1 = AVAILABLE_MODELS[model1_name]['model']
        model2 = AVAILABLE_MODELS[model2_name]['model']

        # Realizar predicciones
        pred1 = predict_single(model1, input_data)
        pred2 = predict_single(model2, input_data)

        # Obtener m√©tricas
        metrics1 = MODEL_METRICS[model1_name]
        metrics2 = MODEL_METRICS[model2_name]

        # Calcular diferencias
        diff_precio = pred1 - pred2
        diff_precio_pct = (diff_precio / pred2) * 100 if pred2 != 0 else 0

        # Determinar mejor modelo
        mejor_r2 = model1_name if metrics1['r2'] > metrics2['r2'] else model2_name
        mejor_mae = model1_name if metrics1['mae'] < metrics2['mae'] else model2_name
        mejor_rmse = model1_name if metrics1['rmse'] < metrics2['rmse'] else model2_name

        # Formatear resultado
        resultado = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë              COMPARACI√ìN DE MODELOS DE PREDICCI√ìN            ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üìä MODELO 1: {model1_name.upper()}
{'‚îÄ'*60}
   Predicci√≥n:        {format_prediction(pred1)}

   M√©tricas en Test Set:
   ‚Ä¢ R¬≤ Score:         {metrics1['r2']:.4f} ({metrics1['r2']*100:.2f}% varianza explicada)
   ‚Ä¢ MAE:              ${metrics1['mae']:,.2f} (error promedio)
   ‚Ä¢ RMSE:             ${metrics1['rmse']:,.2f} (penaliza errores grandes)


üìä MODELO 2: {model2_name.upper()}
{'‚îÄ'*60}
   Predicci√≥n:        {format_prediction(pred2)}

   M√©tricas en Test Set:
   ‚Ä¢ R¬≤ Score:         {metrics2['r2']:.4f} ({metrics2['r2']*100:.2f}% varianza explicada)
   ‚Ä¢ MAE:              ${metrics2['mae']:,.2f} (error promedio)
   ‚Ä¢ RMSE:             ${metrics2['rmse']:,.2f} (penaliza errores grandes)


üîç AN√ÅLISIS COMPARATIVO
{'‚îÄ'*60}
   Diferencia en Predicci√≥n:
   ‚Ä¢ Absoluta:         ${abs(diff_precio):,.2f}
   ‚Ä¢ Relativa:         {abs(diff_precio_pct):.2f}%
   ‚Ä¢ Modelo 1 predice: {"M√ÅS ALTO" if diff_precio > 0 else "M√ÅS BAJO" if diff_precio < 0 else "IGUAL"}

   Mejor Modelo por M√©trica:
   ‚Ä¢ Mayor R¬≤:         {mejor_r2} ‚≠ê
   ‚Ä¢ Menor MAE:        {mejor_mae} ‚≠ê
   ‚Ä¢ Menor RMSE:       {mejor_rmse} ‚≠ê


üí° INTERPRETACI√ìN
{'‚îÄ'*60}
   ‚Ä¢ R¬≤ m√°s alto = Explica mejor la variabilidad de precios
   ‚Ä¢ MAE m√°s bajo = Menor error promedio en predicciones
   ‚Ä¢ RMSE m√°s bajo = Menos errores grandes (m√°s confiable)

   ‚ö†Ô∏è IMPORTANTE: Las m√©tricas son en el conjunto de PRUEBA (20% de datos
   no vistos durante entrenamiento), lo que indica el desempe√±o real.

{'‚ïê'*60}
"""

        return resultado

    except Exception as e:
        return f"‚ùå Error al realizar comparaci√≥n: {str(e)}"


# =============================================================================
# INTERFAZ GRADIO
# =============================================================================

# Definir valores y opciones (mismo que app_gradio.py)
MARCAS = ['Toyota', 'Honda', 'Ford', 'Chevrolet', 'Nissan', 'Mazda',
          'Hyundai', 'Kia', 'BMW', 'Mercedes-Benz', 'Audi']
TIPOS_CARROCERIA = ['Sed√°n', 'SUV', 'Hatchback', 'Pickup', 'Coup√©', 'Minivan']
TIPOS_COMBUSTIBLE = ['Gasolina', 'Diesel', 'H√≠brido', 'El√©ctrico']
TRANSMISIONES = ['Manual', 'Autom√°tica']
COLORES = ['Blanco', 'Negro', 'Gris', 'Plata', 'Rojo', 'Azul', 'Verde', 'Amarillo']
REGIONES = ['Norte', 'Sur', 'Este', 'Oeste', 'Centro']

# Obtener nombres de modelos disponibles
model_names = sorted(AVAILABLE_MODELS.keys()) if AVAILABLE_MODELS else ['No hay modelos']

# Valores por defecto para comparaci√≥n educativa
# LightGBM (mejor) vs Linear Regression (baseline simple)
default_model1 = 'lightgbm' if 'lightgbm' in model_names else model_names[0]
default_model2 = 'linear_regression' if 'linear_regression' in model_names else model_names[-1]


# Crear interfaz
with gr.Blocks(title="Comparaci√≥n de Modelos - Predicci√≥n de Precios") as app:

    gr.Markdown("# üî¨ Comparaci√≥n Interactiva de Modelos ML")
    gr.Markdown(
        """
        Esta aplicaci√≥n permite **comparar dos modelos** de Machine Learning lado a lado.
        Ingresa las caracter√≠sticas de un autom√≥vil y observa c√≥mo diferentes modelos
        predicen su precio, junto con sus m√©tricas de desempe√±o en datos de prueba.
        """
    )

    # Secci√≥n de selecci√≥n de modelos
    gr.Markdown("## Selecciona los Modelos a Comparar")

    with gr.Row():
        model1_dropdown = gr.Dropdown(
            choices=model_names,
            label="üéØ Modelo 1",
            value=default_model1,
            info="Primer modelo para comparar"
        )
        model2_dropdown = gr.Dropdown(
            choices=model_names,
            label="üéØ Modelo 2",
            value=default_model2,
            info="Segundo modelo para comparar"
        )

    gr.Markdown("---")
    gr.Markdown("## Caracter√≠sticas del Autom√≥vil")

    # Formulario de entrada (mismo layout que app_gradio.py)
    with gr.Row():
        # Columna 1: Informaci√≥n general
        with gr.Column():
            gr.Markdown("### Informaci√≥n General")
            marca = gr.Dropdown(choices=MARCAS, label="Marca", value="Toyota")
            tipo_carroceria = gr.Dropdown(choices=TIPOS_CARROCERIA, label="Tipo de Carrocer√≠a", value="SUV")
            a√±o = gr.Number(label="A√±o de Fabricaci√≥n", value=2020, minimum=2000, maximum=2024)
            kilometraje = gr.Number(label="Kilometraje", value=50000, minimum=0)

        # Columna 2: Motor y caracter√≠sticas t√©cnicas
        with gr.Column():
            gr.Markdown("### Caracter√≠sticas T√©cnicas")
            tipo_combustible = gr.Dropdown(choices=TIPOS_COMBUSTIBLE, label="Tipo de Combustible", value="Gasolina")
            transmision = gr.Dropdown(choices=TRANSMISIONES, label="Transmisi√≥n", value="Autom√°tica")
            cilindrada = gr.Number(label="Cilindrada (cc)", value=2000, minimum=1000, maximum=5000)
            potencia = gr.Number(label="Potencia (HP)", value=150, minimum=50, maximum=500)

    with gr.Row():
        # Columna 3: Caracter√≠sticas f√≠sicas
        with gr.Column():
            gr.Markdown("### Caracter√≠sticas F√≠sicas")
            peso = gr.Number(label="Peso (kg)", value=1500, minimum=800, maximum=3000)
            consumo = gr.Number(label="Consumo (L/100km)", value=8.5, minimum=0, maximum=20)
            color = gr.Dropdown(choices=COLORES, label="Color", value="Blanco")

        # Columna 4: Estado y ubicaci√≥n
        with gr.Column():
            gr.Markdown("### Estado y Ubicaci√≥n")
            edad_propietarios = gr.Number(label="N√∫mero de Propietarios Previos", value=1, minimum=1, maximum=5)
            calificacion_estado = gr.Slider(label="Calificaci√≥n del Estado (1-10)", minimum=1, maximum=10, value=8.5, step=0.5)
            region_venta = gr.Dropdown(choices=REGIONES, label="Regi√≥n de Venta", value="Centro")

    # Bot√≥n de comparaci√≥n
    compare_button = gr.Button("üîç Comparar Modelos", variant="primary", size="lg")

    # Salida de resultados
    output = gr.Textbox(
        label="Resultados de la Comparaci√≥n",
        lines=35,
        interactive=False,
        show_copy_button=True
    )

    # Conectar el bot√≥n con la funci√≥n de comparaci√≥n
    compare_button.click(
        fn=compare_models,
        inputs=[
            model1_dropdown, model2_dropdown,
            marca, tipo_carroceria, a√±o, kilometraje, tipo_combustible,
            transmision, cilindrada, potencia, peso, consumo,
            color, edad_propietarios, calificacion_estado, region_venta
        ],
        outputs=output
    )

    # Informaci√≥n adicional
    gr.Markdown("---")
    gr.Markdown(
        """
        ### üìö Gu√≠a de Uso

        1. **Selecciona dos modelos** diferentes en los dropdowns superiores
        2. **Ingresa caracter√≠sticas** del autom√≥vil en el formulario
        3. **Haz clic en "Comparar Modelos"** para ver resultados

        ### üìä M√©tricas Explicadas

        - **R¬≤ (Coeficiente de Determinaci√≥n)**: Mide qu√© % de variabilidad es explicado por el modelo (0-1, mayor es mejor)
        - **MAE (Error Absoluto Medio)**: Promedio de errores en d√≥lares (menor es mejor)
        - **RMSE (Ra√≠z del Error Cuadr√°tico Medio)**: Similar a MAE pero penaliza m√°s los errores grandes (menor es mejor)

        ### üí° Casos de Uso Educativos

        - Compara **LightGBM vs Linear Regression** para ver diferencia entre modelos complejos y simples
        - Compara **Random Forest vs Random Forest Deep** para entender impacto de hiperpar√°metros
        - Compara **modelos similares** (LightGBM vs Random Forest) para elegir el mejor

        ### ‚ö†Ô∏è Nota Importante

        Las m√©tricas mostradas son del **conjunto de prueba** (20% de datos no vistos),
        lo que representa el desempe√±o real del modelo en producci√≥n.

        ---

        **Proyecto Educativo** | Para fines acad√©micos | MIT License
        """
    )


if __name__ == "__main__":
    # Lanzar aplicaci√≥n
    print("\n" + "="*60)
    print("Iniciando servidor Gradio...")
    print("="*60)

    app.launch(
        server_name="0.0.0.0",
        server_port=7861,  # Puerto diferente al app_gradio.py (7860)
        share=False
    )
