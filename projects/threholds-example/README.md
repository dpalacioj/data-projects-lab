# Umbrales de ClasificaciÃ³n

## ðŸ“š DescripciÃ³n

Este proyecto ilustra cÃ³mo el ajuste del umbral de clasificaciÃ³n (threshold) afecta las predicciones y mÃ©tricas de desempeÃ±o en modelos de clasificaciÃ³n binaria, especialmente en escenarios con clases desbalanceadas.

## ðŸŽ¯ Objetivo de Aprendizaje

Entender que:
- Los modelos de clasificaciÃ³n predicen **probabilidades**, no clases directamente
- El umbral convierte probabilidades en decisiones binarias
- El umbral por defecto (0.5) no siempre es Ã³ptimo
- Existe un trade-off entre Precision y Recall controlado por el umbral

## ðŸ“– TeorÃ­a: Umbral de ClasificaciÃ³n

### Â¿QuÃ© es el umbral?

En clasificaciÃ³n binaria, los modelos de machine learning generan una **probabilidad** P(y=1) para cada observaciÃ³n. Para convertir esta probabilidad en una predicciÃ³n de clase, aplicamos un **umbral** (threshold):

```
Si P(y=1) >= umbral â†’ Predecir clase 1 (positivo)
Si P(y=1) < umbral  â†’ Predecir clase 0 (negativo)
```

**Por defecto, el umbral es 0.5**, pero esto es arbitrario y no siempre Ã³ptimo.

### Â¿Por quÃ© ajustar el umbral?

1. **Clases desbalanceadas**: Cuando una clase es minoritaria (ej: 5% de casos positivos), el umbral 0.5 puede ser inadecuado.

2. **Costos asimÃ©tricos**: En muchos problemas, los errores tienen costos diferentes:
   - Falso Negativo en diagnÃ³stico mÃ©dico â†’ Muy costoso
   - Falso Positivo en spam filtering â†’ Menos costoso

3. **Objetivos de negocio**: Diferentes aplicaciones requieren diferentes trade-offs.

### El Trade-off Precision-Recall

Al ajustar el umbral, modificamos el balance entre Precision y Recall:

| Umbral | Predicciones Positivas | Precision | Recall | Uso tÃ­pico |
|--------|------------------------|-----------|--------|------------|
| **Bajo (ej: 0.2)** | MÃ¡s predicciones como positivas | â†“ Baja | â†‘ Alto | DetecciÃ³n de fraude, enfermedades |
| **Medio (0.5)** | Balance estÃ¡ndar | Media | Medio | Default general |
| **Alto (ej: 0.8)** | Menos predicciones como positivas | â†‘ Alta | â†“ Bajo | Marketing dirigido, precisiÃ³n crÃ­tica |

#### Precision (PrecisiÃ³n)

$$\text{Precision} = \frac{TP}{TP + FP}$$

De todas las predicciones positivas, Â¿cuÃ¡ntas son correctas?

#### Recall (Sensibilidad, Cobertura)

$$\text{Recall} = \frac{TP}{TP + FN}$$

De todos los casos positivos reales, Â¿cuÃ¡ntos capturamos?

### MÃ©todos para Seleccionar el Umbral Ã“ptimo

1. **Criterio de Youden (J-statistic)**
   - Maximiza: J = Sensibilidad + Especificidad - 1
   - Equilibra TPR y FPR
   - Usado en este notebook

2. **Maximizar F1-Score**
   - Balance armÃ³nico entre Precision y Recall
   - Ãštil cuando ambas mÃ©tricas son igualmente importantes

3. **Curva Precision-Recall**
   - Analizar el punto de mejor balance segÃºn necesidades del negocio

4. **Costo-beneficio**
   - Asignar costos a FP y FN
   - Minimizar el costo total esperado

## ðŸš€ CÃ³mo usar este proyecto

### Requisitos

```bash
pip install lightgbm scikit-learn pandas matplotlib seaborn jupyter
```

### EjecuciÃ³n

1. Navegar a la carpeta del proyecto:
```bash
cd projects/threholds-example
```

2. Abrir el notebook:
```bash
jupyter notebook umbral_clasificacion.ipynb
```

3. Ejecutar las celdas secuencialmente.

## ðŸ“Š Contenido del Notebook

1. **GeneraciÃ³n de datos sintÃ©ticos** con clases desbalanceadas (90%-10%)
2. **Entrenamiento de LightGBM** para obtener probabilidades
3. **EvaluaciÃ³n con umbral por defecto (0.5)**
4. **ExperimentaciÃ³n con diferentes umbrales** (0.1 a 0.9)
5. **AnÃ¡lisis de la curva ROC** y selecciÃ³n de umbral Ã³ptimo
6. **ComparaciÃ³n de resultados** entre umbrales

## ðŸŽ“ Casos de Uso Reales

### Umbral Bajo (Maximizar Recall)
- **DiagnÃ³stico de enfermedades**: Preferimos capturar todos los casos positivos (pocos FN)
- **DetecciÃ³n de fraude**: No queremos perder transacciones fraudulentas
- **Sistemas de alerta**: Mejor prevenir que lamentar

### Umbral Alto (Maximizar Precision)
- **Marketing dirigido**: Solo contactar clientes con alta probabilidad de conversiÃ³n
- **Recomendaciones crÃ­ticas**: Evitar sugerencias irrelevantes
- **Filtrado de contenido**: Reducir falsos positivos molestos

## ðŸ“ Conclusiones Clave

1. El umbral de 0.5 es una convenciÃ³n, **no una regla universal**
2. Clases desbalanceadas requieren ajuste de umbral casi siempre
3. La elecciÃ³n del umbral depende del **contexto y objetivos del problema**
4. Siempre analiza Precision, Recall y la curva ROC antes de decidir
5. El mejor umbral es el que **minimiza el costo real del error** en tu aplicaciÃ³n

## ðŸ“š Referencias

- [Scikit-learn: Precision-Recall](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)
- [Google ML Crash Course: Classification](https://developers.google.com/machine-learning/crash-course/classification)
- Provost, F., & Fawcett, T. (2001). "Robust Classification for Imprecise Environments"

## ðŸ‘¨â€ðŸ« Para Instructores

Este material es ideal para:
- Cursos de Machine Learning nivel intermedio
- MÃ³dulos sobre mÃ©tricas de clasificaciÃ³n
- Talleres sobre clases desbalanceadas
- Proyectos prÃ¡cticos con datos reales desbalanceados

**DuraciÃ³n estimada**: 45-60 minutos

---

**Licencia**: MIT License
**Autor**: David Palacio JimÃ©nez
